name: CI

# Cancel outdated runs on new pushes to same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Minimal permissions by default
permissions:
  contents: read

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  lint:
    name: Lint (ruff)
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run ruff (lint)
        id: ruff_lint
        continue-on-error: true
        run: |
          set -o pipefail
          uv run ruff check src/ tests/ | tee ruff_lint.log

      - name: Run ruff (format check)
        id: ruff_format
        continue-on-error: true
        run: |
          set -o pipefail
          uv run ruff format --check src/ tests/ | tee ruff_format.log

      - name: Comment on lint warnings
        if: ${{ (steps.ruff_lint.outcome == 'failure' || steps.ruff_format.outcome == 'failure') && github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const lint = fs.existsSync('ruff_lint.log') ? fs.readFileSync('ruff_lint.log', 'utf8').trim() : '';
            const format = fs.existsSync('ruff_format.log') ? fs.readFileSync('ruff_format.log', 'utf8').trim() : '';
            let body = '### Lint warnings detected\n';
            if (lint) {
              body += '\n**ruff check**\n````\n' + lint + '\n````\n';
            }
            if (format) {
              body += '\n**ruff format --check**\n````\n' + format + '\n````\n';
            }
            await github.rest.issues.createComment({
              ...context.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Fail on lint warnings
        if: ${{ steps.ruff_lint.outcome == 'failure' || steps.ruff_format.outcome == 'failure' }}
        run: exit 1

  type-check:
    name: Type Check (mypy)
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run mypy
        id: mypy
        continue-on-error: true
        run: |
          set -o pipefail
          uv run mypy src/ | tee mypy.log

      - name: Comment on type check warnings
        if: ${{ steps.mypy.outcome == 'failure' && github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const output = fs.existsSync('mypy.log') ? fs.readFileSync('mypy.log', 'utf8').trim() : '';
            const body = '### Type check warnings detected\n\n**mypy**\n````\n' + output + '\n````\n';
            await github.rest.issues.createComment({
              ...context.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Fail on type check warnings
        if: ${{ steps.mypy.outcome == 'failure' }}
        run: exit 1

  test-unit:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run unit tests with coverage
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: uv run pytest tests/unit/ -v --cov=src --cov-branch --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage.xml
          flags: unit
          name: unit-tests
          fail_ci_if_error: false

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-unit
          path: coverage.xml
          retention-days: 7

      - name: Upload test results on failure
        if: failure()
        uses: actions/upload-artifact@v5
        with:
          name: unit-test-results
          path: |
            .pytest_cache/
            *.log
          retention-days: 7

  test-e2e:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run E2E tests with coverage
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: uv run pytest tests/e2e/ -v --cov=src --cov-branch --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage.xml
          flags: e2e
          name: e2e-tests
          fail_ci_if_error: false

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-e2e
          path: coverage.xml
          retention-days: 7

      - name: Upload test results on failure
        if: failure()
        uses: actions/upload-artifact@v5
        with:
          name: e2e-test-results
          path: |
            .pytest_cache/
            *.log
            .egregora/
          retention-days: 7

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run safety check
        run: |
          uv pip install safety
          uv run safety check --json
        # Remove continue-on-error - let security issues fail the build

  build:
    name: Build Package
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Build package
        run: uv build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v5
        with:
          name: dist
          path: dist/
          retention-days: 30

  gemini-review:
    name: Gemini PR Code Review
    needs: [lint, type-check, test-unit, test-e2e, security, build]
    if: github.event_name == 'pull_request' && !github.event.pull_request.draft
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Get PR details
        id: pr
        uses: actions/github-script@v8
        with:
          script: |
            const prData = context.payload.pull_request;

            core.setOutput('pr_number', prData.number);
            core.setOutput('base_sha', prData.base.sha);
            core.setOutput('base_ref', prData.base.ref);
            core.setOutput('head_sha', prData.head.sha);
            core.setOutput('pr_title', prData.title);
            core.setOutput('pr_author', prData.user.login);
            core.setOutput('pr_body', prData.body || '(No description provided)');

      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Collect PR diff and context
        id: collect
        env:
          BASE_SHA: ${{ steps.pr.outputs.base_sha }}
          BASE_REF: ${{ steps.pr.outputs.base_ref }}
          HEAD_SHA: ${{ steps.pr.outputs.head_sha }}
          PR_TITLE: ${{ steps.pr.outputs.pr_title }}
          PR_BODY: ${{ steps.pr.outputs.pr_body }}
        run: |
          set -euo pipefail

          # Ensure we have the base ref locally (works for forks and distant branches)
          # No --depth flag since we already did fetch-depth: 0 in checkout
          git fetch origin "${BASE_REF}"

          # Get unified diff between base and head
          git diff --unified=3 "origin/${BASE_REF}" "${HEAD_SHA}" > pr.diff

          # Trim diff to avoid blowing the context window (90KB ~= 22k tokens)
          head -c 90000 pr.diff > pr-trimmed.diff

          # Warn if diff was truncated
          if [ $(wc -c < pr.diff) -gt 90000 ]; then
            echo "‚ö†Ô∏è WARNING: Diff truncated at 90KB. This is a large PR." >> pr-trimmed.diff
            echo "Review may miss issues in truncated sections." >> pr-trimmed.diff
          fi

          # Get commit messages to understand intent
          # Format: hash - subject + body (if any)
          git log --format="%h - %s%n%b" "origin/${BASE_REF}..${HEAD_SHA}" > commits.txt || echo "(No commits found)" > commits.txt

          # Read CLAUDE.md for project-specific context
          CLAUDE_MD=""
          if [ -f "CLAUDE.md" ]; then
            CLAUDE_MD=$(cat CLAUDE.md)
          fi

          # Output diff and context for the next step
          # Use unique delimiters to avoid conflicts with diff content
          DIFF_DELIMITER="ghadelimiter_diff_$(uuidgen)"
          CLAUDE_DELIMITER="ghadelimiter_claude_$(uuidgen)"
          COMMITS_DELIMITER="ghadelimiter_commits_$(uuidgen)"
          PR_BODY_DELIMITER="ghadelimiter_pr_body_$(uuidgen)"

          {
            echo "diff<<${DIFF_DELIMITER}"
            cat pr-trimmed.diff
            echo "${DIFF_DELIMITER}"
            echo "claude_md<<${CLAUDE_DELIMITER}"
            echo "$CLAUDE_MD"
            echo "${CLAUDE_DELIMITER}"
            echo "commits<<${COMMITS_DELIMITER}"
            cat commits.txt
            echo "${COMMITS_DELIMITER}"
            echo "pr_body<<${PR_BODY_DELIMITER}"
            echo "$PR_BODY"
            echo "${PR_BODY_DELIMITER}"
          } >> "$GITHUB_OUTPUT"

      - name: Collect recent CI results for context
        id: ci_results
        uses: actions/github-script@v8
        env:
          PR_NUMBER: ${{ steps.pr.outputs.pr_number }}
        with:
          script: |
            const { owner, repo } = context.repo;
            const prNumber = Number(process.env.PR_NUMBER);

            const { data } = await github.rest.actions.listWorkflowRunsForPullRequest({
              owner,
              repo,
              pull_number: prNumber,
              per_page: 20,
            });

            const ciRuns = data.workflow_runs
              .filter((run) => run.name?.toLowerCase().includes('ci'))
              .slice(0, 5)
              .map((run) => {
                const status = run.conclusion ?? run.status;
                const attempt = run.run_attempt ? ` (attempt ${run.run_attempt})` : '';
                return `- ${run.name} #${run.run_number}${attempt}: ${status} (${run.html_url})`;
              });

            const summary = ciRuns.length
              ? `Recent CI runs for this PR:\n${ciRuns.join('\n')}`
              : 'No recent CI runs named "ci" were found for this PR.';

            core.setOutput('ci_summary', summary);

      - name: Summarize upstream CI job results
        id: ci_needs
        uses: actions/github-script@v8
        env:
          NEEDS_JSON: ${{ toJson(needs) }}
        with:
          script: |
            const needs = JSON.parse(process.env.NEEDS_JSON || '{}');
            const entries = Object.entries(needs).map(([name, data]) => {
              const status = data.result || data.outcome || data.conclusion || 'unknown';
              return `- ${name}: ${status}`;
            });

            const summary = entries.length
              ? `Current workflow job results:\n${entries.join('\n')}`
              : 'No upstream CI job data available.';

            core.setOutput('needs_summary', summary);

      - name: Construct Gemini Prompt
        id: construct_prompt
        uses: actions/github-script@v8
        env:
          DIFF: ${{ steps.collect.outputs.diff }}
          CLAUDE_MD: ${{ steps.collect.outputs.claude_md }}
          COMMITS: ${{ steps.collect.outputs.commits }}
          PR_BODY: ${{ steps.collect.outputs.pr_body }}
          PR_TITLE: ${{ steps.pr.outputs.pr_title }}
          PR_AUTHOR: ${{ steps.pr.outputs.pr_author }}
          PR_NUMBER: ${{ steps.pr.outputs.pr_number }}
          CI_SUMMARY: ${{ steps.ci_results.outputs.ci_summary }}
          NEEDS_SUMMARY: ${{ steps.ci_needs.outputs.needs_summary }}
        with:
          script: |
            const prompt = `You are a senior software engineer and code reviewer for the **egregora** repository.

            ## Project Context

            Egregora is a privacy-first AI pipeline that extracts structured knowledge from unstructured communication.
            - **Stack:** Python 3.12+ | uv | Ibis | DuckDB | Pydantic-AI | Google Gemini
            - **Core Principle:** Privacy before intelligence (names ‚Üí UUIDs before LLM)
            - **Philosophy:** Alpha mindset‚Äîclean breaks over backward compatibility
            - **Architecture:** Three-layer functional (orchestration ‚Üí transformations/adapters ‚Üí data_primitives)

            ---

            ## Review Philosophy: Two-Phase Approach

            Your review has **TWO PHASES**. You MUST complete Phase 1 before Phase 2.

            ### Phase 1: Understanding (REQUIRED FIRST - take 2-5 minutes)

            Your first job is **understanding, not judging**. Ask yourself:

            1. **What changed?** (Read the diff first)
               - Which files? What type of change? (feature/fix/refactor/docs/config)
               - What do imports, function names, and structure tell you?
               - What will this code DO when executed?

            2. **What was the author trying to accomplish?**
               - Read PR title/description and commits (they're often vague or missing - that's OK)
               - What's the strongest interpretation of the code's intent?
               - What constraints might justify this approach?

            3. **Does stated intent match actual changes?**
               - If YES: Great, you understand the goal
               - If NO/PARTIAL: Infer intent from code (code is ground truth)
               - If description is empty: No problem, infer from code

            4. **Steel-man the approach**
               - What's a legitimate reason for doing it this way?
               - What might the author know that isn't obvious?
               - Assume competence unless proven otherwise

            **Phase 1 Output:** Write a 3-5 sentence summary of what this PR does and why.

            ### Phase 2: Evaluation (ONLY AFTER understanding)

            Now that you understand the goal, evaluate the execution:

            **Must check (in priority order):**
            1. **üî¥ Correctness** - Does it work? Are there bugs or logic errors?
            2. **üî¥ Safety** - Privacy bypasses? Security holes? Data loss risks?
            3. **üü° Quality** - Is the approach sound? Are there simpler alternatives?
            4. **üü° Patterns** - Follows egregora conventions? (see CLAUDE.md below)

            **Critical Mindset:**
            - **Different ‚â† Wrong** - Don't flag valid alternative approaches as issues
            - **Avoid nitpicking** - If it's not a bug, security risk, or clear pattern violation, skip it
            - **Trust the author** - They may know constraints you don't see
            - **When uncertain** - Say "Consider..." not "You must..."
            - **Goal:** Make the code better, not perfect

            **Tone:** Professional, direct, concise, actionable. No fluff or unnecessary praise.

            ---

            ## ‚öñÔ∏è Adaptive Review Depth

            Calibrate your review depth to the PR scope:

            **Trivial PRs** (1-10 lines, obvious fixes):
            - Skip Phase 1 if intent is obvious from code
            - 2-3 sentence review: "Changes X to fix Y. ‚úÖ Correct. No issues."
            - Only flag issues if genuinely problematic

            **Small PRs** (10-100 lines, single feature/fix):
            - Brief Phase 1 (2-3 sentences)
            - Focus on correctness and critical issues
            - Target: 200-400 words total

            **Medium PRs** (100-500 lines, typical feature):
            - Standard full review
            - Prioritize top 3-5 most important issues
            - Target: 400-800 words total

            **Large PRs** (500+ lines, major refactor):
            - Focus on architecture and blocking issues
            - Skip minor issues, focus on what could break
            - Suggest splitting if scope is mixed (refactor + new feature)
            - Target: 800-1200 words MAX

            ---

            ## üéØ Special PR Types

            **Documentation-only PRs:**
            - Focus on: accuracy, clarity, completeness, broken links
            - Skip: code quality checks, pattern compliance
            - Verdict: Does it improve understanding?

            **Dependency updates:**
            - Focus on: breaking changes, security fixes, compatibility
            - Check: CHANGELOG/release notes for major version bumps
            - Note any API changes that might affect our code

            **Generated/Lock files** (package-lock.json, migrations, poetry.lock):
            - Don't review line-by-line
            - Note: "Generated files - spot-checked for anomalies"
            - Only flag if something looks suspiciously wrong

            **Test-only changes:**
            - Focus on: test coverage, edge cases, assertion quality
            - Check: Do tests validate intended behavior?
            - Are test names clear about what they're testing?

            **Configuration changes** (.yml, .toml, .env.example):
            - Focus on: security implications, breaking changes
            - Check: Are defaults sensible? Is it documented?
            - Note any changes that affect deployment

            ---

            ## üìö Examples: Good vs Bad Feedback

            **‚ùå BAD - Vague and unhelpful:**
            > "This code could be better. Consider refactoring."

            **‚úÖ GOOD - Specific and actionable:**
            > "**auth.py:67** - üî¥ Password stored in plaintext. Use \`bcrypt.hashpw()\` before saving to DB."

            ---

            **‚ùå BAD - Nitpicking style:**
            > "**utils.py:23** - Should use list comprehension instead of for loop."

            **‚úÖ GOOD - Skip it unless it matters:**
            > *(Don't mention style preferences unless they cause bugs)*

            ---

            **‚ùå BAD - Prescriptive without understanding:**
            > "**models.py:45** - Must use Pydantic models, not dicts."

            **‚úÖ GOOD - Understand the tradeoff:**
            > "**models.py:45** - üü° Using dict instead of Pydantic. If this is for performance in a hot path, add a comment explaining the tradeoff. Otherwise, Pydantic would add type safety."

            ---

            **‚ùå BAD - False positive:**
            > "**test_auth.py:12** - Missing error handling for network failures."
            > *(It's a test with mocked network...)*

            **‚úÖ GOOD - Verify first:**
            > *(Read enough context to confirm it's actually a problem before flagging)*

            ---

            ## Pull Request Details

            - **Repository:** ${{ github.repository }}
            - **PR #${process.env.PR_NUMBER}:** ${process.env.PR_TITLE}
            - **Author:** @${process.env.PR_AUTHOR}

            ### PR Description

            \`\`\`
            ${process.env.PR_BODY}
            \`\`\`

            ### Commit Messages

            \`\`\`
            ${process.env.COMMITS}
            \`\`\`

            ### Recent CI Results

            ${process.env.CI_SUMMARY}

            ### Upstream workflow statuses

            ${process.env.NEEDS_SUMMARY}

            ## Unified Diff

            \`\`\`diff
            ${process.env.DIFF}
            \`\`\`

            ## Egregora-Specific Patterns & Architecture

            <claude_md>
            ${process.env.CLAUDE_MD}
            </claude_md>

            ---

            ## Output Format

            Structure your review to **separate understanding from evaluation**. Skip sections that don't apply.

            ### üéØ Intent & Context (Phase 1 Output)
            *(Skip for trivial PRs where intent is obvious)*

            **What the code does:**
            (2-3 sentences describing actual changes based on diff)

            **Stated intent:**
            (Summary from PR/commits, or "No description provided")

            **Steel-man:**
            (What's the STRONGEST case for this approach? 1-2 sentences)

            ---

            ### ‚úÖ Correctness Check
            *(Does it work? Combine with verdict for simple PRs)*

            - **Primary goal:** [‚úÖ Achieved / ‚ùå Fails / ‚ö†Ô∏è Partial] - Brief explanation
            - **Edge cases:** [‚úÖ Handled / ‚ùå Missing / N/A]

            ---

            ### üîç Critical Issues
            *(REQUIRED SECTION - cannot be skipped)*

            **If issues found:**
            - **file.py:45** - üî¥ Description + impact + fix suggestion

            **If none:**
            ‚úÖ No critical issues (privacy, security, data loss)

            ---

            ### ‚ö†Ô∏è Implementation Concerns
            *(Optional - skip if none)*

            - **file.py:90** - üü° Issue + why it matters + suggested alternative

            ---

            ### ‚úÖ Final Verdict

            **Recommendation:** [LGTM ‚úÖ / MERGE WITH MINOR FIXES ‚ö†Ô∏è / NEEDS CHANGES ‚ùå]

            **If not LGTM:** (Single most important action to take)

            **If LGTM:** (Optional: One sentence on what's good about this PR)

            **PR Description Quality:** [‚úÖ Clear / ‚ö†Ô∏è Vague / ‚ùå Missing]
            *(If not ‚úÖ, suggest what should be added for future PRs)*

            ---

            **Word budget:**
            - Trivial: 50-150 words
            - Small: 200-400 words
            - Medium: 400-800 words
            - Large: 800-1200 words MAX`;

            core.exportVariable('GEMINI_PROMPT', prompt);

      # ----------------------------------------------------------------------
      # Gemini Review Pipeline with Fallback Strategy
      # 1. Gemini 2.5 Pro (Best quality, high cost/quota)
      # 2. Gemini 2.5 Flash (Fast, lower cost/quota)
      # 3. Gemini 2.5 Flash Lite (Cheapest, lowest quota usage)
      # ----------------------------------------------------------------------

      - name: Run Gemini PR Review (Pro)
        id: gemini_pro
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-pro"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Run Gemini PR Review (Flash)
        id: gemini_flash
        if: steps.gemini_pro.outcome == 'failure'
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-flash"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Run Gemini PR Review (Flash Lite)
        id: gemini_lite
        if: steps.gemini_pro.outcome == 'failure' && steps.gemini_flash.outcome == 'failure'
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-flash-lite"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Consolidate Gemini Results
        id: gemini_final
        if: always()
        uses: actions/github-script@v8
        env:
          OUTCOME_PRO: ${{ steps.gemini_pro.outcome }}
          SUMMARY_PRO: ${{ steps.gemini_pro.outputs.summary }}
          OUTCOME_FLASH: ${{ steps.gemini_flash.outcome }}
          SUMMARY_FLASH: ${{ steps.gemini_flash.outputs.summary }}
          OUTCOME_LITE: ${{ steps.gemini_lite.outcome }}
          SUMMARY_LITE: ${{ steps.gemini_lite.outputs.summary }}
        with:
          script: |
            const outcomes = {
              pro: process.env.OUTCOME_PRO,
              flash: process.env.OUTCOME_FLASH,
              lite: process.env.OUTCOME_LITE
            };

            const summaries = {
              pro: process.env.SUMMARY_PRO,
              flash: process.env.SUMMARY_FLASH,
              lite: process.env.SUMMARY_LITE
            };

            let finalOutcome = 'failure';
            let finalSummary = '';
            let finalModel = 'unknown';

            if (outcomes.pro === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.pro;
              finalModel = 'gemini-2.5-pro';
            } else if (outcomes.flash === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.flash;
              finalModel = 'gemini-2.5-flash';
            } else if (outcomes.lite === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.lite;
              finalModel = 'gemini-2.5-flash-lite';
            }

            // Output the results safely
            core.setOutput('outcome', finalOutcome);
            core.setOutput('model', finalModel);

            // For summary, use direct setOutput which handles strings safely
            core.setOutput('summary', finalSummary);

            console.log(`Final Model: ${finalModel}`);
            console.log(`Final Outcome: ${finalOutcome}`);

      - name: Check Gemini step result
        if: always()
        run: |
          echo "Gemini pipeline outcome: ${{ steps.gemini_final.outputs.outcome }}"

          if [ "${{ steps.gemini_final.outputs.outcome }}" != "success" ]; then
            echo "::warning::All Gemini CLI attempts failed or were skipped"
            echo "::group::Debugging Information"
            echo "Job status: ${{ job.status }}"
            echo "Pro outcome: ${{ steps.gemini_pro.outcome }}"
            echo "Flash outcome: ${{ steps.gemini_flash.outcome }}"
            echo "Lite outcome: ${{ steps.gemini_lite.outcome }}"
            echo "::endgroup::"
            # Check if API key is set (without exposing it)
            if [ -z "${{ secrets.GEMINI_API_KEY }}" ]; then
              echo "::error::GEMINI_API_KEY secret is not set!"
            else
              echo "GEMINI_API_KEY is set (length: ${#GEMINI_API_KEY})"
            fi
          fi
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}

      - name: Post review as PR comment
        if: always()
        uses: actions/github-script@v8
        env:
          GEMINI_REVIEW: ${{ steps.gemini_final.outputs.summary }}
          GEMINI_OUTCOME: ${{ steps.gemini_final.outputs.outcome }}
          GEMINI_MODEL: ${{ steps.gemini_final.outputs.model }}
          PR_NUMBER: ${{ steps.pr.outputs.pr_number }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = parseInt(process.env.PR_NUMBER);
            const geminiOutcome = process.env.GEMINI_OUTCOME;
            const geminiModel = process.env.GEMINI_MODEL;

            let review = process.env.GEMINI_REVIEW;
            let body;

            // Add header with trigger mode info
            const triggerEmoji = 'ü§ñ';
            const triggerText = 'Automatic review after CI checks';

            if (geminiOutcome !== 'success' || !review) {
              // Gemini step failed - provide diagnostic info
              body = `## ‚ö†Ô∏è Gemini Code Review Failed

            The Gemini CLI review failed to complete after attempting all fallback models (Pro -> Flash -> Lite).

            Possible causes:
            - **API Key Issues:** The \`GEMINI_API_KEY\` secret may be missing or invalid
            - **Quota/Rate Limiting:** All models may be exhausted
            - **Timeout:** The review took longer than the 10-minute job timeout
            - **Context Window:** The PR diff may exceed the context window for all models

            ### Debugging Steps

            1. **Check Secrets:** Verify that \`GEMINI_API_KEY\` is set in repository secrets
            2. **Review Logs:** Check the [workflow run logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed error messages
            3. **Re-run:** Retry the workflow after addressing potential issues
            4. **Model Status:** Check [Google AI Studio](https://aistudio.google.com/) for model availability

            ---
            *${triggerText} ‚Ä¢ Step outcome: ${geminiOutcome}*`;
            } else {
              // Gemini step succeeded
              body = `## ${triggerEmoji} Gemini Code Review

            ${review}

            ---
            *${triggerText} ‚Ä¢ Generated by ${geminiModel} using official Google GitHub Action*`;
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });

            if (geminiOutcome === 'success') {
              console.log("‚úÖ Review posted successfully!");
            } else {
              console.log("‚ö†Ô∏è Diagnostic comment posted due to Gemini step failure");
            }

  docs:
    name: Build & Deploy Docs
    # Only run on main branch pushes after lint and tests pass
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: [lint, type-check, test-unit, test-e2e]
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required for GitHub Pages deployment
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Set up Python
        run: uv python install 3.12

      - name: Install dependencies
        run: |
          uv sync --extra docs

      - name: Build Documentation
        run: |
          uv run mkdocs build

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: 20

      - name: Generate Repomix bundles
        run: |
          mkdir -p /tmp/bundles
          npx repomix -c repomix-docs.json --output /tmp/bundles/docs.bundle.md
          npx repomix -c repomix-tests.json --output /tmp/bundles/tests.bundle.md
          npx repomix -c repomix-code.json --output /tmp/bundles/code.bundle.md
          mkdir -p site/bundles
          cp /tmp/bundles/*.bundle.md site/bundles/

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./site

      - name: Upload bundles artifact
        uses: actions/upload-artifact@v5
        with:
          name: bundles
          path: /tmp/bundles/
          retention-days: 7
