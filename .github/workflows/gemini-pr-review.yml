name: Gemini PR Code Review

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  issue_comment:
    types: [created]

# Allow concurrent runs - don't cancel in-progress Gemini reviews (they cost API credits)
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.event.issue.number }}
  cancel-in-progress: false

permissions:
  contents: read
  pull-requests: write

jobs:
  gemini-review:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    # Run if:
    # 1. It's a non-draft PR (automatic trigger)
    # 2. OR it's a comment on a PR containing @gemini (manual trigger)
    if: |
      (github.event_name == 'pull_request' && !github.event.pull_request.draft) ||
      (github.event_name == 'issue_comment' && github.event.issue.pull_request && contains(github.event.comment.body, '@gemini'))

    steps:
      - name: Get PR details
        id: pr
        uses: actions/github-script@v8
        with:
          script: |
            let prNumber, prData;

            if (context.eventName === 'issue_comment') {
              // Manual trigger via @gemini comment
              prNumber = context.issue.number;
              const { data: pr } = await github.rest.pulls.get({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: prNumber
              });
              prData = pr;

              // Extract any additional instructions after @gemini
              const match = context.payload.comment.body.match(/@gemini\s*(.*)/s);
              const userInstructions = match ? match[1].trim() : '';
              core.setOutput('user_instructions', userInstructions);
              core.setOutput('trigger_mode', 'manual');
            } else {
              // Automatic trigger on PR event
              prNumber = context.payload.pull_request.number;
              prData = context.payload.pull_request;
              core.setOutput('user_instructions', '');
              core.setOutput('trigger_mode', 'automatic');
            }

            core.setOutput('pr_number', prNumber);
            core.setOutput('base_sha', prData.base.sha);
            core.setOutput('base_ref', prData.base.ref);
            core.setOutput('head_sha', prData.head.sha);
            core.setOutput('pr_title', prData.title);
            core.setOutput('pr_author', prData.user.login);
            core.setOutput('pr_body', prData.body || '(No description provided)');

      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Collect PR diff and context
        id: collect
        env:
          BASE_SHA: ${{ steps.pr.outputs.base_sha }}
          BASE_REF: ${{ steps.pr.outputs.base_ref }}
          HEAD_SHA: ${{ steps.pr.outputs.head_sha }}
          PR_TITLE: ${{ steps.pr.outputs.pr_title }}
          PR_BODY: ${{ steps.pr.outputs.pr_body }}
          USER_INSTRUCTIONS: ${{ steps.pr.outputs.user_instructions }}
          TRIGGER_MODE: ${{ steps.pr.outputs.trigger_mode }}
        run: |
          set -euo pipefail

          # Ensure we have the base ref locally (works for forks and shallow clones)
          git fetch origin "${BASE_REF}" --depth=1

          # Get unified diff between base and head
          git diff --unified=3 "origin/${BASE_REF}" "${HEAD_SHA}" > pr.diff

          # Trim diff to avoid blowing the context window (90KB ~= 22k tokens)
          head -c 90000 pr.diff > pr-trimmed.diff

          # Warn if diff was truncated
          if [ $(wc -c < pr.diff) -gt 90000 ]; then
            echo "‚ö†Ô∏è WARNING: Diff truncated at 90KB. This is a large PR." >> pr-trimmed.diff
            echo "Review may miss issues in truncated sections." >> pr-trimmed.diff
          fi

          # Get commit messages to understand intent
          # Format: hash - subject + body (if any)
          git log --format="%h - %s%n%b" "origin/${BASE_REF}..${HEAD_SHA}" > commits.txt || echo "(No commits found)" > commits.txt

          # Read CLAUDE.md for project-specific context
          CLAUDE_MD=""
          if [ -f "CLAUDE.md" ]; then
            CLAUDE_MD=$(cat CLAUDE.md)
          fi

          # Output diff and context for the next step
          # Use unique delimiters to avoid conflicts with diff content
          DIFF_DELIMITER="ghadelimiter_diff_$(uuidgen)"
          CLAUDE_DELIMITER="ghadelimiter_claude_$(uuidgen)"
          COMMITS_DELIMITER="ghadelimiter_commits_$(uuidgen)"
          PR_BODY_DELIMITER="ghadelimiter_pr_body_$(uuidgen)"

          {
            echo "diff<<${DIFF_DELIMITER}"
            cat pr-trimmed.diff
            echo "${DIFF_DELIMITER}"
            echo "claude_md<<${CLAUDE_DELIMITER}"
            echo "$CLAUDE_MD"
            echo "${CLAUDE_DELIMITER}"
            echo "commits<<${COMMITS_DELIMITER}"
            cat commits.txt
            echo "${COMMITS_DELIMITER}"
            echo "pr_body<<${PR_BODY_DELIMITER}"
            echo "$PR_BODY"
            echo "${PR_BODY_DELIMITER}"
            echo "user_instructions=$USER_INSTRUCTIONS"
            echo "trigger_mode=$TRIGGER_MODE"
          } >> "$GITHUB_OUTPUT"

      # Construct the shared prompt dynamically after collection using JS for safety
      - name: Construct Gemini Prompt
        id: construct_prompt
        uses: actions/github-script@v8
        env:
          DIFF: ${{ steps.collect.outputs.diff }}
          CLAUDE_MD: ${{ steps.collect.outputs.claude_md }}
          COMMITS: ${{ steps.collect.outputs.commits }}
          PR_BODY: ${{ steps.collect.outputs.pr_body }}
          PR_TITLE: ${{ steps.pr.outputs.pr_title }}
          PR_AUTHOR: ${{ steps.pr.outputs.pr_author }}
          PR_NUMBER: ${{ steps.pr.outputs.pr_number }}
          USER_INSTRUCTIONS: ${{ steps.collect.outputs.user_instructions }}
          TRIGGER_MODE: ${{ steps.collect.outputs.trigger_mode }}
        with:
          script: |
            const prompt = `You are a senior software engineer and code reviewer for the **egregora** repository.

            ## Project Context

            Egregora is a privacy-first AI pipeline that extracts structured knowledge from unstructured communication.
            - **Stack:** Python 3.12+ | uv | Ibis | DuckDB | Pydantic-AI | Google Gemini
            - **Core Principle:** Privacy before intelligence (names ‚Üí UUIDs before LLM)
            - **Philosophy:** Alpha mindset‚Äîclean breaks over backward compatibility
            - **Architecture:** Three-layer functional (orchestration ‚Üí transformations/adapters ‚Üí data_primitives)

            ---

            ## Review Philosophy: Two-Phase Approach

            Your review has **TWO PHASES**. You MUST complete Phase 1 before Phase 2.

            ### Phase 1: Understanding (REQUIRED FIRST - take 2-5 minutes)

            Your first job is **understanding, not judging**. Ask yourself:

            1. **What changed?** (Read the diff first)
               - Which files? What type of change? (feature/fix/refactor/docs/config)
               - What do imports, function names, and structure tell you?
               - What will this code DO when executed?

            2. **What was the author trying to accomplish?**
               - Read PR title/description and commits (they're often vague or missing - that's OK)
               - What's the strongest interpretation of the code's intent?
               - What constraints might justify this approach?

            3. **Does stated intent match actual changes?**
               - If YES: Great, you understand the goal
               - If NO/PARTIAL: Infer intent from code (code is ground truth)
               - If description is empty: No problem, infer from code

            4. **Steel-man the approach**
               - What's a legitimate reason for doing it this way?
               - What might the author know that isn't obvious?
               - Assume competence unless proven otherwise

            **Phase 1 Output:** Write a 3-5 sentence summary of what this PR does and why.

            ### Phase 2: Evaluation (ONLY AFTER understanding)

            Now that you understand the goal, evaluate the execution:

            **Must check (in priority order):**
            1. **üî¥ Correctness** - Does it work? Are there bugs or logic errors?
            2. **üî¥ Safety** - Privacy bypasses? Security holes? Data loss risks?
            3. **üü° Quality** - Is the approach sound? Are there simpler alternatives?
            4. **üü° Patterns** - Follows egregora conventions? (see CLAUDE.md below)

            **Critical Mindset:**
            - **Different ‚â† Wrong** - Don't flag valid alternative approaches as issues
            - **Avoid nitpicking** - If it's not a bug, security risk, or clear pattern violation, skip it
            - **Trust the author** - They may know constraints you don't see
            - **When uncertain** - Say "Consider..." not "You must..."
            - **Goal:** Make the code better, not perfect

            **Tone:** Professional, direct, concise, actionable. No fluff or unnecessary praise.

            ---

            ## ‚öñÔ∏è Adaptive Review Depth

            Calibrate your review depth to the PR scope:

            **Trivial PRs** (1-10 lines, obvious fixes):
            - Skip Phase 1 if intent is obvious from code
            - 2-3 sentence review: "Changes X to fix Y. ‚úÖ Correct. No issues."
            - Only flag issues if genuinely problematic

            **Small PRs** (10-100 lines, single feature/fix):
            - Brief Phase 1 (2-3 sentences)
            - Focus on correctness and critical issues
            - Target: 200-400 words total

            **Medium PRs** (100-500 lines, typical feature):
            - Standard full review
            - Prioritize top 3-5 most important issues
            - Target: 400-800 words total

            **Large PRs** (500+ lines, major refactor):
            - Focus on architecture and blocking issues
            - Skip minor issues, focus on what could break
            - Suggest splitting if scope is mixed (refactor + new feature)
            - Target: 800-1200 words MAX

            ---

            ## üéØ Special PR Types

            **Documentation-only PRs:**
            - Focus on: accuracy, clarity, completeness, broken links
            - Skip: code quality checks, pattern compliance
            - Verdict: Does it improve understanding?

            **Dependency updates:**
            - Focus on: breaking changes, security fixes, compatibility
            - Check: CHANGELOG/release notes for major version bumps
            - Note any API changes that might affect our code

            **Generated/Lock files** (package-lock.json, migrations, poetry.lock):
            - Don't review line-by-line
            - Note: "Generated files - spot-checked for anomalies"
            - Only flag if something looks suspiciously wrong

            **Test-only changes:**
            - Focus on: test coverage, edge cases, assertion quality
            - Check: Do tests validate intended behavior?
            - Are test names clear about what they're testing?

            **Configuration changes** (.yml, .toml, .env.example):
            - Focus on: security implications, breaking changes
            - Check: Are defaults sensible? Is it documented?
            - Note any changes that affect deployment

            ---

            ## üìö Examples: Good vs Bad Feedback

            **‚ùå BAD - Vague and unhelpful:**
            > "This code could be better. Consider refactoring."

            **‚úÖ GOOD - Specific and actionable:**
            > "**auth.py:67** - üî¥ Password stored in plaintext. Use \`bcrypt.hashpw()\` before saving to DB."

            ---

            **‚ùå BAD - Nitpicking style:**
            > "**utils.py:23** - Should use list comprehension instead of for loop."

            **‚úÖ GOOD - Skip it unless it matters:**
            > *(Don't mention style preferences unless they cause bugs)*

            ---

            **‚ùå BAD - Prescriptive without understanding:**
            > "**models.py:45** - Must use Pydantic models, not dicts."

            **‚úÖ GOOD - Understand the tradeoff:**
            > "**models.py:45** - üü° Using dict instead of Pydantic. If this is for performance in a hot path, add a comment explaining the tradeoff. Otherwise, Pydantic would add type safety."

            ---

            **‚ùå BAD - False positive:**
            > "**test_auth.py:12** - Missing error handling for network failures."
            > *(It's a test with mocked network...)*

            **‚úÖ GOOD - Verify first:**
            > *(Read enough context to confirm it's actually a problem before flagging)*

            ---

            ## Pull Request Details

            - **Repository:** ${{ github.repository }}
            - **PR #${process.env.PR_NUMBER}:** ${process.env.PR_TITLE}
            - **Author:** @${process.env.PR_AUTHOR}

            **Trigger Mode:** ${process.env.TRIGGER_MODE}
            ${process.env.USER_INSTRUCTIONS ? `**User Request:** ${process.env.USER_INSTRUCTIONS}` : ''}

            ### PR Description

            \`\`\`
            ${process.env.PR_BODY}
            \`\`\`

            ### Commit Messages

            \`\`\`
            ${process.env.COMMITS}
            \`\`\`

            ## Unified Diff

            \`\`\`diff
            ${process.env.DIFF}
            \`\`\`

            ## Egregora-Specific Patterns & Architecture

            <claude_md>
            ${process.env.CLAUDE_MD}
            </claude_md>

            ---

            ## Output Format

            Structure your review to **separate understanding from evaluation**. Skip sections that don't apply.

            ### üéØ Intent & Context (Phase 1 Output)
            *(Skip for trivial PRs where intent is obvious)*

            **What the code does:**
            (2-3 sentences describing actual changes based on diff)

            **Stated intent:**
            (Summary from PR/commits, or "No description provided")

            **Steel-man:**
            (What's the STRONGEST case for this approach? 1-2 sentences)

            ---

            ### ‚úÖ Correctness Check
            *(Does it work? Combine with verdict for simple PRs)*

            - **Primary goal:** [‚úÖ Achieved / ‚ùå Fails / ‚ö†Ô∏è Partial] - Brief explanation
            - **Edge cases:** [‚úÖ Handled / ‚ùå Missing / N/A]

            ---

            ### üîç Critical Issues
            *(REQUIRED SECTION - cannot be skipped)*

            **If issues found:**
            - **file.py:45** - üî¥ Description + impact + fix suggestion

            **If none:**
            ‚úÖ No critical issues (privacy, security, data loss)

            ---

            ### ‚ö†Ô∏è Implementation Concerns
            *(Optional - skip if none)*

            - **file.py:90** - üü° Issue + why it matters + suggested alternative

            ---

            ### ‚úÖ Final Verdict

            **Recommendation:** [LGTM ‚úÖ / MERGE WITH MINOR FIXES ‚ö†Ô∏è / NEEDS CHANGES ‚ùå]

            **If not LGTM:** (Single most important action to take)

            **If LGTM:** (Optional: One sentence on what's good about this PR)

            **PR Description Quality:** [‚úÖ Clear / ‚ö†Ô∏è Vague / ‚ùå Missing]
            *(If not ‚úÖ, suggest what should be added for future PRs)*

            ---

            **Word budget:**
            - Trivial: 50-150 words
            - Small: 200-400 words
            - Medium: 400-800 words
            - Large: 800-1200 words MAX`;

            core.exportVariable('GEMINI_PROMPT', prompt);

      # ----------------------------------------------------------------------
      # Gemini Review Pipeline with Fallback Strategy
      # 1. Gemini 2.5 Pro (Best quality, high cost/quota)
      # 2. Gemini 2.5 Flash (Fast, lower cost/quota)
      # 3. Gemini 2.5 Flash Lite (Cheapest, lowest quota usage)
      # ----------------------------------------------------------------------

      - name: Run Gemini PR Review (Pro)
        id: gemini_pro
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-pro"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Run Gemini PR Review (Flash)
        id: gemini_flash
        if: steps.gemini_pro.outcome == 'failure'
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-flash"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Run Gemini PR Review (Flash Lite)
        id: gemini_lite
        if: steps.gemini_pro.outcome == 'failure' && steps.gemini_flash.outcome == 'failure'
        continue-on-error: true
        uses: google-github-actions/run-gemini-cli@v0
        with:
          gemini_api_key: ${{ secrets.GEMINI_API_KEY }}
          gemini_model: "gemini-2.5-flash-lite"
          prompt: ${{ env.GEMINI_PROMPT }}

      - name: Consolidate Gemini Results
        id: gemini_final
        if: always()
        uses: actions/github-script@v8
        env:
          OUTCOME_PRO: ${{ steps.gemini_pro.outcome }}
          SUMMARY_PRO: ${{ steps.gemini_pro.outputs.summary }}
          OUTCOME_FLASH: ${{ steps.gemini_flash.outcome }}
          SUMMARY_FLASH: ${{ steps.gemini_flash.outputs.summary }}
          OUTCOME_LITE: ${{ steps.gemini_lite.outcome }}
          SUMMARY_LITE: ${{ steps.gemini_lite.outputs.summary }}
        with:
          script: |
            const outcomes = {
              pro: process.env.OUTCOME_PRO,
              flash: process.env.OUTCOME_FLASH,
              lite: process.env.OUTCOME_LITE
            };

            const summaries = {
              pro: process.env.SUMMARY_PRO,
              flash: process.env.SUMMARY_FLASH,
              lite: process.env.SUMMARY_LITE
            };

            let finalOutcome = 'failure';
            let finalSummary = '';
            let finalModel = 'unknown';

            if (outcomes.pro === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.pro;
              finalModel = 'gemini-2.5-pro';
            } else if (outcomes.flash === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.flash;
              finalModel = 'gemini-2.5-flash';
            } else if (outcomes.lite === 'success') {
              finalOutcome = 'success';
              finalSummary = summaries.lite;
              finalModel = 'gemini-2.5-flash-lite';
            }

            // Output the results safely
            core.setOutput('outcome', finalOutcome);
            core.setOutput('model', finalModel);

            // For summary, use direct setOutput which handles strings safely
            core.setOutput('summary', finalSummary);

            console.log(`Final Model: ${finalModel}`);
            console.log(`Final Outcome: ${finalOutcome}`);

      - name: Check Gemini step result
        if: always()
        run: |
          echo "Gemini pipeline outcome: ${{ steps.gemini_final.outputs.outcome }}"

          if [ "${{ steps.gemini_final.outputs.outcome }}" != "success" ]; then
            echo "::warning::All Gemini CLI attempts failed or were skipped"
            echo "::group::Debugging Information"
            echo "Job status: ${{ job.status }}"
            echo "Pro outcome: ${{ steps.gemini_pro.outcome }}"
            echo "Flash outcome: ${{ steps.gemini_flash.outcome }}"
            echo "Lite outcome: ${{ steps.gemini_lite.outcome }}"
            echo "::endgroup::"
            # Check if API key is set (without exposing it)
            if [ -z "${{ secrets.GEMINI_API_KEY }}" ]; then
              echo "::error::GEMINI_API_KEY secret is not set!"
            else
              echo "GEMINI_API_KEY is set (length: ${#GEMINI_API_KEY})"
            fi
          fi
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}

      - name: Post review as PR comment
        if: always()
        uses: actions/github-script@v8
        env:
          GEMINI_REVIEW: ${{ steps.gemini_final.outputs.summary }}
          GEMINI_OUTCOME: ${{ steps.gemini_final.outputs.outcome }}
          GEMINI_MODEL: ${{ steps.gemini_final.outputs.model }}
          PR_NUMBER: ${{ steps.pr.outputs.pr_number }}
          TRIGGER_MODE: ${{ steps.collect.outputs.trigger_mode }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = parseInt(process.env.PR_NUMBER);
            const triggerMode = process.env.TRIGGER_MODE;
            const geminiOutcome = process.env.GEMINI_OUTCOME;
            const geminiModel = process.env.GEMINI_MODEL;

            let review = process.env.GEMINI_REVIEW;
            let body;

            // Add header with trigger mode info
            const triggerEmoji = triggerMode === 'manual' ? 'üëã' : 'ü§ñ';
            const triggerText = triggerMode === 'manual' ? 'Manual review requested' : 'Automatic review';

            if (geminiOutcome !== 'success' || !review) {
              // Gemini step failed - provide diagnostic info
              body = `## ‚ö†Ô∏è Gemini Code Review Failed

            The Gemini CLI review failed to complete after attempting all fallback models (Pro -> Flash -> Lite).

            Possible causes:
            - **API Key Issues:** The \`GEMINI_API_KEY\` secret may be missing or invalid
            - **Quota/Rate Limiting:** All models may be exhausted
            - **Timeout:** The review took longer than the 10-minute job timeout
            - **Context Window:** The PR diff may exceed the context window for all models

            ### Debugging Steps

            1. **Check Secrets:** Verify that \`GEMINI_API_KEY\` is set in repository secrets
            2. **Review Logs:** Check the [workflow run logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed error messages
            3. **Manual Trigger:** Try commenting \`@gemini\` on this PR to re-trigger the review
            4. **Model Status:** Check [Google AI Studio](https://aistudio.google.com/) for model availability

            ---
            *${triggerText} ‚Ä¢ Step outcome: ${geminiOutcome}*`;
            } else {
              // Gemini step succeeded
              body = `## ${triggerEmoji} Gemini Code Review

            ${review}

            ---
            *${triggerText} ‚Ä¢ Generated by ${geminiModel} using official Google GitHub Action*`;
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });

            if (geminiOutcome === 'success') {
              console.log("‚úÖ Review posted successfully!");
            } else {
              console.log("‚ö†Ô∏è Diagnostic comment posted due to Gemini step failure");
            }
