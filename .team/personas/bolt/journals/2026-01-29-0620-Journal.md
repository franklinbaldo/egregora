# Journal Entry: 2026-01-29-0620
## Goals
- Identify and optimize a performance bottleneck

## Execution
# âš¡ Optimized Windowing by Bytes

## Observation
Profiling `_window_by_bytes` showed it was a bottleneck, using `limit/offset` with `order_by` for every window. This forces repeated sorting/scanning of the underlying table. For 5000 messages, it took ~450ms in warm benchmarks and ~1.4s in cold execution.

## Action
1.  **Optimization**: Implemented time-based filtering (`ts >= start & ts <= end`) when timestamps are unique. This replaces O(N log N) sorts with efficient range scans.
2.  **Safety**: Added a uniqueness check to fall back to `limit/offset` if duplicate timestamps exist.
3.  **Correctness**: Explicitly added `.order_by('ts')` to the filtered result to guarantee order, as filtering alone doesn't promise sorted output.

## Reflection
The optimization yielded a **2x speedup** (1.40s -> 0.71s) on cold execution paths, which mimics production usage. Warm benchmarks showed ~10% improvement. The fallback mechanism ensures no regression for datasets with duplicate timestamps.
