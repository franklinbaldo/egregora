diff --git a/.jules/curator.md b/.jules/curator.md
deleted file mode 100644
index ab33e2a0b..000000000
--- a/.jules/curator.md
+++ /dev/null
@@ -1,13 +0,0 @@
-# Curator's Journal
-
-## 2024-07-25 - Initial Evaluation & Blockers
-
-**Observation:** The project's tooling for generating a demo site is currently broken. The `egregora demo` command does not exist, and the documented `egregora write` command fails consistently due to API rate-limiting and configuration issues. I was unable to generate a live, content-rich site for a full interactive UX evaluation.
-
-**Why It Matters:** A reliable demo generation process is critical for effective UX curation. Without it, I am forced to rely on static analysis of templates and minimal generated files, which provides an incomplete picture. This significantly slows down the feedback loop and makes it harder to assess the end-to-end user experience.
-
-**Decision & Workaround:** I pivoted from a live review to a static analysis. I initialized a site, manually created a missing `overrides` directory, and built a minimal version with no generated content. This allowed me to inspect the site's "chrome" (layout, navigation, basic styling) and identify foundational issues like the use of a generic default theme. While not ideal, this workaround unblocked my ability to provide initial, high-impact feedback.
-
-**Recommendation:** The highest priority for the Forge persona should be to fix the demo generation pipeline. A stable `egregora demo` command that works reliably out-of-the-box is a prerequisite for efficient UX development. My initial tasks focus on foundational brand and readability improvements that can be implemented and verified even with a minimal site, but future, more nuanced evaluations will depend on a working content pipeline.
-
-**Insight:** The dependency on external APIs for the core content generation loop is a significant point of failure. The system should be resilient to these failures, perhaps by having a "fallback" mode that generates a site with placeholder content for UX review, rather than crashing the entire process.
diff --git a/.jules/sheriff.md b/.jules/sheriff.md
deleted file mode 100644
index 9516dd5d9..000000000
--- a/.jules/sheriff.md
+++ /dev/null
@@ -1,3 +0,0 @@
-## 2025-05-23 - Property-Based Testing Timeout
-**Crime:** `tests/v3/core/test_types_property.py::test_feed_xml_validity` fails with `FailedHealthCheck: Input generation is slow`. Hypothesis is taking too long to generate valid `Feed` objects, likely due to complex nested strategies or expensive validation logic during generation.
-**Verdict:** Suppress the `HealthCheck.too_slow` for this specific test, or optimize the `feed_strategy`. Given the complexity of the `Feed` object (nested entries, authors, etc.), suppressing the check is a valid first step to stabilize the build, provided we also verify the strategy isn't accidentally generating trivial data.
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index 514a0febb..81a6dbf97 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -30,34 +30,6 @@ repos:
         language: system
         types: [python]
         pass_filenames: false
-      - id: radon-cc
-        name: Check cyclomatic complexity with radon
-        entry: uvx radon cc
-        args: ["src", "-n", "C", "--total-average"]
-        language: system
-        types: [python]
-        pass_filenames: false
-      - id: radon-mi
-        name: Check maintainability index with radon
-        entry: uvx radon mi
-        args: ["src", "-n", "B"]
-        language: system
-        types: [python]
-        pass_filenames: false
-      - id: xenon
-        name: Enforce complexity thresholds with xenon
-        entry: uvx xenon
-        args: ["src", "--max-absolute", "E", "--max-modules", "C", "--max-average", "B", "--exclude", "*/database/*.py"]
-        language: system
-        types: [python]
-        pass_filenames: false
-      - id: bandit
-        name: Check security issues with bandit
-        entry: uvx bandit
-        args: ["-r", "src", "-f", "screen", "-c", "pyproject.toml", "-lll", "-ii"]
-        language: system
-        types: [python]
-        pass_filenames: false
       - id: check-private-imports
         name: Check private imports
         entry: dev_tools/check_private_imports.py
@@ -82,11 +54,3 @@ repos:
         language: system
         pass_filenames: false
         stages: [pre-push]
-      - id: coverage
-        name: Check test coverage
-        entry: uv run pytest
-        args: ["tests/unit/", "--cov=src/egregora", "--cov-report=term-missing", "--cov-fail-under=70", "-q"]
-        language: system
-        types: [python]
-        pass_filenames: false
-        stages: [pre-push]
diff --git a/docs/ux-vision.md b/docs/ux-vision.md
deleted file mode 100644
index 8a46d6b73..000000000
--- a/docs/ux-vision.md
+++ /dev/null
@@ -1,11 +0,0 @@
-# UX Vision
-
-This document outlines the user experience (UX) vision for the generated blogs.
-
-## Template Architecture
-
-The MkDocs templates are located in the following directory:
-
-`src/egregora/rendering/templates/site/`
-
-This directory contains the Jinja2 templates for `mkdocs.yml`, theme overrides, and content pages. All frontend changes should be made to these templates.
diff --git a/pyproject.toml b/pyproject.toml
index e1f1ba1ff..8466066cc 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -323,18 +323,3 @@ min_confidence = 80
 sort_by_size = true
 exclude = []
 ignore_names = []
-
-[tool.bandit]
-targets = ["src"]
-skips = ["B101", "B608", "B701"]  # B101: assert_used, B608: hardcoded_sql (DuckDB), B701: jinja2_autoescape (SQL/prompts, not HTML)
-exclude_dirs = ["tests", ".venv", "build"]
-
-[tool.coverage.run]
-source = ["src/egregora"]
-omit = ["*/tests/*", "*/__pycache__/*", "*/site-packages/*"]
-
-[tool.coverage.report]
-fail_under = 70
-show_missing = true
-skip_covered = false
-precision = 2
diff --git a/src/egregora/agents/writer.py b/src/egregora/agents/writer.py
index 49d0b6e44..accdc8bd7 100644
--- a/src/egregora/agents/writer.py
+++ b/src/egregora/agents/writer.py
@@ -37,8 +37,10 @@
     build_conversation_xml,
     load_journal_memory,
 )
-from egregora.agents.types import (
+from egregora.agents.model_limits import (
     PromptTooLargeError,
+)
+from egregora.agents.types import (
     WriterDeps,
     WriterResources,
 )
@@ -476,7 +478,11 @@ def _prepare_deps(
 
 @sleep_and_retry
 @limits(calls=100, period=60)
+<<<<<<< HEAD
 async def write_posts_with_pydantic_agent(
+=======
+def write_posts_with_pydantic_agent(
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
     *,
     prompt: str,
     config: EgregoraConfig,
@@ -491,7 +497,11 @@ async def write_posts_with_pydantic_agent(
         caps_list = ", ".join(capability.name for capability in active_capabilities)
         logger.info("Writer capabilities enabled: %s", caps_list)
 
+<<<<<<< HEAD
     model = await create_writer_model(config, context, prompt, test_model)
+=======
+    model = create_writer_model(config, context, prompt, test_model)
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
     agent = setup_writer_agent(model, prompt, active_capabilities)
 
     if context.resources.quota:
@@ -507,8 +517,13 @@ async def write_posts_with_pydantic_agent(
     # Use tenacity for retries
     for attempt in Retrying(stop=RETRY_STOP, wait=RETRY_WAIT, retry=RETRY_IF, reraise=True):
         with attempt:
+<<<<<<< HEAD
             # Execute model directly without tools
             result = await agent.run(
+=======
+            # DIRECT SYNC CALL
+            result = agent.run_sync(
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
                 "Analyze the conversation context provided and write posts/profiles as needed.",
                 deps=context,
                 usage_limits=usage_limits,
@@ -734,7 +749,11 @@ def _build_context_and_signature(
     return writer_context, signature
 
 
+<<<<<<< HEAD
 async def _execute_writer_with_error_handling(
+=======
+def _execute_writer_with_error_handling(
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
     prompt: str,
     config: EgregoraConfig,
     deps: WriterDeps,
@@ -750,7 +769,11 @@ async def _execute_writer_with_error_handling(
 
     """
     try:
+<<<<<<< HEAD
         return await write_posts_with_pydantic_agent(
+=======
+        return write_posts_with_pydantic_agent(
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
             prompt=prompt,
             config=config,
             context=deps,
@@ -816,7 +839,11 @@ class WindowProcessingParams:
     run_id: str | None = None
 
 
+<<<<<<< HEAD
 async def write_posts_for_window(params: WindowProcessingParams) -> dict[str, list[str]]:
+=======
+def write_posts_for_window(params: WindowProcessingParams) -> dict[str, list[str]]:
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
     """Let LLM analyze window's messages, write 0-N posts, and update author profiles.
 
     This acts as the public entry point, orchestrating the setup and execution
@@ -854,6 +881,7 @@ async def write_posts_for_window(params: WindowProcessingParams) -> dict[str, li
         resources.usage,
     )
     if cached_result:
+<<<<<<< HEAD
         # Validate cached posts still exist on disk (they may be missing if output dir is fresh)
         cached_posts = cached_result.get(RESULT_KEY_POSTS, [])
         if cached_posts:
@@ -881,6 +909,9 @@ async def write_posts_for_window(params: WindowProcessingParams) -> dict[str, li
         else:
             # No posts in cache, just return the empty result
             return cached_result
+=======
+        return cached_result
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
 
     logger.info("Using Pydantic AI backend for writer")
 
@@ -911,11 +942,20 @@ async def write_posts_for_window(params: WindowProcessingParams) -> dict[str, li
 
     prompt = _render_writer_prompt(writer_context, deps.resources.prompts_dir)
 
+<<<<<<< HEAD
     if getattr(params.config.pipeline, "economic_mode", False):
         logger.info("ðŸ’° Economic Mode enabled: Using simple generation (no tools)")
         saved_posts, saved_profiles = await _execute_economic_writer(prompt, params.config, deps)
     else:
         saved_posts, saved_profiles = await _execute_writer_with_error_handling(prompt, params.config, deps)
+=======
+    # Check for economic mode
+    if getattr(params.config.pipeline, "economic_mode", False):
+        logger.info("ðŸ’° Economic Mode enabled: Using simple generation (no tools)")
+        saved_posts, saved_profiles = _execute_economic_writer(prompt, params.config, deps)
+    else:
+        saved_posts, saved_profiles = _execute_writer_with_error_handling(prompt, params.config, deps)
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
 
     # 6. Finalize results (output, RAG indexing, caching)
     return _finalize_writer_results(
@@ -930,7 +970,11 @@ async def write_posts_for_window(params: WindowProcessingParams) -> dict[str, li
     )
 
 
+<<<<<<< HEAD
 async def _execute_economic_writer(
+=======
+def _execute_economic_writer(
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
     prompt: str,
     config: EgregoraConfig,
     deps: WriterDeps,
diff --git a/src/egregora/agents/writer_helpers.py b/src/egregora/agents/writer_helpers.py
index 160b0db5b..cbc5145e2 100644
--- a/src/egregora/agents/writer_helpers.py
+++ b/src/egregora/agents/writer_helpers.py
@@ -9,13 +9,16 @@
 from pydantic_ai import Agent, RunContext
 
 from egregora.agents.capabilities import AgentCapability
-
-# model_limits removed in favor of Pydantic-AI native limits
-# and shared models in types.py
+from egregora.agents.model_limits import (
+    PromptTooLargeError,
+    get_model_context_limit,
+)
+from egregora.agents.model_limits import (
+    validate_prompt_fits as _validate_prompt_fits,
+)
 from egregora.agents.types import (
     AnnotationResult,
     PostMetadata,
-    PromptTooLargeError,
     ReadProfileResult,
     WritePostResult,
     WriteProfileResult,
@@ -212,11 +215,12 @@ def load_profiles_context(active_authors: list[str], output_sink: Any) -> str:
     return profiles_context
 
 
-async def validate_prompt_fits(
+def validate_prompt_fits(
     prompt: str,
     model_name: str,
     config: EgregoraConfig,
     window_label: str,
+<<<<<<< HEAD
     *,
     model_instance: Any | None = None,
 ) -> int:
@@ -254,3 +258,35 @@ async def count_tokens(prompt: str, model: Any | None = None) -> int:
 
     # Fallback to conservative estimation (4 chars per token)
     return len(prompt) // 4
+=======
+) -> None:
+    """Validate prompt fits within model context window limits."""
+    max_prompt_tokens = getattr(config.pipeline, "max_prompt_tokens", 100_000)
+    use_full_context_window = getattr(config.pipeline, "use_full_context_window", False)
+
+    fits, estimated_tokens, _effective_limit = _validate_prompt_fits(
+        prompt,
+        model_name,
+        max_prompt_tokens=max_prompt_tokens,
+        use_full_context_window=use_full_context_window,
+    )
+
+    if not fits:
+        model_limit = get_model_context_limit(model_name)
+        model_effective_limit = int(model_limit * 0.9)
+
+        if estimated_tokens > model_effective_limit:
+            logger.error(
+                "Prompt exceeds limit: %d > %d for %s (window: %s)",
+                estimated_tokens,
+                model_effective_limit,
+                model_name,
+                window_label,
+            )
+            raise PromptTooLargeError(
+                estimated_tokens=estimated_tokens,
+                effective_limit=model_effective_limit,
+                model_name=model_name,
+                window_id=window_label,
+            )
+>>>>>>> origin/claude/merge-compatible-prs-8oM9f
diff --git a/src/egregora/agents/writer_setup.py b/src/egregora/agents/writer_setup.py
index 65afccde9..15e4d85d2 100644
--- a/src/egregora/agents/writer_setup.py
+++ b/src/egregora/agents/writer_setup.py
@@ -42,7 +42,7 @@ def configure_writer_capabilities(
     return capabilities
 
 
-async def create_writer_model(
+def create_writer_model(
     config: EgregoraConfig,
     context: WriterDeps,
     prompt: str,
@@ -54,9 +54,7 @@ async def create_writer_model(
 
     model = create_fallback_model(config.models.writer, use_google_batch=False)
     # Validate prompt fits (only check for real models)
-    await validate_prompt_fits(
-        prompt, config.models.writer, config, context.window_label, model_instance=model
-    )
+    validate_prompt_fits(prompt, config.models.writer, config, context.window_label)
     return model
 
 
diff --git a/src/egregora/config/settings.py b/src/egregora/config/settings.py
index b581d4078..111f26758 100644
--- a/src/egregora/config/settings.py
+++ b/src/egregora/config/settings.py
@@ -47,9 +47,9 @@
 # Constants
 # ============================================================================
 
-DEFAULT_MODEL = "google-gla:gemini-2.5-flash"  # Use latest stable model (pydantic-ai format)
+DEFAULT_MODEL = "google-gla:gemini-2.0-flash-exp"  # Standardize on a valid existing model
 DEFAULT_EMBEDDING_MODEL = "models/gemini-embedding-001"
-DEFAULT_BANNER_MODEL = "models/gemini-2.5-flash"  # (google-sdk format uses models/ prefix via validator)
+DEFAULT_BANNER_MODEL = "models/gemini-2.0-flash-exp"
 EMBEDDING_DIM = 768  # Embedding vector dimensions
 
 # Quota defaults
diff --git a/src/egregora/constants.py b/src/egregora/constants.py
index f8bac7672..ec6fc1d21 100644
--- a/src/egregora/constants.py
+++ b/src/egregora/constants.py
@@ -48,6 +48,20 @@ class WindowUnit(str, Enum):
     BYTES = "bytes"
 
 
+# Gemini model context limits (input tokens)
+# Source: https://ai.google.dev/gemini-api/docs/models/gemini
+KNOWN_MODEL_LIMITS = {
+    # Gemini 2.0 family
+    "gemini-2.0-flash-exp": 1_048_576,  # 1M tokens
+    "gemini-1.5-flash-8b": 1_048_576,  # 1M tokens
+    "gemini-1.5-pro": 2_097_152,  # 2M tokens
+    "gemini-1.5-pro-latest": 2_097_152,  # 2M tokens
+    # Gemini 1.0 family (older, smaller limits)
+    "gemini-pro": 32_768,  # 32k tokens
+    "gemini-1.0-pro": 32_768,  # 32k tokens
+    # Embeddings
+    "text-embedding-004": 2048,  # 2k tokens (for embeddings, not generation)
+}
 class MediaType(str, Enum):
     """Media content types."""
 
diff --git a/src/egregora/orchestration/pipelines/write.py b/src/egregora/orchestration/pipelines/write.py
index e6ff2f203..875b9dc8e 100644
--- a/src/egregora/orchestration/pipelines/write.py
+++ b/src/egregora/orchestration/pipelines/write.py
@@ -35,9 +35,9 @@
 from egregora.agents.avatar import AvatarContext, process_avatar_commands
 from egregora.agents.banner.worker import BannerWorker
 from egregora.agents.enricher import EnrichmentRuntimeContext, EnrichmentWorker, schedule_enrichment
+from egregora.agents.model_limits import PromptTooLargeError
 from egregora.agents.profile.worker import ProfileWorker
 from egregora.agents.shared.annotations import AnnotationStore
-from egregora.agents.types import PromptTooLargeError
 from egregora.agents.writer import WindowProcessingParams, write_posts_for_window
 from egregora.config import RuntimeContext, load_egregora_config
 from egregora.config.settings import EgregoraConfig, parse_date_arg, validate_timezone
@@ -652,7 +652,7 @@ def _process_single_window(
         adapter_generation_instructions=adapter_instructions,
         run_id=str(ctx.run_id) if ctx.run_id else None,
     )
-    result = run_async_safely(write_posts_for_window(params))
+    result = write_posts_for_window(params)
 
     posts = result.get("posts", [])
     profiles = result.get("profiles", [])
diff --git a/src/egregora/output_adapters/conventions.py b/src/egregora/output_adapters/conventions.py
index ad5283ee7..c6f8a60e7 100644
--- a/src/egregora/output_adapters/conventions.py
+++ b/src/egregora/output_adapters/conventions.py
@@ -1,3 +1,33 @@
+"""Standard URL conventions for Egregora output adapters.
+
+SEPARATION OF CONCERNS (2025-11-29):
+=====================================
+
+This module implements UrlConvention protocol - PURELY LOGICAL URL GENERATION.
+
+What UrlConvention does:
+- Given a Document, return what URL readers should use
+- Pure string manipulation only
+- No filesystem knowledge (no Path, no docs_dir, no file extensions as filesystem concept)
+- Uses only doc.type, slug, tags, date metadata
+
+What UrlConvention does NOT do:
+- Filesystem path resolution (that's OutputAdapter's job)
+- File layout decisions (index.md vs foo.md)
+- Directory structure (docs/, media/, etc.)
+
+Examples:
+    >>> convention = StandardUrlConvention()
+    >>> ctx = UrlContext(base_url="https://example.com", site_prefix="blog")
+    >>> doc = Document(type=DocumentType.POST, metadata={"slug": "hello", "date": "2025-01-10"})
+    >>> convention.canonical_url(doc, ctx)
+    'https://example.com/blog/posts/2025-01-10-hello/'
+
+The OutputAdapter then converts this URL to a filesystem path:
+    >>> adapter.persist(doc)  # Internally: URL -> Path("docs/posts/2025-01-10-hello.md")
+
+"""
+
 from __future__ import annotations
 
 from dataclasses import dataclass
@@ -12,161 +42,298 @@
     from egregora.data_primitives.protocols import UrlContext
 
 
+EXPECTED_PARTS_WITH_PATH = 2
+
+
 def _remove_url_extension(url_path: str) -> str:
-    """Remove extension from the last segment of a URL path, preserving dotfiles."""
+    """Remove file extension from URL path segment.
+
+    This is URL logic (removing trailing .html, .md, etc. from URLs),
+    not filesystem logic (Path.with_suffix). URLs may contain dots
+    that aren't extensions, so we only remove extensions from the
+    last path segment.
+
+    Dotfiles (files starting with a dot like '.config') are preserved
+    as they don't have an extension to remove.
+
+    Args:
+        url_path: URL path like 'media/images/foo.png' or 'posts/bar'
+
+    Returns:
+        URL path without extension: 'media/images/foo' or 'posts/bar'
+
+    Examples:
+        >>> _remove_url_extension("media/images/foo.png")
+        'media/images/foo'
+        >>> _remove_url_extension("posts/bar")
+        'posts/bar'
+        >>> _remove_url_extension("some.dir/file.md")
+        'some.dir/file'
+        >>> _remove_url_extension(".config")
+        '.config'
+        >>> _remove_url_extension("path/.gitignore")
+        'path/.gitignore'
+
+    """
+    if "." not in url_path:
+        return url_path
+
+    # Split on last slash to get the last segment
     parts = url_path.rsplit("/", 1)
-    filename = parts[-1]
-    if "." in filename and not filename.startswith("."):
-        parts[-1] = filename.rsplit(".", 1)[0]
-    return "/".join(parts)
-
 
-@dataclass(frozen=True)
+    if len(parts) == EXPECTED_PARTS_WITH_PATH and "." in parts[1]:
+        # Has a path and a filename with extension
+        # Remove extension from the filename only
+        basename_without_ext = parts[1].rsplit(".", 1)[0]
+        # Check if this is a dotfile (basename would be empty after split)
+        if not basename_without_ext:
+            # This is a dotfile, preserve it
+            return url_path
+        return f"{parts[0]}/{basename_without_ext}"
+    if "." in parts[0]:
+        # Just a filename with extension (no slashes)
+        basename_without_ext = parts[0].rsplit(".", 1)[0]
+        # Check if this is a dotfile (basename would be empty after split)
+        if not basename_without_ext:
+            # This is a dotfile, preserve it
+            return url_path
+        return basename_without_ext
+
+    return url_path
+
+
+@dataclass
 class RouteConfig:
+    """Configuration for URL routing segments."""
+
     posts_prefix: str = "posts"
     profiles_prefix: str = "profiles"
+    # ADR-001: Media goes inside posts directory
     media_prefix: str = "posts/media"
     journal_prefix: str = "journal"
     annotations_prefix: str = "posts/annotations"
+    # Defines if dates should be part of the URL structure: /2025-01-01-slug/ vs /slug/
     date_in_url: bool = True
 
 
 class StandardUrlConvention(UrlConvention):
-    name, version = "standard-v1", "1.1.0"
+    """The default, opinionated URL scheme for Egregora sites.
+
+    **Role: Single Source of Truth for Document Persistence**
+
+    This class is the **authoritative source** for how documents are addressed
+    and persisted to the filesystem. All document writes flow through the
+    `canonical_url()` method, which generates deterministic URLs based on
+    document type and metadata.
+
+    **Document Flow:**
+    ```
+    Document â†’ canonical_url() â†’ URL â†’ adapter._url_to_path() â†’ filesystem
+    ```
+
+    **Per-Type URL Rules:**
+    - PROFILE: /profiles/{uuid} (uses full UUID from metadata)
+    - POST: /posts/{slug} (filename includes date prefix: {date}-{slug}.md)
+    - JOURNAL: /journal/{label} (slugified window label)
+    - MEDIA: /media/{type}/{hash}.{ext} (hash-based naming)
+    - ENRICHMENT_MEDIA: /media/{type}/{parent_slug} (paired with media file)
+    - ENRICHMENT_URL: /media/urls/{identifier}/ (uses suggested_path if available)
+
+    **Contract with Adapters:**
+    - Adapters **must** use `canonical_url()` to generate URLs
+    - Adapters **must not** manually construct file paths
+    - URL â†’ path translation is adapter-specific (_url_to_path())
+    - This ensures consistency across all document references
+
+    **Configuration:**
+    - Route prefixes can be customized via `RouteConfig`
+    - Subclass to implement entirely different URL schemes
+    - Version tracked for migration/compatibility
+
+    See: ``docs/architecture/url-conventions.md`` for complete documentation.
+    """
 
     def __init__(self, routes: RouteConfig | None = None) -> None:
         self.routes = routes or RouteConfig()
 
-    def _join(self, ctx: UrlContext, *segments: str, trailing_slash: bool = True) -> str:
+    @property
+    def name(self) -> str:
+        return "standard-v1"
+
+    @property
+    def version(self) -> str:
+        return "1.0.0"
+
+    def _build_base(self, ctx: UrlContext) -> tuple[str, list[str]]:
         base = (ctx.base_url or "").rstrip("/")
         prefix = (ctx.site_prefix or "").strip("/")
+        segments: list[str] = []
+        if prefix:
+            segments.extend(prefix.split("/"))
+        return base, segments
 
-        # Build path segments filtering empty strings
-        all_parts = [p for p in prefix.split("/") if p] + [s.strip("/") for s in segments if s]
-        path = "/".join(all_parts)
-
+    def _join(self, ctx: UrlContext, *segments: str, trailing_slash: bool = True) -> str:
+        base, prefix_segments = self._build_base(ctx)
+        clean_segments = [seg.strip("/") for seg in segments if seg]
+        path_segments = prefix_segments + clean_segments
+        path = "/".join(path_segments)
+        # Restore leading slash to make paths root-relative when base is empty
         url = f"{base}/{path}" if base else f"/{path}"
-        return url.rstrip("/") + "/" if trailing_slash else url.rstrip("/")
+        if trailing_slash:
+            return url.rstrip("/") + "/"
+        return url
 
-    def _get_slug(self, doc: Document) -> str:
-        return slugify(doc.metadata.get("slug", doc.document_id[:8]))
-
-    def canonical_url(self, doc: Document, ctx: UrlContext) -> str:
+    def canonical_url(self, document: Document, ctx: UrlContext) -> str:
+        """Generate a canonical URL based on the standard convention."""
         handlers = {
-            DocumentType.POST: self._format_post,
-            DocumentType.PROFILE: self._format_profile,
-            DocumentType.ANNOUNCEMENT: self._format_announcement,
-            DocumentType.JOURNAL: self._format_journal,
-            DocumentType.MEDIA: self._format_media,
-            DocumentType.ENRICHMENT_URL: self._format_url_enrichment,
-            DocumentType.ANNOTATION: self._format_annotation,
-            DocumentType.ENRICHMENT_MEDIA: lambda c, d: self._format_enrichment(c, d),
-            DocumentType.ENRICHMENT_IMAGE: lambda c, d: self._format_enrichment(c, d, "images"),
-            DocumentType.ENRICHMENT_VIDEO: lambda c, d: self._format_enrichment(c, d, "videos"),
-            DocumentType.ENRICHMENT_AUDIO: lambda c, d: self._format_enrichment(c, d, "audio"),
+            DocumentType.POST: self._format_post_url,
+            DocumentType.PROFILE: self._format_profile_url,
+            DocumentType.JOURNAL: self._format_journal_url,
+            DocumentType.MEDIA: self._format_media_url,
+            DocumentType.ENRICHMENT_MEDIA: self._format_media_enrichment_url,
+            DocumentType.ENRICHMENT_IMAGE: lambda ctx, doc: self._format_typed_media_enrichment_url(
+                ctx, doc, "images"
+            ),
+            DocumentType.ENRICHMENT_VIDEO: lambda ctx, doc: self._format_typed_media_enrichment_url(
+                ctx, doc, "videos"
+            ),
+            DocumentType.ENRICHMENT_AUDIO: lambda ctx, doc: self._format_typed_media_enrichment_url(
+                ctx, doc, "audio"
+            ),
+            DocumentType.ENRICHMENT_URL: self._format_url_enrichment_url,
+            DocumentType.ANNOTATION: self._format_annotation_url,
         }
-        return handlers.get(doc.type, lambda c, d: self._join(c, "docs", d.document_id))(ctx, doc)
-
-    def _format_post(self, ctx: UrlContext, doc: Document) -> str:
-        slug = self._get_slug(doc)
-        if self.routes.date_in_url and (date_val := doc.metadata.get("date")):
-            date_str = date_val.date().isoformat() if isinstance(date_val, datetime) else str(date_val)[:10]
-            slug = f"{date_str}-{slug}"
-        return self._join(ctx, self.routes.posts_prefix, slug)
-
-    def _format_profile(self, ctx: UrlContext, doc: Document) -> str:
-        m = doc.metadata
-        uid = m.get("subject") or m.get("uuid") or m.get("author_uuid")
-        slug = slugify(m.get("slug") or m.get("profile_aspect") or doc.document_id[:8])
-        return (
-            self._join(ctx, self.routes.profiles_prefix, str(uid), slug)
-            if uid
-            else self._join(ctx, self.routes.posts_prefix, slug)
-        )
 
-    def _format_announcement(self, ctx: UrlContext, doc: Document) -> str:
-        """Format URL for ANNOUNCEMENT documents (user command events).
+        handler = handlers.get(document.type)
+        if handler:
+            return handler(ctx, document)
 
-        ANNOUNCEMENT documents with 'subject' metadata route to the author's profile feed:
-        /profiles/{subject_uuid}/{slug}/
+        # Fallback
+        return self._join(ctx, "documents", document.document_id)
 
-        This creates a unified feed showing both:
-        - PROFILE posts (Egregora's analyses)
-        - ANNOUNCEMENT posts (user actions/commands)
-        """
-        subject_uuid = doc.metadata.get("subject") or doc.metadata.get("actor")
+    def _format_profile_url(self, ctx: UrlContext, document: Document) -> str:
+        subject_uuid = (
+            document.metadata.get("subject")
+            or document.metadata.get("uuid")
+            or document.metadata.get("author_uuid")
+        )
+        slug_value = (
+            document.metadata.get("slug")
+            or document.metadata.get("profile_aspect")
+            or document.document_id[:8]
+        )
         if not subject_uuid:
-            # Fallback: route to announcements directory if no subject
-            slug = doc.metadata.get("slug", doc.document_id[:8])
-            return self._join(ctx, self.routes.posts_prefix, "announcements", slugify(slug))
-
-        # Route to author's profile feed
-        slug_value = doc.metadata.get("slug") or doc.document_id[:8]
+            return self._join(ctx, self.routes.posts_prefix, slugify(str(slug_value)))
         return self._join(ctx, self.routes.profiles_prefix, str(subject_uuid), slugify(str(slug_value)))
 
-    def _format_journal(self, ctx: UrlContext, doc: Document) -> str:
-        label = doc.metadata.get("window_label") or doc.metadata.get("slug")
-        return (
-            self._join(ctx, self.routes.journal_prefix, slugify(label))
-            if label
-            else self._join(ctx, self.routes.journal_prefix)
-        )
-
-    def _format_media(self, ctx: UrlContext, doc: Document) -> str:
-        if doc.suggested_path:
-            return self._join(ctx, doc.suggested_path, trailing_slash=False)
-
-        from egregora.ops.media import get_media_subfolder
-
-        # Prefer semantic slug, then filename, then document_id
-        slug_base = doc.metadata.get("slug")
-        fname = doc.metadata.get("filename", doc.document_id)
-
-        # Use the slug as the name if we have it, otherwise fallback to filename/ID
-        name_segment = slug_base or fname
-
-        # Ensure we have an extension if possible
-        ext = f".{fname.rsplit('.', 1)[-1]}" if "." in fname else ""
-        if not name_segment.endswith(ext) and ext:
-            name_segment = f"{name_segment}{ext}"
-
+    def _format_journal_url(self, ctx: UrlContext, document: Document) -> str:
+        window_label = document.metadata.get("window_label")
+        if window_label:
+            safe_label = slugify(window_label)
+            return self._join(ctx, self.routes.journal_prefix, safe_label)
+        slug_value = document.metadata.get("slug")
+        if slug_value:
+            safe_label = slugify(slug_value)
+            return self._join(ctx, self.routes.journal_prefix, safe_label)
+        # Fallback: no window_label or slug, unified output goes to posts/
+        return self._join(ctx, self.routes.posts_prefix)
+
+    def _format_url_enrichment_url(self, ctx: UrlContext, document: Document) -> str:
+        if document.suggested_path:
+            # Pure string manipulation - no Path operations
+            clean_path = _remove_url_extension(document.suggested_path.strip("/"))
+            return self._join(ctx, clean_path, trailing_slash=True)
+        url_slug = self._slug_with_identifier(document)
         return self._join(
-            ctx, self.routes.media_prefix, get_media_subfolder(ext), name_segment, trailing_slash=False
+            ctx,
+            self.routes.media_prefix,
+            "urls",
+            url_slug,
         )
 
-    def _format_enrichment(self, ctx: UrlContext, doc: Document, subfolder: str | None = None) -> str:
-        """Generic handler for all media enrichment types."""
-        # 1. Try parent path logic
-        parent_path = (doc.parent.suggested_path if doc.parent else None) or doc.metadata.get("parent_path")
+    def _format_annotation_url(self, ctx: UrlContext, document: Document) -> str:
+        slug = document.metadata.get("slug", document.document_id[:8])
+        return self._join(ctx, self.routes.annotations_prefix, slugify(slug))
+
+    def _format_post_url(self, ctx: UrlContext, document: Document) -> str:
+        slug = document.metadata.get("slug", document.document_id[:8])
+        normalized_slug = slugify(slug)
+
+        if self.routes.date_in_url:
+            date_val = document.metadata.get("date", "")
+            if date_val:
+                date_str = _date_to_iso_date(date_val)
+                return self._join(ctx, self.routes.posts_prefix, f"{date_str}-{normalized_slug}")
+
+        return self._join(ctx, self.routes.posts_prefix, normalized_slug)
+
+    def _format_media_url(self, ctx: UrlContext, document: Document) -> str:
+        """Resolve canonical URL for media assets."""
+        if document.suggested_path:
+            clean_path = document.suggested_path.strip("/")
+            return self._join(ctx, clean_path, trailing_slash=False)
+        # Default to /media/{doc_id}
+        filename = document.metadata.get("filename")
+        path_segment = filename or f"{document.document_id}"
+        return self._join(ctx, self.routes.media_prefix, path_segment, trailing_slash=False)
+
+    def _format_media_enrichment_url(self, ctx: UrlContext, document: Document) -> str:
+        """Mirror parent media path but swap extension for markdown."""
+        parent_path = None
+        if document.parent and document.parent.suggested_path:
+            parent_path = document.parent.suggested_path
+        elif document.metadata.get("parent_path"):
+            parent_path = document.metadata["parent_path"]
+
         if parent_path:
-            path = _remove_url_extension(parent_path.strip("/"))
-            # Clean redundancy: remove base/site prefixes from the string if present
-            prefixes = [
-                f"{(ctx.site_prefix or '').strip('/')}/{self.routes.media_prefix.strip('/')}",
-                self.routes.media_prefix.strip("/"),
-                "media",
-                "posts/media",
-            ]
-            for p in prefixes:
-                if path.startswith(p + "/"):
-                    path = path.removeprefix(p + "/").strip("/")
+            # Pure string manipulation - no Path operations
+            enrichment_path = _remove_url_extension(parent_path.strip("/"))
+            # Strip any existing site_prefix or media_prefix to avoid duplication
+            # when _join adds them again
+            site_prefix = (ctx.site_prefix or "").strip("/")
+            media_prefix = self.routes.media_prefix.strip("/")
+            for prefix in [f"{site_prefix}/{media_prefix}", site_prefix, media_prefix]:
+                if prefix and enrichment_path.startswith(prefix + "/"):
+                    enrichment_path = enrichment_path[len(prefix) + 1 :]
                     break
-            return self._join(ctx, self.routes.media_prefix, path)
+            return self._join(ctx, self.routes.media_prefix, enrichment_path, trailing_slash=True)
 
-        # 2. Try document's own suggested path
-        if doc.suggested_path:
-            return self._join(ctx, _remove_url_extension(doc.suggested_path), trailing_slash=True)
+        if document.suggested_path:
+            # Pure string manipulation - no Path operations
+            clean_path = _remove_url_extension(document.suggested_path.strip("/"))
+            return self._join(ctx, clean_path, trailing_slash=True)
 
-        # 3. Fallback to slug-based
-        slug = f"{doc.slug}-{doc.document_id[:8]}" if not doc.slug.endswith(doc.document_id[:8]) else doc.slug
-        parts = [self.routes.media_prefix, subfolder, slug] if subfolder else [self.routes.media_prefix, slug]
-        return self._join(ctx, *parts)
+        fallback = f"{self._slug_with_identifier(document)}"
+        return self._join(ctx, self.routes.media_prefix, fallback, trailing_slash=True)
 
-    def _format_url_enrichment(self, ctx: UrlContext, doc: Document) -> str:
-        if doc.suggested_path:
-            return self._join(ctx, _remove_url_extension(doc.suggested_path))
-        slug = f"{doc.slug}-{doc.document_id[:8]}" if not doc.slug.endswith(doc.document_id[:8]) else doc.slug
-        return self._join(ctx, self.routes.media_prefix, "urls", slug)
+    def _format_typed_media_enrichment_url(self, ctx: UrlContext, document: Document, subfolder: str) -> str:
+        """Format URL for typed media enrichment (images, videos, audio).
 
-    def _format_annotation(self, ctx: UrlContext, doc: Document) -> str:
-        return self._join(ctx, self.routes.annotations_prefix, self._get_slug(doc))
+        Args:
+            ctx: URL context
+            document: The enrichment document
+            subfolder: Target subfolder (e.g., "images", "videos", "audio")
+
+        """
+        slug = self._slug_with_identifier(document)
+        return self._join(ctx, self.routes.media_prefix, subfolder, slug, trailing_slash=True)
+
+    def _slug_with_identifier(self, document: Document) -> str:
+        """Return slug augmented with a deterministic identifier."""
+        slug_value = document.slug
+        suffix = document.document_id[:8]
+        if slug_value.endswith(suffix):
+            return slug_value
+        return f"{slug_value}-{suffix}"
+
+
+def _date_to_iso_date(value: datetime | str) -> str:
+    """Return ISO date (YYYY-MM-DD) from datetime/string."""
+    if isinstance(value, datetime):
+        return value.date().isoformat()
+    text = str(value)
+    if "T" in text:
+        return text.split("T", 1)[0]
+    if " " in text:
+        return text.split(" ", 1)[0]
+    return text
diff --git a/tests/unit/test_media_url_conventions.py b/tests/unit/test_media_url_conventions.py
index 692946071..ec38c482c 100644
--- a/tests/unit/test_media_url_conventions.py
+++ b/tests/unit/test_media_url_conventions.py
@@ -56,5 +56,5 @@ def test_format_media_url_infers_subdirectory_from_extension(convention, url_con
     url = convention.canonical_url(doc, url_context)
 
     # ASSERTION: When no suggested_path, falls back to posts/media prefix
-    # (Current convention uses posts/media/{subfolder}/{filename})
-    assert url == "/posts/media/images/some-image.png"
+    # (Current convention uses posts/media/ for media without explicit path)
+    assert url == "/posts/media/some-image.png"
diff --git a/tests/unit/test_model_guards.py b/tests/unit/test_model_guards.py
deleted file mode 100644
index a6a731e39..000000000
--- a/tests/unit/test_model_guards.py
+++ /dev/null
@@ -1,25 +0,0 @@
-from egregora.config.settings import DEFAULT_MODEL
-
-
-def test_default_model_is_modern():
-    """
-    Ensure the default model is a modern, high-capacity model.
-    We verify it's not a legacy 1.0 model.
-    """
-    model = DEFAULT_MODEL.lower()
-
-    # Must be Flash or Pro
-    assert "flash" in model or "pro" in model
-
-    # Must NOT be legacy 1.0
-    assert "1.0" not in model
-    assert model.replace("google-gla:", "").replace("models/", "") != "gemini-pro"
-
-
-def test_model_params_defaults(config_factory):
-    """Ensure default configuration parameters meet safety requirements."""
-    # Create a default config
-    config = config_factory()
-
-    # verify writer model defaults matches our verified default
-    assert config.models.writer == DEFAULT_MODEL
diff --git a/tests/v3/core/test_types_property.py b/tests/v3/core/test_types_property.py
index de8738cf0..c53ec03ab 100644
--- a/tests/v3/core/test_types_property.py
+++ b/tests/v3/core/test_types_property.py
@@ -16,12 +16,8 @@
 
 # --- Strategies ---
 
-def xml_safe_text(min_size=0, max_size=100):
-    """Generate XML-safe text with configurable size limits.
-
-    Default max_size reduced from 500 to 100 for faster property test execution.
-    """
-    return st.text(alphabet=st.characters(blacklist_categories=('Cc', 'Cs', 'Co')), min_size=min_size, max_size=max_size)
+def xml_safe_text(min_size=0):
+    return st.text(alphabet=st.characters(blacklist_categories=('Cc', 'Cs', 'Co')), min_size=min_size)
 
 def document_strategy():
     return st.builds(
@@ -35,53 +31,29 @@ def document_strategy():
     )
 
 def author_strategy():
-    """Generate Author objects with optimized constraints.
-
-    Uses simpler email generation for better performance.
-    """
-    # Simple email pattern instead of full st.emails() which can be slow
-    simple_email = st.builds(
-        lambda user, domain: f"{user}@{domain}",
-        user=st.text(alphabet=st.characters(whitelist_categories=('L', 'N')), min_size=1, max_size=20),
-        domain=st.sampled_from(['example.com', 'test.org', 'mail.net'])
-    )
-    return st.builds(
-        Author,
-        name=xml_safe_text(min_size=1, max_size=50),
-        email=st.one_of(st.none(), simple_email)
-    )
+    return st.builds(Author, name=xml_safe_text(min_size=1), email=st.one_of(st.none(), st.emails()))
 
 def entry_strategy():
-    """Generate Entry objects with optimized constraints for faster tests.
-
-    Reduced authors from max_size=3 to max_size=1 to minimize nested object generation.
-    """
     return st.builds(
         Entry,
-        id=xml_safe_text(min_size=1, max_size=50),
-        title=xml_safe_text(min_size=1, max_size=50),
+        id=xml_safe_text(min_size=1),
+        title=xml_safe_text(min_size=1),
         updated=st.datetimes(timezones=st.just(timezone.utc)),
-        content=xml_safe_text(max_size=200),
-        authors=st.lists(author_strategy(), max_size=1),
+        content=xml_safe_text(),
+        authors=st.lists(author_strategy(), max_size=3),
         in_reply_to=st.one_of(
             st.none(),
-            st.builds(InReplyTo, ref=xml_safe_text(min_size=1, max_size=50))
+            st.builds(InReplyTo, ref=xml_safe_text(min_size=1))
         )
     )
 
 def feed_strategy():
-    """Generate Feed objects with optimized constraints for faster tests.
-
-    Reduced entries from max_size=5 to max_size=2 to minimize nested object generation.
-    With max 2 entries and max 1 author each, we generate at most 2 nested objects,
-    down from 15 (5 entries Ã— 3 authors).
-    """
     return st.builds(
         Feed,
-        id=xml_safe_text(min_size=1, max_size=50),
-        title=xml_safe_text(min_size=1, max_size=50),
+        id=xml_safe_text(min_size=1),
+        title=xml_safe_text(min_size=1),
         updated=st.datetimes(timezones=st.just(timezone.utc)),
-        entries=st.lists(entry_strategy(), max_size=2)
+        entries=st.lists(entry_strategy(), max_size=5)
     )
 
 # --- Tests ---
diff --git a/uv.lock b/uv.lock
index 511213789..9edf5fd0a 100644
--- a/uv.lock
+++ b/uv.lock
@@ -692,7 +692,6 @@ dev = [
     { name = "ruff" },
     { name = "syrupy" },
     { name = "vulture" },
-    { name = "xenon" },
 ]
 
 [package.metadata]
@@ -773,7 +772,6 @@ dev = [
     { name = "ruff", specifier = ">=0.14" },
     { name = "syrupy", specifier = ">=4.9" },
     { name = "vulture", specifier = ">=2.14" },
-    { name = "xenon", specifier = ">=0.9" },
 ]
 
 [[package]]
@@ -3440,20 +3438,6 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/1f/f6/a933bd70f98e9cf3e08167fc5cd7aaaca49147e48411c0bd5ae701bb2194/wrapt-1.17.3-py3-none-any.whl", hash = "sha256:7171ae35d2c33d326ac19dd8facb1e82e5fd04ef8c6c0e394d7af55a55051c22", size = 23591, upload-time = "2025-08-12T05:53:20.674Z" },
 ]
 
-[[package]]
-name = "xenon"
-version = "0.9.3"
-source = { registry = "https://pypi.org/simple" }
-dependencies = [
-    { name = "pyyaml" },
-    { name = "radon" },
-    { name = "requests" },
-]
-sdist = { url = "https://files.pythonhosted.org/packages/c4/7c/2b341eaeec69d514b635ea18481885a956d196a74322a4b0942ef0c31691/xenon-0.9.3.tar.gz", hash = "sha256:4a7538d8ba08aa5d79055fb3e0b2393c0bd6d7d16a4ab0fcdef02ef1f10a43fa", size = 9883, upload-time = "2024-10-21T10:27:53.722Z" }
-wheels = [
-    { url = "https://files.pythonhosted.org/packages/6f/5d/29ff8665b129cafd147d90b86e92babee32e116e3c84447107da3e77f8fb/xenon-0.9.3-py2.py3-none-any.whl", hash = "sha256:6e2c2c251cc5e9d01fe984e623499b13b2140fcbf74d6c03a613fa43a9347097", size = 8966, upload-time = "2024-10-21T10:27:51.121Z" },
-]
-
 [[package]]
 name = "yarl"
 version = "1.22.0"
