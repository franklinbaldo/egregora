# MCP Server para RAG de Newsletters

## üìã Vis√£o Geral

O reposit√≥rio inclui um **MCP (Model Context Protocol) server local** que exp√µe o sistema RAG como servi√ßo, permitindo:

1. **Claude Desktop** buscar contexto hist√≥rico durante conversas
2. **Pipeline Egregora** consumir RAG via protocolo padronizado
3. **Desenvolvedores** testar e debugar queries facilmente
4. **Outras ferramentas** integrar com o RAG

**Vantagem principal:** Separa√ß√£o de responsabilidades + interface padronizada.

### Status atual (2025-10-03)

- ‚úÖ Servidor MCP funcional em `src/egregora/mcp_server/server.py`.
- ‚úÖ Ferramentas expostas: `search_newsletters`, `list_newsletters`, `get_newsletter`.
- ‚úÖ Integra√ß√£o direta com o `NewsletterRAG` e cache persistente.
- üîÑ Suporte opcional a embeddings do Gemini (`--use-gemini-embeddings`).
- üß™ Testes automatizados planejados (`test_mcp_server.py`).

---

## üéØ O Que √© MCP?

**Model Context Protocol** - Protocolo da Anthropic para conectar LLMs a fontes de dados:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claude      ‚îÇ  ‚ÜêMCP‚Üí  ‚îÇ MCP Server  ‚îÇ  ‚Üê‚îÄ‚Üí    ‚îÇ Data Source  ‚îÇ
‚îÇ Desktop     ‚îÇ         ‚îÇ (Local)     ‚îÇ         ‚îÇ (Newsletters)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Features MCP:**
- ‚úÖ Protocolo padr√£o (JSON-RPC)
- ‚úÖ Tools (fun√ß√µes que Claude pode chamar)
- ‚úÖ Resources (dados que Claude pode ler)
- ‚úÖ Prompts (templates reutiliz√°veis)

---

## üèóÔ∏è Arquitetura Proposta

### Componentes

```
egregora/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ egregora/
‚îÇ       ‚îú‚îÄ‚îÄ rag/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core.py              # N√∫cleo do RAG (sem depend√™ncia MCP)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ indexer.py           # Indexa√ß√£o incremental
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ search.py            # Busca e ranqueamento
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ query_gen.py         # Gera√ß√£o autom√°tica de queries
‚îÇ       ‚îú‚îÄ‚îÄ mcp_server/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ server.py            # Servidor MCP
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ tools.py             # Defini√ß√£o das tools
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ config.py            # Configura√ß√£o do server
‚îÇ       ‚îî‚îÄ‚îÄ pipeline.py              # Integra√ß√£o com o RAG/MCP
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ start_mcp_server.py          # Script auxiliar para desenvolvimento
```

### Separa√ß√£o de Responsabilidades

**1. Core RAG (independente de MCP):**
- Indexa√ß√£o incremental
- Gera√ß√£o de embeddings
- Busca sem√¢ntica
- Query inteligente
- Cache em disco

**2. MCP Server (camada de interface):**
- Expor RAG via protocolo MCP
- Tools para Claude chamar
- Resources para ler newsletters
- Valida√ß√£o de requests

**Embeddings do Gemini**
- Ative via `RAGConfig(use_gemini_embeddings=True)` ou flag CLI `--use-gemini-embeddings`.
- O servidor MCP utiliza automaticamente o √≠ndice configurado (TF-IDF ou embeddings).
- Cache persistente em `cache/embeddings/` garante custos previs√≠veis.

**3. Pipeline (MCP client):**
- Conectar ao MCP server
- Chamar tools via protocolo
- Integra√ß√£o transparente

---

## üîß Implementa√ß√£o do MCP Server

### Arquivo: `src/egregora/mcp_server/server.py`

```python
"""
MCP Server para RAG de newsletters.

Exp√µe o sistema RAG via Model Context Protocol,
permitindo que Claude e outras ferramentas busquem
contexto hist√≥rico de newsletters anteriores.
"""

from __future__ import annotations

import asyncio
import json
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import date

from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.types import (
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    Resource,
    ResourceTemplate,
)

from ..rag.index import NewsletterRAG
from ..rag.query_gen import QueryGenerator
from ..config import RAGConfig

# Inicializar server
app = Server("egregora-rag")


class RAGServer:
    """Wrapper do RAG para MCP server."""
    
    def __init__(self, config_path: Path | None = None):
        # Carregar config
        if config_path and config_path.exists():
            import tomli
            with open(config_path, 'rb') as f:
                config_dict = tomli.load(f)
            self.config = RAGConfig(**config_dict.get('rag', {}))
        else:
            self.config = RAGConfig()
        
        # Inicializar RAG
        self.rag = NewsletterRAG(
            newsletters_dir=Path("data/daily"),
            cache_dir=Path("cache/rag"),
            config=self.config,
        )
        
        # Query generator
        self.query_gen = QueryGenerator(self.config)
        
        # Lazy loading de √≠ndice
        self._indexed = False
    
    async def ensure_indexed(self):
        """Garante que √≠ndice est√° carregado."""
        if not self._indexed:
            await asyncio.to_thread(self.rag.load_index)
            self._indexed = True
    
    async def search_newsletters(
        self,
        query: str,
        top_k: int = 5,
        min_similarity: float = 0.7,
        exclude_recent_days: int = 7,
    ) -> List[Dict[str, Any]]:
        """Busca trechos relevantes."""
        await self.ensure_indexed()
        
        results = await asyncio.to_thread(
            self.rag.search,
            query=query,
            top_k=top_k,
            min_similarity=min_similarity,
            exclude_recent_days=exclude_recent_days,
        )
        
        return [
            {
                "text": chunk.text,
                "date": chunk.newsletter_date.isoformat(),
                "section": chunk.section_title,
                "similarity": score,
                "path": str(chunk.newsletter_path),
            }
            for chunk, score in results
        ]
    
    async def generate_query(
        self,
        transcripts: str,
        model: str = "gemini-2.0-flash-exp",
    ) -> Dict[str, Any]:
        """Gera query inteligente dos transcritos."""
        query_data = await asyncio.to_thread(
            self.query_gen.generate,
            transcripts=transcripts,
            model=model,
        )
        return query_data
    
    async def get_newsletter(self, date_str: str) -> Optional[str]:
        """Retorna conte√∫do completo de uma newsletter."""
        await self.ensure_indexed()
        
        try:
            newsletter_date = date.fromisoformat(date_str)
            newsletter_path = Path("data/daily") / f"{date_str}.md"
            
            if newsletter_path.exists():
                return newsletter_path.read_text(encoding="utf-8")
        except (ValueError, OSError):
            pass
        
        return None
    
    async def list_newsletters(
        self,
        limit: int = 50,
        offset: int = 0,
    ) -> List[Dict[str, str]]:
        """Lista newsletters dispon√≠veis."""
        newsletters_dir = Path("data/daily")
        
        if not newsletters_dir.exists():
            return []
        
        # Listar e ordenar por data (mais recente primeiro)
        files = sorted(
            newsletters_dir.glob("*.md"),
            key=lambda p: p.stem,
            reverse=True,
        )
        
        # Pagina√ß√£o
        paginated = files[offset:offset + limit]
        
        return [
            {
                "date": f.stem,
                "path": str(f),
                "size_kb": f.stat().st_size // 1024,
            }
            for f in paginated
        ]
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do RAG."""
        await self.ensure_indexed()
        
        return await asyncio.to_thread(self.rag.get_stats)
    
    async def reindex(self, force: bool = False) -> Dict[str, int]:
        """Reindexar newsletters."""
        return await asyncio.to_thread(
            self.rag.update_index,
            force_rebuild=force,
        )


# Inst√¢ncia global
rag_server: Optional[RAGServer] = None


@app.list_tools()
async def handle_list_tools() -> List[Tool]:
    """Lista tools dispon√≠veis no servidor."""
    
    return [
        Tool(
            name="search_newsletters",
            description=(
                "Busca trechos relevantes em newsletters anteriores usando "
                "busca sem√¢ntica. Retorna chunks similares √† query fornecida."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Query de busca (pode ser pergunta, t√≥picos, keywords)"
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "N√∫mero m√°ximo de resultados (padr√£o: 5)",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 20,
                    },
                    "min_similarity": {
                        "type": "number",
                        "description": "Similaridade m√≠nima 0-1 (padr√£o: 0.7)",
                        "default": 0.7,
                        "minimum": 0.0,
                        "maximum": 1.0,
                    },
                    "exclude_recent_days": {
                        "type": "integer",
                        "description": "Excluir newsletters dos √∫ltimos N dias (padr√£o: 7)",
                        "default": 7,
                        "minimum": 0,
                    }
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="generate_search_query",
            description=(
                "Gera uma query de busca otimizada a partir de transcritos "
                "de conversas. Usa LLM para identificar t√≥picos principais."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "transcripts": {
                        "type": "string",
                        "description": "Texto dos transcritos a analisar"
                    },
                    "model": {
                        "type": "string",
                        "description": "Modelo LLM a usar (padr√£o: gemini-2.0-flash-exp)",
                        "default": "gemini-2.0-flash-exp",
                    }
                },
                "required": ["transcripts"],
            },
        ),
        Tool(
            name="get_newsletter",
            description=(
                "Retorna o conte√∫do completo de uma newsletter espec√≠fica "
                "por data (formato YYYY-MM-DD)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "date": {
                        "type": "string",
                        "description": "Data da newsletter (YYYY-MM-DD)",
                        "pattern": r"^\d{4}-\d{2}-\d{2}$",
                    }
                },
                "required": ["date"],
            },
        ),
        Tool(
            name="list_newsletters",
            description="Lista newsletters dispon√≠veis com pagina√ß√£o.",
            inputSchema={
                "type": "object",
                "properties": {
                    "limit": {
                        "type": "integer",
                        "description": "N√∫mero m√°ximo de resultados (padr√£o: 50)",
                        "default": 50,
                        "minimum": 1,
                        "maximum": 100,
                    },
                    "offset": {
                        "type": "integer",
                        "description": "Offset para pagina√ß√£o (padr√£o: 0)",
                        "default": 0,
                        "minimum": 0,
                    }
                },
            },
        ),
        Tool(
            name="get_rag_stats",
            description="Retorna estat√≠sticas do sistema RAG (chunks, cache, etc).",
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),
        Tool(
            name="reindex_newsletters",
            description=(
                "Reconstr√≥i o √≠ndice do RAG e informa quantas newsletters e "
                "chunks foram processados. Use force=true para reindexar tudo."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "force": {
                        "type": "boolean",
                        "description": "Se true, reprocessa todas as newsletters",
                        "default": False,
                    }
                },
            },
        ),
    ]


@app.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
    """Handler para chamadas de tools."""
    
    global rag_server
    
    if rag_server is None:
        return [TextContent(
            type="text",
            text="Erro: RAG server n√£o inicializado"
        )]
    
    try:
        if name == "search_newsletters":
            results = await rag_server.search_newsletters(**arguments)
            
            # Formatar resultados
            output = ["# Trechos Relevantes de Newsletters Anteriores\n"]
            
            for i, result in enumerate(results, 1):
                output.append(f"## Trecho {i} ‚Äî {result['date']} "
                            f"(relev√¢ncia: {result['similarity']:.0%})")
                if result['section']:
                    output.append(f"*{result['section']}*\n")
                output.append(result['text'])
                output.append("\n---\n")
            
            return [TextContent(type="text", text="\n".join(output))]
        
        elif name == "generate_search_query":
            query_data = await rag_server.generate_query(**arguments)
            
            output = [
                "# Query Gerada\n",
                f"**T√≥picos Principais:** {', '.join(query_data['main_topics'])}\n",
                f"**Keywords:** {', '.join(query_data['keywords'])}\n",
                f"\n**Query de Busca:**\n{query_data['search_query']}\n",
                f"\n**Contexto:**\n{query_data['context']}",
            ]
            
            return [TextContent(type="text", text="\n".join(output))]
        
        elif name == "get_newsletter":
            content = await rag_server.get_newsletter(arguments["date"])
            
            if content:
                return [TextContent(type="text", text=content)]
            else:
                return [TextContent(
                    type="text",
                    text=f"Newsletter n√£o encontrada: {arguments['date']}"
                )]
        
        elif name == "list_newsletters":
            newsletters = await rag_server.list_newsletters(**arguments)
            
            output = ["# Newsletters Dispon√≠veis\n"]
            for nl in newsletters:
                output.append(f"- {nl['date']} ({nl['size_kb']} KB)")
            
            return [TextContent(type="text", text="\n".join(output))]
        
        elif name == "get_rag_stats":
            stats = await rag_server.get_stats()

            output = [
                "# Estat√≠sticas do RAG\n",
                f"**Total de Newsletters:** {stats['total_newsletters']}",
                f"**Total de Chunks:** {stats['total_chunks']}",
                f"**Vector Store:** {stats['vector_store']}",
                f"**Persist√™ncia:** {stats['persist_dir']}",
            ]

            return [TextContent(type="text", text="\n".join(output))]
        
        elif name == "reindex_newsletters":
            result = await rag_server.reindex(**arguments)

            output = [
                "# Reindexa√ß√£o Conclu√≠da\n",
                f"- Newsletters processadas: {result['newsletters_count']}",
                f"- Chunks indexados: {result['chunks_count']}",
            ]

            return [TextContent(type="text", text="\n".join(output))]
        
        else:
            return [TextContent(
                type="text",
                text=f"Tool desconhecido: {name}"
            )]
    
    except Exception as e:
        import traceback
        error_msg = f"Erro ao executar {name}: {str(e)}\n\n{traceback.format_exc()}"
        return [TextContent(type="text", text=error_msg)]


@app.list_resources()
async def handle_list_resources() -> List[Resource]:
    """Lista resources dispon√≠veis (newsletters)."""
    
    global rag_server
    
    if rag_server is None:
        return []
    
    newsletters = await rag_server.list_newsletters(limit=100)
    
    return [
        Resource(
            uri=f"newsletter://{nl['date']}",
            name=f"Newsletter {nl['date']}",
            description=f"Newsletter do dia {nl['date']} ({nl['size_kb']} KB)",
            mimeType="text/markdown",
        )
        for nl in newsletters
    ]


@app.read_resource()
async def handle_read_resource(uri: str) -> str:
    """L√™ conte√∫do de um resource."""
    
    global rag_server
    
    if not uri.startswith("newsletter://"):
        raise ValueError(f"URI inv√°lida: {uri}")
    
    date_str = uri.replace("newsletter://", "")
    
    content = await rag_server.get_newsletter(date_str)
    
    if content is None:
        raise ValueError(f"Newsletter n√£o encontrada: {date_str}")
    
    return content


async def main():
    """Inicia o MCP server."""
    
    global rag_server
    
    # Inicializar RAG server
    print("[MCP Server] Inicializando RAG...")
    rag_server = RAGServer()
    print("[MCP Server] ‚úÖ RAG inicializado")
    
    # Rodar server
    print("[MCP Server] Servidor pronto!")
    print("[MCP Server] Aguardando conex√µes...")
    
    async with app.run_stdio():
        await asyncio.Event().wait()


if __name__ == "__main__":
    asyncio.run(main())
```

---

## üìù Configura√ß√£o do Claude Desktop

### Arquivo: `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "egregora-rag": {
      "command": "uv",
      "args": [
        "run",
        "--directory",
        "/caminho/para/egregora",
        "python",
        "-m",
        "egregora.mcp_server.server"
      ],
      "env": {
        "GEMINI_API_KEY": "sua-api-key-aqui"
      }
    }
  }
}
```

---

## üîå Integra√ß√£o no Pipeline

### Modificar `pipeline.py` para usar MCP client

```python
"""Pipeline usando MCP client para RAG."""

from mcp.client import Client, StdioServerParameters

async def generate_newsletter_with_rag(
    config: PipelineConfig,
    ...
) -> PipelineResult:
    
    # ... c√≥digo existente ...
    
    # Conectar ao MCP server
    if config.rag.enabled:
        server_params = StdioServerParameters(
            command="uv",
            args=["run", "python", "-m", "egregora.mcp_server.server"],
            env={"GEMINI_API_KEY": os.environ["GEMINI_API_KEY"]}
        )
        
        async with Client() as client:
            # Iniciar servidor
            await client.connect(server_params)
            
            # 1. Gerar query inteligente
            transcripts_sample = prepare_transcripts_sample(sanitized_transcripts)
            
            query_result = await client.call_tool(
                "generate_search_query",
                arguments={"transcripts": transcripts_sample}
            )
            
            # 2. Buscar trechos relevantes
            search_result = await client.call_tool(
                "search_newsletters",
                arguments={
                    "query": query_data['search_query'],
                    "top_k": config.rag.top_k,
                    "min_similarity": config.rag.min_similarity,
                }
            )
            
            # 3. Adicionar ao prompt
            rag_context = search_result[0].text
    
    # ... resto do c√≥digo ...
```

---

## üéØ Casos de Uso

### 1. **Claude Desktop - Chat Interativo**

```
Usu√°rio: O que j√° discutimos sobre privacidade em IA?

Claude: [chama search_newsletters com query "privacidade IA"]

Claude: Encontrei 3 discuss√µes relevantes:

1. **2025-09-15** (87% relevante)
   Debatemos sobre uso de dados pessoais em modelos de IA...
   
2. **2025-09-20** (82% relevante)
   Discutimos conformidade com LGPD...

...
```

### 2. **Pipeline Egregora - Gera√ß√£o Au
