"""Simple enrichment: extract media, add LLM-described context as table rows.\n\nEnrichment adds context for URLs and media as new table rows with author 'egregora'.\nThe LLM sees enrichment context inline with original messages.\n\nDocumentation:\n- Architecture (Enricher): docs/guides/architecture.md#4-enricher-enricherpy\n- Core Concepts: docs/getting-started/concepts.md#4-enrich-optional\n"""\n\nimport logging\nimport os\nimport re\nimport tempfile\nimport uuid\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nimport duckdb\nimport ibis\nfrom ibis.expr.types import Table\n\nfrom ...config import ModelConfig\nfrom ...core import database_schema\nfrom ...core.database_schema import CONVERSATION_SCHEMA\nfrom ...prompt_templates import (\n    DetailedMediaEnrichmentPromptTemplate,\n    DetailedUrlEnrichmentPromptTemplate,\n)\nfrom ...utils import EnrichmentCache, GeminiBatchClient, make_enrichment_cache_key\nfrom ...utils.batch import BatchPromptResult\nfrom .batch import (\n    MediaEnrichmentJob,\n    UrlEnrichmentJob,\n    _ensure_datetime,\n    _safe_timestamp_plus_one,\n    _table_to_pylist,\n    build_batch_requests,\n    map_batch_results,\n)\nfrom .media import (\n    detect_media_type,\n    extract_urls,\n    find_media_references,\n    replace_media_mentions,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nif TYPE_CHECKING:\n    from ibis.backends.duckdb import Backend as DuckDBBackend\nelse:  # pragma: no cover - duckdb backend available at runtime when installed\n    DuckDBBackend = Any\n\n\ndef _atomic_write_text(path: Path, content: str, encoding: str = "utf-8") -> None:\n    """Write text to a file atomically to prevent partial writes during concurrent runs.\n\n    Writes to a temporary file in the same directory, then atomically renames it.\n    This ensures readers never see partial/incomplete content.\n    """\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create temp file in same directory for atomic rename (must be same filesystem)\n    fd, temp_path = tempfile.mkstemp(\n        dir=path.parent,\n        prefix=f".{path.name}.",\n        suffix=".tmp",\n    )\n    try:\n        # Write content to temp file\n        with os.fdopen(fd, "w", encoding=encoding) as f:\n            f.write(content)\n\n        # Atomic rename (replaces destination if it exists)\n        os.replace(temp_path, path)\n    except Exception:\n        # Clean up temp file on error\n        try:\n            os.unlink(temp_path)\n        except OSError:\n            pass\n        raise\n\n\nif TYPE_CHECKING:\n    from ibis.backends.duckdb import Backend as DuckDBBackend\nelse:  # pragma: no cover - duckdb backend available at runtime when installed\n    DuckDBBackend = Any\n\n\ndef enrich_table(\n    messages_table: Table,\n    media_mapping: dict[str, Path],\n    text_batch_client: GeminiBatchClient,\n    vision_batch_client: GeminiBatchClient,\n    cache: EnrichmentCache,\n    docs_dir: Path,\n    posts_dir: Path,\n    model_config: ModelConfig | None = None,\n    enable_url: bool = True,\n    enable_media: bool = True,\n    max_enrichments: int = 50,\n    *,\n    duckdb_connection: "DuckDBBackend | None" = None,\n    target_table: str | None = None,\n) -> Table:\n    """Add LLM-generated enrichment rows to Table for URLs and media."""\n    if model_config is None:\n        model_config = ModelConfig()\n\n    url_model = model_config.get_model("enricher")\n    vision_model = model_config.get_model("enricher_vision")\n    logger.info("[blue]ðŸŒ Enricher text model:[/] %s", url_model)\n    logger.info("[blue]ðŸ–¼ï¸  Enricher vision model:[/] %s", vision_model)\n\n    if messages_table.count().execute() == 0:\n        return messages_table\n\n    # Use streaming helper to avoid loading entire table into memory\n    rows = _table_to_pylist(messages_table)\n    new_rows: list[dict[str, Any]] = []\n    enrichment_count = 0\n    pii_detected_count = 0\n    pii_media_deleted = False\n    seen_url_keys: set[str] = set()\n    seen_media_keys: set[str] = set()\n\n    url_jobs: list[UrlEnrichmentJob] = []\n    media_jobs: list[MediaEnrichmentJob] = []\n\n    # Build reverse lookup: filename -> (original_filename, file_path)\n    # This avoids O(nÃ—m) substring matching in the hot path\n    media_filename_lookup: dict[str, tuple[str, Path]] = {}\n    if enable_media and media_mapping:\n        for original_filename, file_path in media_mapping.items():\n            media_filename_lookup[original_filename] = (original_filename, file_path)\n            media_filename_lookup[file_path.name] = (original_filename, file_path)\n\n    for row in rows:\n        if enrichment_count >= max_enrichments:\n            break\n\n        message = row.get("message", "")\n        timestamp = row["timestamp"]\n        author = row.get("author", "unknown")\n\n        if enable_url and message:\n            urls = extract_urls(message)\n            for url in urls[:3]:\n                if enrichment_count >= max_enrichments:\n                    break\n                cache_key = make_enrichment_cache_key(kind="url", identifier=url)\n                if cache_key in seen_url_keys:\n                    continue\n\n                enrichment_id = uuid.uuid5(uuid.NAMESPACE_URL, url)\n                enrichment_path = docs_dir / "media" / "urls" / f"{enrichment_id}.md"\n                url_job = UrlEnrichmentJob(\n                    key=cache_key,\n                    url=url,\n                    original_message=message,\n                    sender_uuid=author,\n                    timestamp=timestamp,\n                    path=enrichment_path,\n                    tag=f"url:{cache_key}",\n                )\n\n                cache_entry = cache.load(cache_key)\n                if cache_entry:\n                    url_job.markdown = cache_entry.get("markdown")\n                    url_job.cached = True\n\n                url_jobs.append(url_job)\n                seen_url_keys.add(cache_key)\n                enrichment_count += 1\n\n        if enable_media and media_filename_lookup and message:\n            # Extract media references efficiently:\n            # 1. WhatsApp-style references (original filenames)\n            media_refs = find_media_references(message)\n\n            # 2. UUID-based filenames in markdown links (after media replacement)\n            # Pattern: extract filenames from markdown links like ![Image](media/images/uuid.jpg)\n            markdown_media_pattern = r"!\[[^\]]*\]\([^)]*?([a-f0-9\-]+\.\w+)\)"\n            markdown_matches = re.findall(markdown_media_pattern, message)\n            media_refs.extend(markdown_matches)\n\n            # Also check for direct UUID-based filenames (without path)\n            uuid_filename_pattern = r"\b([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}\.\w+)\b"\n            uuid_matches = re.findall(uuid_filename_pattern, message)\n            media_refs.extend(uuid_matches)\n\n            # Deduplicate and only keep refs that exist in our lookup\n            media_refs = [ref for ref in set(media_refs) if ref in media_filename_lookup]\n\n            for ref in media_refs:\n                if enrichment_count >= max_enrichments:\n                    break\n\n                # Look up the file in our hash table (O(1) instead of O(m))\n                lookup_result = media_filename_lookup.get(ref)\n                if not lookup_result:\n                    continue\n\n                original_filename, file_path = lookup_result\n                cache_key = make_enrichment_cache_key(kind="media", identifier=str(file_path))\n                if cache_key in seen_media_keys:\n                    continue\n\n                media_type = detect_media_type(file_path)\n                if not media_type:\n                    logger.warning("Unsupported media type for enrichment: %s", file_path.name)\n                    continue\n\n                # Place enrichment .md in the same folder as the media file\n                enrichment_path = file_path.with_suffix(file_path.suffix + ".md")\n                media_job = MediaEnrichmentJob(\n                    key=cache_key,\n                    original_filename=original_filename,\n                    file_path=file_path,\n                    original_message=message,\n                    sender_uuid=author,\n                    timestamp=timestamp,\n                    path=enrichment_path,\n                    tag=f"media:{cache_key}",\n                    media_type=media_type,\n                )\n\n                cache_entry = cache.load(cache_key)\n                if cache_entry:\n                    media_job.markdown = cache_entry.get("markdown")\n                    media_job.cached = True\n\n                media_jobs.append(media_job)\n                seen_media_keys.add(cache_key)\n                enrichment_count += 1\n\n    pending_url_jobs = [url_job for url_job in url_jobs if url_job.markdown is None]\n    if pending_url_jobs:\n        url_records = []\n        for url_job in pending_url_jobs:\n            ts = _ensure_datetime(url_job.timestamp)\n            prompt = DetailedUrlEnrichmentPromptTemplate(\n                url=url_job.url,\n                original_message=url_job.original_message,\n                sender_uuid=url_job.sender_uuid,\n                date=ts.strftime("%Y-%m-%d"),\n                time=ts.strftime("%H:%M"),\n            ).render()\n            url_records.append({"tag": url_job.tag, "prompt": prompt})\n\n        url_table = ibis.memtable(url_records)\n        requests = build_batch_requests(_table_to_pylist(url_table), url_model)\n\n        responses = text_batch_client.generate_content(\n            requests,\n            display_name="Egregora URL Enrichment",\n        )\n\n        result_map = map_batch_results(responses)\n        for url_job in pending_url_jobs:\n            result = result_map.get(url_job.tag)\n            if not result or result.error or not result.response:\n                logger.warning(\n                    "Failed to enrich URL %s: %s",\n                    url_job.url,\n                    result.error if result else "no result",\n                )\n                url_job.markdown = f"[Failed to enrich URL: {url_job.url}]"\n                continue\n\n            markdown_content = (result.response.text or "").strip()\n            if not markdown_content:\n                markdown_content = f"[No enrichment generated for URL: {url_job.url}]"\n\n            url_job.markdown = markdown_content\n            cache.store(url_job.key, {"markdown": markdown_content, "type": "url"})\n\n    pending_media_jobs = [job for job in media_jobs if job.markdown is None]\n    if pending_media_jobs:\n        media_records = []\n        for media_job in pending_media_jobs:\n            uploaded_file = vision_batch_client.upload_file(\n                path=str(media_job.file_path),\n                display_name=media_job.file_path.name,\n            )\n            media_job.upload_uri = getattr(uploaded_file, "uri", None)\n            media_job.mime_type = getattr(uploaded_file, "mime_type", None)\n\n            ts = _ensure_datetime(media_job.timestamp)\n            try:\n                media_path = media_job.file_path.relative_to(docs_dir)\n            except ValueError:\n                media_path = media_job.file_path\n\n            prompt = DetailedMediaEnrichmentPromptTemplate(\n                media_type=media_job.media_type or "unknown",\n                media_filename=media_job.file_path.name,\n                media_path=str(media_path),\n                original_message=media_job.original_message,\n                sender_uuid=media_job.sender_uuid,\n                date=ts.strftime("%Y-%m-%d"),\n                time=ts.strftime("%H:%M"),\n            ).render()\n            media_records.append(\n                {\n                    "tag": media_job.tag,\n                    "prompt": prompt,\n                    "file_uri": media_job.upload_uri,\n                    "mime_type": media_job.mime_type,\n                }\n            )\n\n        media_responses: list[BatchPromptResult] = []\n        if media_records:\n            media_table = ibis.memtable(media_records)\n            records = _table_to_pylist(media_table)\n            requests = build_batch_requests(records, vision_model, include_file=True)\n\n            if requests:\n                media_responses = vision_batch_client.generate_content(\n                    requests,\n                    display_name="Egregora Media Enrichment",\n                )\n\n        result_map = map_batch_results(media_responses)\n        for media_job in pending_media_jobs:\n            if media_job.markdown is not None:\n                continue\n\n            result = result_map.get(media_job.tag)\n            if not result or result.error or not result.response:\n                logger.warning(\n                    "Failed to enrich media %s: %s",\n                    media_job.file_path.name,\n                    result.error if result else "no result",\n                )\n                # Don't save markdown on failure - leave it as None so it won't be written\n                media_job.markdown = None\n                continue\n\n            markdown_content = (result.response.text or "").strip()\n            if not markdown_content:\n                markdown_content = (\n                    f"[No enrichment generated for media: {media_job.file_path.name}]"\n                )\n\n            if "PII_DETECTED" in markdown_content:\n                logger.warning(\n                    "PII detected in media: %s. Media will be deleted after redaction.",\n                    media_job.file_path.name,\n                )\n                markdown_content = markdown_content.replace("PII_DETECTED", "").strip()\n                try:\n                    media_job.file_path.unlink()\n                    logger.info("Deleted media file containing PII: %s", media_job.file_path)\n                    pii_media_deleted = True\n                    pii_detected_count += 1\n                except Exception as delete_error:\n                    logger.error("Failed to delete %s: %s", media_job.file_path, delete_error)\n\n            media_job.markdown = markdown_content\n            cache.store(media_job.key, {"markdown": markdown_content, "type": "media"})\n\n    for url_job in url_jobs:\n        if not url_job.markdown:\n            continue\n\n        _atomic_write_text(url_job.path, url_job.markdown)\n\n        enrichment_timestamp = _safe_timestamp_plus_one(url_job.timestamp)\n        new_rows.append(\n            {\n                "timestamp": enrichment_timestamp,\n                "date": enrichment_timestamp.date(),\n                "author": "egregora",\n                "message": f"[URL Enrichment] {url_job.url}\nEnrichment saved: {url_job.path}",\n                "original_line": "",\n                "tagged_line": "",\n            }\n        )\n\n    for media_job in media_jobs:\n        if not media_job.markdown:\n            continue\n\n        _atomic_write_text(media_job.path, media_job.markdown)\n\n        enrichment_timestamp = _safe_timestamp_plus_one(media_job.timestamp)\n        new_rows.append(\n            {\n                "timestamp": enrichment_timestamp,\n                "date": enrichment_timestamp.date(),\n                "author": "egregora",\n                "message": (\n                    f"[Media Enrichment] {media_job.file_path.name}\n"\n                    f"Enrichment saved: {media_job.path}"\n                ),\n                "original_line": "",\n                "tagged_line": "",\n            }\n        )\n\n    if pii_media_deleted:\n\n        @ibis.udf.scalar.python\n        def replace_media_udf(message: str) -> str:\n            return (\n                replace_media_mentions(message, media_mapping, docs_dir, posts_dir)\n                if message\n                else message\n            )\n\n        messages_table = messages_table.mutate(message=replace_media_udf(messages_table.message))\n\n    if not new_rows:\n        return messages_table\n\n    # TENET-BREAK: Downstream consumers (e.g., writer) expect CONVERSATION_SCHEMA\n    # and will fail if extra columns are present.\n    # To isolate enrichment from upstream changes, we filter `messages_table`\n    # to match the core schema before uniting it with `enrichment_table`.\n    # This ensures that `enrich_table` always returns a table with a predictable\n    # schema, preventing downstream errors.\n    schema = CONVERSATION_SCHEMA\n    # Normalize rows to match schema, filling missing columns with None\n    normalized_rows = [{column: row.get(column) for column in schema.names} for row in new_rows]\n    enrichment_table = ibis.memtable(normalized_rows, schema=schema)\n\n    # Filter messages_table to only include columns from CONVERSATION_SCHEMA\n    messages_table_filtered = messages_table.select(*schema.names)\n\n    # Ensure timestamp column is in UTC to match CONVERSATION_SCHEMA\n    messages_table_filtered = messages_table_filtered.mutate(\n        timestamp=messages_table_filtered.timestamp.cast("timestamp('UTC', 9)")\n    )\n\n    combined = messages_table_filtered.union(enrichment_table, distinct=False)\n    combined = combined.order_by("timestamp")\n\n    # Optional DuckDB persistence\n    if (duckdb_connection is None) != (target_table is None):\n        raise ValueError(\n            "duckdb_connection and target_table must be provided together when persisting"\n        )\n\n    if duckdb_connection and target_table:\n        if not re.fullmatch(r"[A-Za-z_][A-Za-z0-9_]*", target_table):\n            raise ValueError("target_table must be a valid DuckDB identifier")\n\n        # Ensure target table exists with correct schema\n        database_schema.create_table_if_not_exists(\n            duckdb_connection,\n            target_table,\n            CONVERSATION_SCHEMA,\n        )\n\n        # Quote identifiers for SQL safety (defense in depth beyond regex validation)\n        quoted_table = database_schema.quote_identifier(target_table)\n        column_list = ", ".join(database_schema.quote_identifier(col) for col in CONVERSATION_SCHEMA.names)\n\n        # Replace table contents with enriched data (idempotent operation)\n        # Use transaction to make DELETE + INSERT atomic (prevents race conditions)\n        temp_view = f"_egregora_enrichment_{uuid.uuid4().hex}"\n        try:\n            duckdb_connection.create_view(temp_view, combined, overwrite=True)\n            quoted_view = database_schema.quote_identifier(temp_view)\n\n            # Atomic replace: BEGIN, DELETE, INSERT, COMMIT\n            # This prevents race conditions and ensures no window where table is empty\n            duckdb_connection.raw_sql("BEGIN TRANSACTION")\n            try:\n                duckdb_connection.raw_sql(f"DELETE FROM {quoted_table}")\n                duckdb_connection.raw_sql(\n                    f"INSERT INTO {quoted_table} ({column_list}) SELECT {column_list} FROM {quoted_view}"\n                )\n                duckdb_connection.raw_sql("COMMIT")\n            except Exception:\n                duckdb_connection.raw_sql("ROLLBACK")\n                raise\n        finally:\n            duckdb_connection.drop_view(temp_view, force=True)\n\n    if pii_detected_count > 0:\n        logger.info(\n            "Privacy summary: %d media file(s) deleted due to PII detection",\n            pii_detected_count,\n        )\n\n    return combined\n