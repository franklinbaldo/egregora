"""Pure SQL/Ibis WhatsApp parser - no pyparsing dependency.

This module replaces pyparsing with pure DuckDB/Ibis operations for parsing WhatsApp chat exports.
Uses vectorized string operations and window functions instead of line-by-line iteration.

Benefits:
- No pyparsing dependency
- Vectorized processing (faster)
- Pure SQL/Ibis (follows "Ibis everywhere" principle)
- Functional transformations
- More declarative
"""

from __future__ import annotations

import io
import logging
import re
import unicodedata
import zipfile
from datetime import UTC, date, datetime
from typing import TYPE_CHECKING
from zoneinfo import ZoneInfo

import ibis
from dateutil import parser as date_parser

from egregora.database.ir_schema import MESSAGE_SCHEMA, ensure_message_schema
from egregora.privacy.anonymizer import anonymize_table
from egregora.privacy.uuid_namespaces import deterministic_author_uuid
from egregora.utils.zip import ZipValidationError, ensure_safe_member_size, validate_zip_contents

if TYPE_CHECKING:
    from collections.abc import Sequence

    from ibis.expr.types import Table

    from egregora.input_adapters.whatsapp.models import WhatsAppExport

logger = logging.getLogger(__name__)
_IMPORT_ORDER_COLUMN = "_import_order"
_IMPORT_SOURCE_COLUMN = "_import_source"
_AUTHOR_UUID_HEX_COLUMN = "_author_uuid_hex"

# WhatsApp message line pattern
# Matches: "28/10/2025 14:10 - Franklin: message text"
# MUST have author and colon (to match pyparsing behavior)
# Groups: (date, time, author, message)
WHATSAPP_LINE_PATTERN = re.compile(
    r"^(\d{1,2}[/\.\-]\d{1,2}[/\.\-]\d{2,4})(?:,\s*|\s+)(\d{1,2}:\d{2}(?:\s*[AaPp][Mm])?)\s*[â€”\-]\s*([^:]+):\s*(.*)$"
)

# Text normalization
_INVISIBLE_MARKS = re.compile(r"[\u200e\u200f\u202a-\u202e]")


def _normalize_text(value: str) -> str:
    """Normalize unicode text."""
    normalized = unicodedata.normalize("NFKC", value)
    normalized = normalized.replace("\u202f", " ")
    return _INVISIBLE_MARKS.sub("", normalized)


def _parse_message_date(token: str) -> date | None:
    """Parse date token into a date object."""
    normalized = token.strip()
    if not normalized:
        return None

    # Try ISO format first
    try:
        parsed = date_parser.isoparse(normalized)
        parsed = parsed.replace(tzinfo=UTC) if parsed.tzinfo is None else parsed.astimezone(UTC)
        return parsed.date()
    except (TypeError, ValueError, OverflowError):
        pass

    # Try dayfirst=True then dayfirst=False
    for dayfirst in (True, False):
        try:
            parsed = date_parser.parse(normalized, dayfirst=dayfirst)
            parsed = parsed.replace(tzinfo=UTC) if parsed.tzinfo is None else parsed.astimezone(UTC)
            return parsed.date()
        except (TypeError, ValueError, OverflowError):
            continue

    return None


def _parse_message_time(time_token: str) -> datetime.time | None:
    """Parse time token into a time object (naive, for later localization)."""
    time_token = time_token.strip()

    # Check for AM/PM
    am_pm_match = re.search(r"([AaPp][Mm])$", time_token)
    if am_pm_match:
        am_pm = am_pm_match.group(1).upper()
        time_str = time_token[: am_pm_match.start()].strip()
        try:
            return datetime.strptime(f"{time_str} {am_pm}", "%I:%M %p").time()
        except ValueError:
            return None

    # 24-hour format
    try:
        return datetime.strptime(time_token, "%H:%M").time()
    except ValueError:
        return None


def parse_source(
    export: WhatsAppExport,
    timezone: str | ZoneInfo | None = None,
    *,
    expose_raw_author: bool = False,
) -> Table:
    """Parse WhatsApp export using pure Ibis/DuckDB operations.

    This function replaces pyparsing with vectorized SQL operations:
    1. Read all lines into table with line numbers
    2. Parse message headers with regex
    3. Group multi-line messages with window functions
    4. Aggregate continuation lines
    5. Return IR-compliant table

    Args:
        export: WhatsApp export metadata
        timezone: Timezone for timestamp normalization
        expose_raw_author: When True, keep the original WhatsApp author names in the
            returned table. Defaults to False so downstream consumers continue
            receiving anonymized author identifiers.

    Returns:
        Parsed Table conforming to MESSAGE_SCHEMA. Authors are anonymized by
        default unless ``expose_raw_author`` is set.

    """
    with zipfile.ZipFile(export.zip_path) as zf:
        validate_zip_contents(zf)
        ensure_safe_member_size(zf, export.chat_file)
        try:
            with zf.open(export.chat_file) as raw:
                text_stream = io.TextIOWrapper(raw, encoding="utf-8", errors="strict")
                lines = [line.rstrip("\n") for line in text_stream]
        except UnicodeDecodeError as exc:
            msg = f"Failed to decode chat file '{export.chat_file}': {exc}"
            raise ZipValidationError(msg) from exc

    if not lines:
        logger.warning("No messages found in %s", export.zip_path)
        empty_table = ibis.memtable([], schema=ibis.schema(MESSAGE_SCHEMA))
        return ensure_message_schema(empty_table, timezone=timezone)

    # Parse lines with pure Python (will be replaced with SQL in next step)
    rows = _parse_messages_pure_python(lines, export, timezone)

    if not rows:
        empty_table = ibis.memtable([], schema=ibis.schema(MESSAGE_SCHEMA))
        return ensure_message_schema(empty_table, timezone=timezone)

    messages = ibis.memtable(rows)
    if _IMPORT_ORDER_COLUMN in messages.columns:
        messages = messages.order_by([messages.timestamp, messages[_IMPORT_ORDER_COLUMN]])
    else:
        messages = messages.order_by("timestamp")

    messages = _add_message_ids(messages)

    if _IMPORT_ORDER_COLUMN in messages.columns:
        messages = messages.drop(_IMPORT_ORDER_COLUMN)

    messages = anonymize_table(messages)

    if not expose_raw_author and _AUTHOR_UUID_HEX_COLUMN in messages.columns:
        messages = messages.mutate(
            author=messages[_AUTHOR_UUID_HEX_COLUMN].substr(0, 8)
        )

    helper_columns = sorted({"author_raw", "author_uuid", _AUTHOR_UUID_HEX_COLUMN} & set(messages.columns))
    if helper_columns:
        messages = messages.drop(*helper_columns)

    return ensure_message_schema(messages, timezone=timezone)


def parse_multiple(
    exports: Sequence[WhatsAppExport],
    timezone: str | ZoneInfo | None = None,
    *,
    expose_raw_author: bool = False,
) -> Table:
    """Parse multiple exports and concatenate them ordered by timestamp."""
    tables: list[Table] = []

    for export in exports:
        try:
            table = parse_source(export, timezone=timezone, expose_raw_author=expose_raw_author)
            # Add import source tracking
            if table.count().execute() > 0:
                table = table.mutate(**{_IMPORT_SOURCE_COLUMN: ibis.literal(len(tables))})
                tables.append(table)
        except ZipValidationError as exc:
            logger.warning("Skipping %s due to unsafe ZIP: %s", export.zip_path.name, exc)
            continue

    if not tables:
        empty_table = ibis.memtable([], schema=ibis.schema(MESSAGE_SCHEMA))
        return ensure_message_schema(empty_table, timezone=timezone)

    combined = tables[0]
    for table in tables[1:]:
        combined = combined.union(table, distinct=False)

    # Re-order and add message IDs
    order_keys = [combined.timestamp]
    if _IMPORT_SOURCE_COLUMN in combined.columns:
        order_keys.append(combined[_IMPORT_SOURCE_COLUMN])
    combined = combined.order_by(order_keys)

    combined = _add_message_ids(combined)

    # Cleanup tracking columns
    if _IMPORT_SOURCE_COLUMN in combined.columns:
        combined = combined.drop(_IMPORT_SOURCE_COLUMN)

    return ensure_message_schema(combined, timezone=timezone)


def _parse_messages_pure_python(
    lines: list[str], export: WhatsAppExport, timezone: str | ZoneInfo | None
) -> list[dict]:
    """Parse messages using pure Python (transitional implementation).

    This is a simplified version without pyparsing dependency.
    Uses regex instead of pyparsing grammar.
    """
    rows: list[dict] = []
    current_date = export.export_date
    current_message: dict | None = None
    position = 0
    tz = _resolve_timezone(timezone)
    tenant_id = str(export.group_slug)
    source_identifier = "whatsapp"

    for raw_line in lines:
        # Normalize line
        stripped = raw_line.rstrip("\n")
        normalized = _normalize_text(stripped)
        trimmed = normalized.strip()

        # Empty lines are added to current message
        if not trimmed:
            if current_message is not None:
                current_message["_continuation_lines"].append("")
                current_message["_original_lines"].append("")
            continue

        # Try parsing as new message line
        match = WHATSAPP_LINE_PATTERN.match(trimmed)

        if not match:
            # Not a message line - append to current message
            if current_message is not None:
                current_message["_continuation_lines"].append(trimmed)
                current_message["_original_lines"].append(normalized)
            continue

        # Finalize previous message
        if current_message is not None:
            finalized = _finalize_message(current_message, tenant_id, source_identifier)
            # Skip empty messages (WhatsApp exports occasionally include blank lines)
            if finalized and finalized["message"] and finalized["message"].strip():
                finalized[_IMPORT_ORDER_COLUMN] = position
                rows.append(finalized)
                position += 1

        # Parse new message components (regex now captures author and message directly)
        date_str = match.group(1)
        time_str = match.group(2)
        author = _normalize_text(match.group(3).strip())
        message = _normalize_text(match.group(4).strip())

        # Parse date
        msg_date = _parse_message_date(date_str)
        if msg_date:
            current_date = msg_date
        else:
            msg_date = current_date

        # Parse time
        msg_time = _parse_message_time(time_str)
        if msg_time is None:
            # Invalid time, skip this line
            continue

        # Create timestamp
        timestamp = datetime.combine(msg_date, msg_time, tzinfo=tz)

        # Start new message
        current_message = {
            "timestamp": timestamp,
            "date": msg_date,
            "author": author,
            "author_raw": author,
            "_continuation_lines": [message],
            "_original_lines": [normalized],
        }

    # Finalize last message
    if current_message is not None:
        finalized = _finalize_message(current_message, tenant_id, source_identifier)
        # Skip empty messages (WhatsApp exports occasionally include blank lines)
        if finalized and finalized["message"] and finalized["message"].strip():
            finalized[_IMPORT_ORDER_COLUMN] = position
            rows.append(finalized)

    return rows


def _finalize_message(msg: dict, tenant_id: str, source: str) -> dict:
    """Finalize a message by joining continuation lines."""
    message_text = "\n".join(msg["_continuation_lines"]).strip()
    original_text = "\n".join(msg["_original_lines"]).strip()

    author_raw = msg["author_raw"]
    author_uuid = deterministic_author_uuid(tenant_id, source, author_raw)

    return {
        "timestamp": msg["timestamp"],
        "date": msg["date"],
        "author": author_raw,
        "author_raw": author_raw,
        "author_uuid": str(author_uuid),
        _AUTHOR_UUID_HEX_COLUMN: author_uuid.hex,
        "message": message_text,
        "original_line": original_text or None,
        "tagged_line": None,
    }


def _resolve_timezone(timezone: str | ZoneInfo | None) -> ZoneInfo:
    """Resolve timezone string or object to ZoneInfo."""
    if timezone is None:
        return UTC
    if isinstance(timezone, ZoneInfo):
        return timezone
    return ZoneInfo(timezone)


def _add_message_ids(messages: Table) -> Table:
    """Add deterministic message_id column based on milliseconds since group creation."""
    if int(messages.count().execute()) == 0:
        return messages

    min_timestamp = messages.timestamp.min()
    delta_ms = (
        ((messages.timestamp.epoch_seconds() - min_timestamp.epoch_seconds()) * 1000).round().cast("int64")
    )

    order_columns = [messages.timestamp]
    if _IMPORT_SOURCE_COLUMN in messages.columns:
        order_columns.append(messages[_IMPORT_SOURCE_COLUMN])
    if _IMPORT_ORDER_COLUMN in messages.columns:
        order_columns.append(messages[_IMPORT_ORDER_COLUMN])
    if "author" in messages.columns:
        order_columns.append(messages.author)
    if "message" in messages.columns:
        order_columns.append(messages.message)

    row_number = ibis.row_number().over(order_by=order_columns)
    return messages.mutate(message_id=delta_ms.cast("string") + "_" + row_number.cast("string"))


__all__ = ["parse_multiple", "parse_source"]
