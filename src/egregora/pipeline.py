"""Ultra-simple pipeline: parse → anonymize → group → enrich → write."""

import logging
import shutil
import re
import zipfile
from datetime import datetime
from pathlib import Path

import ibis
from google import genai
from ibis.expr.types import Table

from .enricher import enrich_dataframe, extract_and_replace_media
from .model_config import ModelConfig, load_site_config
from .models import WhatsAppExport
from .parser import extract_commands, filter_egregora_messages, parse_export
from .profiler import filter_opted_out_authors, process_commands
from .rag import VectorStore, index_all_media
from .site_config import SitePaths, resolve_site_paths
from .types import GroupSlug
from .writer import write_posts_for_period

logger = logging.getLogger(__name__)

SINGLE_DIGIT_THRESHOLD = 10

PLACEHOLDER_FILES = {".gitkeep", "index.md", "README.md"}


def _contains_real_content(directory: Path) -> bool:
    """Check if a directory has files other than scaffold placeholders."""

    for child in directory.iterdir():
        if child.name in PLACEHOLDER_FILES or child.name.startswith("."):
            continue
        return True

    return False


def _migrate_directory(source: Path, target: Path, label: str) -> None:
    """Move legacy content directory into the current docs tree."""

    try:
        source_resolved = source.resolve()
        target_resolved = target.resolve()
    except FileNotFoundError:
        return

    if source_resolved == target_resolved or not source_resolved.exists():
        return

    items = list(source_resolved.iterdir())
    if not items:
        return

    if target_resolved.exists() and _contains_real_content(target_resolved):
        logger.debug(
            "Skipping %s migration: target %s already contains content", label, target_resolved
        )
        return

    target_resolved.mkdir(parents=True, exist_ok=True)

    for item in items:
        shutil.move(str(item), str(target_resolved / item.name))

    try:
        source_resolved.rmdir()
    except OSError:
        pass

    logger.info("Migrated %s directory from %s to %s", label, source_resolved, target_resolved)


def _migrate_legacy_structure(site_paths: SitePaths) -> None:
    """Normalize legacy site structure generated by older scaffold versions."""

    legacy_posts = site_paths.site_root / "posts"
    legacy_profiles = site_paths.site_root / "profiles"
    legacy_media = site_paths.site_root / "media"

    _migrate_directory(legacy_posts, site_paths.posts_dir, "posts")
    _migrate_directory(legacy_profiles, site_paths.profiles_dir, "profiles")
    _migrate_directory(legacy_media, site_paths.media_dir, "media")


def discover_chat_file(zip_path: Path) -> tuple[str, str]:
    """Find the chat .txt file in the ZIP and extract group name."""

    with zipfile.ZipFile(zip_path) as zf:
        for member in zf.namelist():
            if member.endswith(".txt") and not member.startswith("__"):
                # Generic pattern to capture group name from WhatsApp chat files
                pattern = r"WhatsApp(?: Chat with|.*) (.+)\.txt"
                match = re.match(pattern, Path(member).name)
                if match:
                    return match.group(1), member
                return Path(member).stem, member

    raise ValueError(f"No WhatsApp chat file found in {zip_path}")


def period_has_posts(period_key: str, posts_dir: Path) -> bool:
    """Check if posts already exist for this period."""
    if not posts_dir.exists():
        return False

    # Look for files matching {period_key}-*.md
    pattern = f"{period_key}-*.md"
    existing_posts = list(posts_dir.glob(pattern))

    return len(existing_posts) > 0


def group_by_period(df: Table, period: str = "day") -> dict[str, Table]:
    """
    Group Table by time period.

    Args:
        df: Table with timestamp column
        period: "day", "week", or "month"

    Returns:
        Dict mapping period string to Table
    """
    if df.count().execute() == 0:
        return {}

    if period == "day":
        df = df.mutate(period=df.timestamp.date().cast("string"))
    elif period == "week":
        # ISO week format: YYYY-Wnn
        year_str = df.timestamp.year().cast("string")
        week_num = df.timestamp.week_of_year()
        week_str = ibis.ifelse(
            week_num < SINGLE_DIGIT_THRESHOLD,
            ibis.literal("0") + week_num.cast("string"),
            week_num.cast("string")
        )
        df = df.mutate(period=year_str + ibis.literal("-W") + week_str)
    elif period == "month":
        # Format: YYYY-MM
        year_str = df.timestamp.year().cast("string")
        month_num = df.timestamp.month()
        # Zero-pad month: use lpad to ensure 2 digits
        month_str = ibis.ifelse(
            month_num < SINGLE_DIGIT_THRESHOLD,
            ibis.literal("0") + month_num.cast("string"),
            month_num.cast("string")
        )
        df = df.mutate(period=year_str + ibis.literal("-") + month_str)
    else:
        raise ValueError(f"Unknown period: {period}")

    grouped = {}
    # Get unique period values, sorted
    period_values = sorted(df.select("period").distinct().execute()["period"].tolist())

    for period_value in period_values:
        period_df = df.filter(df.period == period_value).drop("period")
        grouped[period_value] = period_df

    return grouped


async def process_whatsapp_export(  # noqa: PLR0912, PLR0913, PLR0915
    zip_path: Path,
    output_dir: Path = Path("output"),
    period: str = "day",
    enable_enrichment: bool = True,
    from_date=None,
    to_date=None,
    timezone=None,
    gemini_api_key: str | None = None,
    model: str | None = None,
) -> dict[str, dict[str, list[str]]]:
    """
    Complete pipeline: ZIP → posts + profiles.

    Args:
        zip_path: WhatsApp export ZIP file
        output_dir: Where to save posts and profiles
        period: "day", "week", or "month"
        enable_enrichment: Add URL/media context
        from_date: Only process messages from this date onwards (date object)
        to_date: Only process messages up to this date (date object)
        timezone: ZoneInfo timezone object (WhatsApp export phone timezone)
        gemini_api_key: Google Gemini API key
        model: Gemini model to use (overrides mkdocs.yml config)

    Returns:
        Dict mapping period to {'posts': [...], 'profiles': [...]}
    """

    output_dir = output_dir.expanduser().resolve()
    site_paths = resolve_site_paths(output_dir)

    # Validate MkDocs scaffold exists before proceeding
    if not site_paths.mkdocs_path or not site_paths.mkdocs_path.exists():
        raise ValueError(
            f"No mkdocs.yml found for site at {output_dir}. "
            "Run 'egregora init <site-dir>' before processing exports."
        )

    if not site_paths.docs_dir.exists():
        raise ValueError(
            f"Docs directory not found: {site_paths.docs_dir}. "
            "Re-run 'egregora init' to scaffold the MkDocs project."
        )

    # Move legacy structures (from older scaffolds) into docs_dir if needed
    _migrate_legacy_structure(site_paths)

    # Load site config and create model config
    site_config = load_site_config(site_paths.site_root)
    model_config = ModelConfig(cli_model=model, site_config=site_config)

    client: genai.Client | None = None
    try:
        client = genai.Client(api_key=gemini_api_key)

        logger.info("Parsing export: %s", zip_path)
        group_name, chat_file = discover_chat_file(zip_path)
        group_slug = GroupSlug(group_name.lower().replace(" ", "-"))
        logger.info("Discovered chat '%s' (file: %s)", group_name, chat_file)

        export = WhatsAppExport(
            zip_path=zip_path,
            group_name=group_name,
            group_slug=group_slug,
            export_date=datetime.now().date(),
            chat_file=chat_file,
            media_files=[],
        )

        # Parse and anonymize (with timezone from phone)
        df = parse_export(export, timezone=timezone)
        total_messages = df.count().execute()
        logger.info("Loaded %s messages after parsing", total_messages)

        # Ensure key directories exist and live inside docs/
        content_dirs = {
            "posts": site_paths.posts_dir,
            "profiles": site_paths.profiles_dir,
            "media": site_paths.media_dir,
        }
        for label, directory in content_dirs.items():
            try:
                directory.relative_to(site_paths.docs_dir)
            except ValueError as exc:
                raise ValueError(
                    f"{label.capitalize()} directory must reside inside the MkDocs docs_dir. "
                    f"Expected parent {site_paths.docs_dir}, got {directory}."
                ) from exc
            directory.mkdir(parents=True, exist_ok=True)

        # Extract and process egregora commands (before filtering)
        commands = extract_commands(df)
        if commands:
            process_commands(commands, site_paths.profiles_dir)
            logger.info(f"Processed {len(commands)} egregora commands")
        else:
            logger.info("No egregora commands found in this export")

        # Remove ALL /egregora messages (commands + ad-hoc exclusions)
        df, egregora_removed = filter_egregora_messages(df)
        if egregora_removed:
            logger.info("Removed %s /egregora messages", egregora_removed)

        # Filter out opted-out authors EARLY (before any processing)
        df, removed_count = filter_opted_out_authors(df, site_paths.profiles_dir)
        if removed_count > 0:
            logger.warning(f"⚠️  Total: {removed_count} messages removed from opted-out users")

        # Filter by date range if specified
        if from_date or to_date:
            original_count = df.count().execute()

            if from_date and to_date:
                df = df.filter(
                    (df.timestamp.date() >= from_date) & (df.timestamp.date() <= to_date)
                )
                logger.info(f"📅 Filtering messages from {from_date} to {to_date}")
            elif from_date:
                df = df.filter(df.timestamp.date() >= from_date)
                logger.info(f"📅 Filtering messages from {from_date} onwards")
            elif to_date:
                df = df.filter(df.timestamp.date() <= to_date)
                logger.info(f"📅 Filtering messages up to {to_date}")

            filtered_count = df.count().execute()
            removed_by_date = original_count - filtered_count

            if removed_by_date > 0:
                logger.info(
                    f"🗓️  Filtered out {removed_by_date} messages by date (kept {filtered_count})"
                )
            else:
                logger.info(f"✓ All {filtered_count} messages are within the specified date range")

        # Group by period first (media extraction handled per-period)
        logger.info("🎯 Grouping messages by period='%s'", period)
        periods = group_by_period(df, period)
        if not periods:
            logger.info("No periods found after grouping")
            return {}

        results = {}
        posts_dir = site_paths.posts_dir
        profiles_dir = site_paths.profiles_dir

        for period_key in sorted(periods.keys()):
            period_df = periods[period_key]
            period_count = period_df.count().execute()
            logger.info("➡️  Period %s — %s messages", period_key, period_count)

            # Early exit: skip if posts already exist for this period
            if period_has_posts(period_key, posts_dir):
                logger.info("↺ Skipping %s — posts already exist", period_key)
                existing_posts = list(posts_dir.glob(f"{period_key}-*.md"))
                results[period_key] = {"posts": [str(p) for p in existing_posts], "profiles": []}
                continue

            # Extract and replace media for this period only
            period_df, media_mapping = extract_and_replace_media(
                period_df,
                zip_path,
                site_paths.docs_dir,
                posts_dir,
                str(group_slug),
            )

            logger.info(f"Processing {period_key}...")

            enriched_df = period_df

            # Optionally add LLM-generated enrichment rows
            if enable_enrichment:
                logger.info("✨ Enriching period %s", period_key)
                enriched_df = await enrich_dataframe(
                    period_df,
                    media_mapping,
                    client,
                    site_paths.docs_dir,
                    posts_dir,
                    model_config,
                )

            enriched_dir = site_paths.enriched_dir
            enriched_dir.mkdir(parents=True, exist_ok=True)
            enriched_path = enriched_dir / f"{period_key}-enriched.csv"
            # Write CSV using Ibis - need to execute to pandas first
            enriched_df.execute().to_csv(enriched_path, index=False)
            logger.info("Saved enrichment data for %s to %s", period_key, enriched_path)

            result = await write_posts_for_period(
                enriched_df,
                period_key,
                client,
                posts_dir,
                profiles_dir,
                site_paths.rag_dir,
                model_config,
            )

            results[period_key] = result
            logger.info(
                "Generated %s posts and %s profiles for %s",
                len(result.get("posts", [])),
                len(result.get("profiles", [])),
                period_key,
            )

        # Index all media enrichments into RAG (if enrichment was enabled)
        if enable_enrichment and results:
            logger.info("Indexing media enrichments into RAG...")
            try:
                rag_dir = site_paths.rag_dir
                store = VectorStore(rag_dir / "chunks.parquet")
                media_chunks = await index_all_media(site_paths.docs_dir, client, store)
                if media_chunks > 0:
                    logger.info(f"✓ Indexed {media_chunks} media chunks into RAG")
                else:
                    logger.info("No media enrichments to index for this run")
            except Exception as e:
                logger.error(f"Failed to index media into RAG: {e}")

        return results
    finally:
        if client:
            client.close()
