"""Ultra-simple pipeline: parse â†’ anonymize â†’ group â†’ enrich â†’ write."""

import logging
import shutil
import re
import zipfile
from datetime import datetime
from pathlib import Path

import ibis
from google import genai
from ibis.expr.types import Table

from .enricher import enrich_dataframe, extract_and_replace_media
from .model_config import ModelConfig, load_site_config
from .models import WhatsAppExport
from .parser import extract_commands, filter_egregora_messages, parse_export
from .profiler import filter_opted_out_authors, process_commands
from .rag import VectorStore, index_all_media
from .site_config import SitePaths, resolve_site_paths
from .types import GroupSlug
from .writer import write_posts_for_period

logger = logging.getLogger(__name__)

SINGLE_DIGIT_THRESHOLD = 10

PLACEHOLDER_FILES = {".gitkeep", "index.md", "README.md"}


def _contains_real_content(directory: Path) -> bool:
    """Check if a directory has files other than scaffold placeholders."""

    for child in directory.iterdir():
        if child.name in PLACEHOLDER_FILES or child.name.startswith("."):
            continue
        return True

    return False


def _migrate_directory(source: Path, target: Path, label: str) -> None:
    """Move legacy content directory into the current docs tree."""

    try:
        source_resolved = source.resolve()
        target_resolved = target.resolve()
    except FileNotFoundError:
        return

    if source_resolved == target_resolved or not source_resolved.exists():
        return

    items = list(source_resolved.iterdir())
    if not items:
        return

    if target_resolved.exists() and _contains_real_content(target_resolved):
        logger.debug(
            "Skipping %s migration: target %s already contains content", label, target_resolved
        )
        return

    target_resolved.mkdir(parents=True, exist_ok=True)

    for item in items:
        shutil.move(str(item), str(target_resolved / item.name))

    try:
        source_resolved.rmdir()
    except OSError:
        pass

    logger.info("Migrated %s directory from %s to %s", label, source_resolved, target_resolved)


def _migrate_legacy_structure(site_paths: SitePaths) -> None:
    """Normalize legacy site structure generated by older scaffold versions."""

    legacy_posts = site_paths.site_root / "posts"
    legacy_profiles = site_paths.site_root / "profiles"
    legacy_media = site_paths.site_root / "media"

    _migrate_directory(legacy_posts, site_paths.posts_dir, "posts")
    _migrate_directory(legacy_profiles, site_paths.profiles_dir, "profiles")
    _migrate_directory(legacy_media, site_paths.media_dir, "media")


def discover_chat_file(zip_path: Path) -> tuple[str, str]:
    """Find the chat .txt file in the ZIP and extract group name."""

    with zipfile.ZipFile(zip_path) as zf:
        for member in zf.namelist():
            if member.endswith(".txt") and not member.startswith("__"):
                # Generic pattern to capture group name from WhatsApp chat files
                pattern = r"WhatsApp(?: Chat with|.*) (.+)\.txt"
                match = re.match(pattern, Path(member).name)
                if match:
                    return match.group(1), member
                return Path(member).stem, member

    raise ValueError(f"No WhatsApp chat file found in {zip_path}")


def period_has_posts(period_key: str, posts_dir: Path) -> bool:
    """Check if posts already exist for this period."""
    if not posts_dir.exists():
        return False

    # Look for files matching {period_key}-*.md
    pattern = f"{period_key}-*.md"
    existing_posts = list(posts_dir.glob(pattern))

    return len(existing_posts) > 0


def group_by_period(df: Table, period: str = "day") -> dict[str, Table]:
    """
    Group Table by time period.

    Args:
        df: Table with timestamp column
        period: "day", "week", or "month"

    Returns:
        Dict mapping period string to Table
    """
    if df.count().execute() == 0:
        return {}

    if period == "day":
        df = df.mutate(period=df.timestamp.date().cast("string"))
    elif period == "week":
        # ISO week format: YYYY-Wnn
        year_str = df.timestamp.year().cast("string")
        week_num = df.timestamp.week_of_year()
        week_str = ibis.ifelse(
            week_num < SINGLE_DIGIT_THRESHOLD,
            ibis.literal("0") + week_num.cast("string"),
            week_num.cast("string")
        )
        df = df.mutate(period=year_str + ibis.literal("-W") + week_str)
    elif period == "month":
        # Format: YYYY-MM
        year_str = df.timestamp.year().cast("string")
        month_num = df.timestamp.month()
        # Zero-pad month: use lpad to ensure 2 digits
        month_str = ibis.ifelse(
            month_num < SINGLE_DIGIT_THRESHOLD,
            ibis.literal("0") + month_num.cast("string"),
            month_num.cast("string")
        )
        df = df.mutate(period=year_str + ibis.literal("-") + month_str)
    else:
        raise ValueError(f"Unknown period: {period}")

    grouped = {}
    # Get unique period values, sorted
    period_values = sorted(df.select("period").distinct().execute()["period"].tolist())

    for period_value in period_values:
        period_df = df.filter(df.period == period_value).drop("period")
        grouped[period_value] = period_df

    return grouped


async def process_whatsapp_export(  # noqa: PLR0912, PLR0913, PLR0915
    zip_path: Path,
    output_dir: Path = Path("output"),
    period: str = "day",
    enable_enrichment: bool = True,
    from_date=None,
    to_date=None,
    timezone=None,
    gemini_api_key: str | None = None,
    model: str | None = None,
) -> dict[str, dict[str, list[str]]]:
    """
    Complete pipeline: ZIP â†’ posts + profiles.

    Args:
        zip_path: WhatsApp export ZIP file
        output_dir: Where to save posts and profiles
        period: "day", "week", or "month"
        enable_enrichment: Add URL/media context
        from_date: Only process messages from this date onwards (date object)
        to_date: Only process messages up to this date (date object)
        timezone: ZoneInfo timezone object (WhatsApp export phone timezone)
        gemini_api_key: Google Gemini API key
        model: Gemini model to use (overrides mkdocs.yml config)

    Returns:
        Dict mapping period to {'posts': [...], 'profiles': [...]}
    """

    output_dir = output_dir.expanduser().resolve()
    site_paths = resolve_site_paths(output_dir)

    # Validate MkDocs scaffold exists before proceeding
    if not site_paths.mkdocs_path or not site_paths.mkdocs_path.exists():
        raise ValueError(
            f"No mkdocs.yml found for site at {output_dir}. "
            "Run 'egregora init <site-dir>' before processing exports."
        )

    if not site_paths.docs_dir.exists():
        raise ValueError(
            f"Docs directory not found: {site_paths.docs_dir}. "
            "Re-run 'egregora init' to scaffold the MkDocs project."
        )

    # Move legacy structures (from older scaffolds) into docs_dir if needed
    _migrate_legacy_structure(site_paths)

    # Load site config and create model config
    site_config = load_site_config(site_paths.site_root)
    model_config = ModelConfig(cli_model=model, site_config=site_config)

    client: genai.Client | None = None
    try:
        client = genai.Client(api_key=gemini_api_key)

        logger.info("Parsing export: %s", zip_path)
        group_name, chat_file = discover_chat_file(zip_path)
        group_slug = GroupSlug(group_name.lower().replace(" ", "-"))
        logger.info("Discovered chat '%s' (file: %s)", group_name, chat_file)

        export = WhatsAppExport(
            zip_path=zip_path,
            group_name=group_name,
            group_slug=group_slug,
            export_date=datetime.now().date(),
            chat_file=chat_file,
            media_files=[],
        )

        # Parse and anonymize (with timezone from phone)
        df = parse_export(export, timezone=timezone)
        total_messages = df.count().execute()
        logger.info("Loaded %s messages after parsing", total_messages)

        # Ensure key directories exist and live inside docs/
        content_dirs = {
            "posts": site_paths.posts_dir,
            "profiles": site_paths.profiles_dir,
            "media": site_paths.media_dir,
        }
        for label, directory in content_dirs.items():
            try:
                directory.relative_to(site_paths.docs_dir)
            except ValueError as exc:
                raise ValueError(
                    f"{label.capitalize()} directory must reside inside the MkDocs docs_dir. "
                    f"Expected parent {site_paths.docs_dir}, got {directory}."
                ) from exc
            directory.mkdir(parents=True, exist_ok=True)

        # Extract and process egregora commands (before filtering)
        commands = extract_commands(df)
        if commands:
            process_commands(commands, site_paths.profiles_dir)
            logger.info(f"Processed {len(commands)} egregora commands")
        else:
            logger.info("No egregora commands found in this export")

        # Remove ALL /egregora messages (commands + ad-hoc exclusions)
        df, egregora_removed = filter_egregora_messages(df)
        if egregora_removed:
            logger.info("Removed %s /egregora messages", egregora_removed)

        # Filter out opted-out authors EARLY (before any processing)
        df, removed_count = filter_opted_out_authors(df, site_paths.profiles_dir)
        if removed_count > 0:
            logger.warning(f"âš ï¸  Total: {removed_count} messages removed from opted-out users")

        # Filter by date range if specified
        if from_date or to_date:
            original_count = df.count().execute()

            if from_date and to_date:
                df = df.filter(
                    (df.timestamp.date() >= from_date) & (df.timestamp.date() <= to_date)
                )
                logger.info(f"ðŸ“… Filtering messages from {from_date} to {to_date}")
            elif from_date:
                df = df.filter(df.timestamp.date() >= from_date)
                logger.info(f"ðŸ“… Filtering messages from {from_date} onwards")
            elif to_date:
                df = df.filter(df.timestamp.date() <= to_date)
                logger.info(f"ðŸ“… Filtering messages up to {to_date}")

            filtered_count = df.count().execute()
            removed_by_date = original_count - filtered_count

            if removed_by_date > 0:
                logger.info(
                    f"ðŸ—“ï¸  Filtered out {removed_by_date} messages by date (kept {filtered_count})"
                )
            else:
                logger.info(f"âœ“ All {filtered_count} messages are within the specified date range")

        # Group by period first (media extraction handled per-period)
        logger.info("ðŸŽ¯ Grouping messages by period='%s'", period)
        periods = group_by_period(df, period)
        if not periods:
            logger.info("No periods found after grouping")
            return {}

        results = {}
        posts_dir = site_paths.posts_dir
        profiles_dir = site_paths.profiles_dir

        for period_key in sorted(periods.keys()):
            period_df = periods[period_key]
            period_count = period_df.count().execute()
            logger.info("âž¡ï¸  Period %s â€” %s messages", period_key, period_count)

            # Early exit: skip if posts already exist for this period
            if period_has_posts(period_key, posts_dir):
                logger.info("â†º Skipping %s â€” posts already exist", period_key)
                existing_posts = list(posts_dir.glob(f"{period_key}-*.md"))
                results[period_key] = {"posts": [str(p) for p in existing_posts], "profiles": []}
                continue

            # Extract and replace media for this period only
            period_df, media_mapping = extract_and_replace_media(
                period_df,
                zip_path,
                site_paths.docs_dir,
                posts_dir,
                str(group_slug),
            )

            logger.info(f"Processing {period_key}...")

            enriched_df = period_df

            # Optionally add LLM-generated enrichment rows
            if enable_enrichment:
                logger.info("âœ¨ Enriching period %s", period_key)
                enriched_df = await enrich_dataframe(
                    period_df,
                    media_mapping,
                    client,
                    site_paths.docs_dir,
                    posts_dir,
                    model_config,
                )

            enriched_dir = site_paths.enriched_dir
            enriched_dir.mkdir(parents=True, exist_ok=True)
            enriched_path = enriched_dir / f"{period_key}-enriched.csv"
            # Write CSV using Ibis - need to execute to pandas first
            enriched_df.execute().to_csv(enriched_path, index=False)
            logger.info("Saved enrichment data for %s to %s", period_key, enriched_path)

            result = await write_posts_for_period(
                enriched_df,
                period_key,
                client,
                posts_dir,
                profiles_dir,
                site_paths.rag_dir,
                model_config,
            )

            results[period_key] = result
            logger.info(
                "Generated %s posts and %s profiles for %s",
                len(result.get("posts", [])),
                len(result.get("profiles", [])),
                period_key,
            )

        # Index all media enrichments into RAG (if enrichment was enabled)
        if enable_enrichment and results:
            logger.info("Indexing media enrichments into RAG...")
            try:
                rag_dir = site_paths.rag_dir
                store = VectorStore(rag_dir / "chunks.parquet")
                media_chunks = await index_all_media(site_paths.docs_dir, client, store)
                if media_chunks > 0:
                    logger.info(f"âœ“ Indexed {media_chunks} media chunks into RAG")
                else:
                    logger.info("No media enrichments to index for this run")
            except Exception as e:
                logger.error(f"Failed to index media into RAG: {e}")

        return results
    finally:
        if client:
            client.close()
