{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import unicodedata\n",
    "import zipfile\n",
    "from datetime import date, datetime\n",
    "\n",
    "import polars as pl\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "ZIP_PATH = \"tests/data/zips/Conversa do WhatsApp com Teste.zip\"\n",
    "CHAT_FILE = \"Conversa do WhatsApp com Teste.txt\"\n",
    "GROUP_SLUG = \"test-group\"\n",
    "GROUP_NAME = \"Test Group\"\n",
    "# Set a fallback date for messages that don't have one (usually the first messages in the file)\n",
    "EXPORT_DATE = date(2023, 1, 1)\n",
    "\n",
    "# --- 2. Parsing Logic (adapted from src/egregora/parser.py) ---\n",
    "\n",
    "# Regex to capture date, time, author, and message from a WhatsApp chat line\n",
    "_LINE_PATTERN = re.compile(\n",
    "    r\"^\"\n",
    "    r\"(?:(?P<date>\\\\d{1,2}/\\\\d{1,2}/\\\\d{2,4})(?:,\\\\s*|\\\\s+))?\"\n",
    "    r\"(?P<time>\\\\d{1,2}:\\\\d{2})\"\n",
    "    r\"(?:\\\\s*(?P<ampm>[APap][Mm]))?\"\n",
    "    r\"\\\\s*[â€”\\\\-]\\\\s*\"\n",
    "    r\"(?P<author>[^:]+?):\\\\s*\"\n",
    "    r\"(?P<message>.+)\"\n",
    "    r\"$\"\n",
    ")\n",
    "_INVISIBLE_MARKS = re.compile(r\"[\\\\u200e\\\\u200f\\\\u202a-\\\\u202e]\")\n",
    "_DATE_PARSE_PREFERENCES = ({\"dayfirst\": True}, {\"dayfirst\": False})\n",
    "\n",
    "def _normalize_text(value: str) -> str:\n",
    "    \"\"\"Normalize text by applying NFKC normalization and removing invisible characters.\"\"\"\n",
    "    normalized = unicodedata.normalize(\"NFKC\", value)\n",
    "    normalized = normalized.replace(\"\\u202f\", \" \")\n",
    "    normalized = _INVISIBLE_MARKS.sub(\"\", normalized)\n",
    "    return normalized\n",
    "\n",
    "def _parse_with_preferences(value: str) -> datetime | None:\n",
    "    \"\"\"Attempt to parse a date string using a list of format preferences.\"\"\"\n",
    "    for options in _DATE_PARSE_PREFERENCES:\n",
    "        try:\n",
    "            return date_parser.parse(value, **options)\n",
    "        except (TypeError, ValueError, OverflowError):\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _parse_message_date(token: str) -> date | None:\n",
    "    \"\"\"Parse a date token into a UTC date object.\"\"\"\n",
    "    normalized = token.strip()\n",
    "    if not normalized:\n",
    "        return None\n",
    "    try:\n",
    "        parsed = _parse_with_preferences(normalized)\n",
    "        if parsed is None:\n",
    "            return None\n",
    "        return parsed.date()\n",
    "    except (TypeError, ValueError, OverflowError):\n",
    "        return None\n",
    "\n",
    "def _parse_message_time(time_token: str, am_pm: str | None):\n",
    "    \"\"\"Parse a time token into a time object.\"\"\"\n",
    "    try:\n",
    "        if am_pm:\n",
    "            return datetime.strptime(f\"{time_token} {am_pm.upper()}\", \"%I:%M %p\").time()\n",
    "        return datetime.strptime(time_token, \"%H:%M\").time()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# A simple class to build multi-line messages\n",
    "class _MessageBuilder:\n",
    "    def __init__(self, *, timestamp: datetime, author: str, group_slug: str, group_name: str):\n",
    "        self.timestamp = timestamp\n",
    "        self.author = author\n",
    "        self.group_slug = group_slug\n",
    "        self.group_name = group_name\n",
    "        self._message_lines: list[str] = []\n",
    "\n",
    "    def append(self, content: str):\n",
    "        self._message_lines.append(content)\n",
    "\n",
    "    def finalize(self) -> dict:\n",
    "        message_text = \"\\n\".join(self._message_lines).strip()\n",
    "        return {\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"date\": self.timestamp.date(),\n",
    "            \"time\": self.timestamp.strftime(\"%H:%M\"),\n",
    "            \"author\": self.author,\n",
    "            \"message\": message_text,\n",
    "            \"group_slug\": self.group_slug,\n",
    "            \"group_name\": self.group_name,\n",
    "        }\n",
    "\n",
    "# Main parsing loop\n",
    "rows = []\n",
    "builder: _MessageBuilder | None = None\n",
    "current_date = EXPORT_DATE\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH) as zf:\n",
    "    with zf.open(CHAT_FILE) as raw:\n",
    "        text_stream = io.TextIOWrapper(raw, encoding=\"utf-8\")\n",
    "        \n",
    "        for line in text_stream:\n",
    "            normalized_line = _normalize_text(line.rstrip(\"\\n\"))\n",
    "            trimmed_line = normalized_line.strip()\n",
    "\n",
    "            if not trimmed_line:\n",
    "                if builder:\n",
    "                    builder.append(\"\")\n",
    "                continue\n",
    "\n",
    "            match = _LINE_PATTERN.match(trimmed_line)\n",
    "            if not match:\n",
    "                if builder:\n",
    "                    builder.append(trimmed_line)\n",
    "                continue\n",
    "\n",
    "            if builder:\n",
    "                rows.append(builder.finalize())\n",
    "\n",
    "            date_token = match.group(\"date\")\n",
    "            msg_date = _parse_message_date(date_token) if date_token else current_date\n",
    "            if msg_date:\n",
    "                current_date = msg_date\n",
    "\n",
    "            msg_time = _parse_message_time(match.group(\"time\"), match.group(\"ampm\"))\n",
    "            if not msg_time:\n",
    "                continue\n",
    "            \n",
    "            timestamp = datetime.combine(current_date, msg_time)\n",
    "            author = _normalize_text(match.group(\"author\").strip())\n",
    "            initial_message = _normalize_text(match.group(\"message\").strip())\n",
    "\n",
    "            builder = _MessageBuilder(\n",
    "                timestamp=timestamp,\n",
    "                author=author,\n",
    "                group_slug=GROUP_SLUG,\n",
    "                group_name=GROUP_NAME,\n",
    "            )\n",
    "            builder.append(initial_message)\n",
    "\n",
    "if builder:\n",
    "    rows.append(builder.finalize())\n",
    "\n",
    "# --- 3. Create and Display DataFrame ---\n",
    "df_parsed = pl.DataFrame()\n",
    "if rows:\n",
    "    df_parsed = pl.DataFrame(rows).sort(\"timestamp\")\n",
    "    # Ensure schema has placeholder columns for later steps\n",
    "    if \"anon_author\" not in df_parsed.columns:\n",
    "        df_parsed = df_parsed.with_columns(pl.lit(None, dtype=pl.String).alias(\"anon_author\"))\n",
    "    if \"enriched_summary\" not in df_parsed.columns:\n",
    "        df_parsed = df_parsed.with_columns(pl.lit(None, dtype=pl.String).alias(\"enriched_summary\"))\n",
    "\n",
    "print(\"### Parsed DataFrame:\")\n",
    "df_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import uuid\n",
    "from pathlib import Path, PurePosixPath\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MEDIA_OUTPUT_DIR = Path(\"media\")\n",
    "MEDIA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 2. Media Extraction Logic (adapted from src/egregora/media_extractor.py) ---\n",
    "\n",
    "# Regex to find attachment markers in message text\n",
    "_ATTACHMENT_MARKERS = (\"(arquivo anexado)\", \"(file attached)\")\n",
    "_ATTACHMENT_PATTERN = re.compile(\n",
    "    r\"[^\n]*?(?:\" + \"|\".join(re.escape(marker) for marker in _ATTACHMENT_MARKERS) + r\")\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "MEDIA_TYPE_BY_EXTENSION = {\n",
    "    \".jpg\": \"image\", \".jpeg\": \"image\", \".png\": \"image\", \".gif\": \"image\", \".webp\": \"image\",\n",
    "    \".mp4\": \"video\", \".mov\": \"video\",\n",
    "    \".opus\": \"audio\", \".mp3\": \"audio\",\n",
    "    \".pdf\": \"document\",\n",
    "}\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class MediaFile:\n",
    "    filename: str\n",
    "    media_type: str\n",
    "    dest_path: Path\n",
    "    relative_path: str\n",
    "\n",
    "def _detect_media_type(filename: str) -> str | None:\n",
    "    extension = Path(filename).suffix.lower()\n",
    "    return MEDIA_TYPE_BY_EXTENSION.get(extension)\n",
    "\n",
    "def _clean_attachment_name(filename: str) -> str:\n",
    "    return filename.translate(str.maketrans(\"\", \"\", \"\\u200e\\u200f\\u202a-\\u202e\")).strip()\n",
    "\n",
    "# Find all attachment references in the DataFrame\n",
    "attachment_names = set()\n",
    "for message in df_parsed.get_column(\"message\"):\n",
    "    if not message: continue\n",
    "    for match in _ATTACHMENT_PATTERN.finditer(message):\n",
    "        segment = match.group(0)\n",
    "        for marker in _ATTACHMENT_MARKERS:\n",
    "            if marker in segment.lower():\n",
    "                prefix = segment.lower().split(marker)[0]\n",
    "                if \": \" in prefix:\n",
    "                    attachment_name = prefix.rsplit(\": \", 1)[1]\n",
    "                else:\n",
    "                    attachment_name = prefix\n",
    "                attachment_names.add(_clean_attachment_name(attachment_name.strip()))\n",
    "\n",
    "# Extract media files from the zip\n",
    "media_files: dict[str, MediaFile] = {}\n",
    "namespace = uuid.uuid5(uuid.NAMESPACE_DNS, GROUP_SLUG)\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    for info in zf.infolist():\n",
    "        if info.is_dir(): continue\n",
    "        original_name = Path(info.filename).name\n",
    "        cleaned_name = _clean_attachment_name(original_name)\n",
    "        if cleaned_name in attachment_names:\n",
    "            media_type = _detect_media_type(cleaned_name)\n",
    "            if not media_type: continue\n",
    "\n",
    "            with zf.open(info, \"r\") as source:\n",
    "                file_content = source.read()\n",
    "\n",
    "            content_hash = hashlib.sha256(file_content).hexdigest()\n",
    "            file_uuid = uuid.uuid5(namespace, content_hash)\n",
    "            file_extension = Path(cleaned_name).suffix\n",
    "            new_filename = f\"{file_uuid}{file_extension}\"\n",
    "\n",
    "            dest_path = MEDIA_OUTPUT_DIR / new_filename\n",
    "            if not dest_path.exists():\n",
    "                dest_path.write_bytes(file_content)\n",
    "\n",
    "            relative_path = str(PurePosixPath(os.path.relpath(dest_path, \".\")))\n",
    "            media_files[cleaned_name] = MediaFile(\n",
    "                filename=new_filename,\n",
    "                media_type=media_type,\n",
    "                dest_path=dest_path,\n",
    "                relative_path=relative_path,\n",
    "            )\n",
    "\n",
    "# --- 3. Update DataFrame with Media Links ---\n",
    "\n",
    "def _format_markdown_reference(media: MediaFile) -> str:\n",
    "    if media.media_type == \"image\":\n",
    "        return f\"![{media.filename}]({media.relative_path})\"\n",
    "    return f\"[{media.filename}]({media.relative_path})\"\n",
    "\n",
    "def _replace_media_references(text: str | None) -> str:\n",
    "    if not text: return \"\"\n",
    "    def replacement(match: re.Match[str]) -> str:\n",
    "        segment = match.group(0)\n",
    "        for marker in _ATTACHMENT_MARKERS:\n",
    "            if marker in segment.lower():\n",
    "                prefix = segment.lower().split(marker)[0]\n",
    "                if \": \" in prefix:\n",
    "                    attachment_name = prefix.rsplit(\": \", 1)[1]\n",
    "                else:\n",
    "                    attachment_name = prefix\n",
    "                cleaned_name = _clean_attachment_name(attachment_name.strip())\n",
    "                media = media_files.get(cleaned_name)\n",
    "                if media:\n",
    "                    return segment.replace(f\"{attachment_name.strip()} {marker}\", _format_markdown_reference(media))\n",
    "        return segment\n",
    "    return _ATTACHMENT_PATTERN.sub(replacement, text)\n",
    "\n",
    "df_with_media = df_parsed.with_columns(\n",
    "    pl.col(\"message\").map_elements(_replace_media_references, return_dtype=pl.String).alias(\"message\")\n",
    ")\n",
    "\n",
    "print(\"### DataFrame with Media Links:\")\n",
    "df_with_media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# --- 1. Anonymization Logic (adapted from src/egregora/anonymizer.py) ---\n",
    "\n",
    "NAMESPACE_NICKNAME = uuid.UUID(\"6ba7b811-9dad-11d1-80b4-00c04fd430c9\")\n",
    "FormatType = Literal[\"human\", \"short\", \"full\"]\n",
    "\n",
    "def _format_uuid(uuid_str: str, prefix: str, format: FormatType) -> str:\n",
    "    if format == \"human\":\n",
    "        short = uuid_str.split(\"-\")[0][:4].upper()\n",
    "        return f\"{prefix}-{short}\"\n",
    "    if format == \"short\":\n",
    "        return uuid_str.split(\"-\")[0][:8].lower()\n",
    "    return uuid_str\n",
    "\n",
    "def anonymize_nickname(nickname: str, format: FormatType = \"human\", prefix: str = \"Member\") -> str:\n",
    "    normalized = \" \".join(nickname.strip().lower().split())\n",
    "    uuid_full = str(uuid.uuid5(NAMESPACE_NICKNAME, normalized))\n",
    "    return _format_uuid(uuid_full, prefix, format)\n",
    "\n",
    "_MENTION_PATTERN = re.compile(r\"(?P<prefix>@?)\\\\u2068(?P<label>.*?)\\\\u2069\")\n",
    "\n",
    "def anonymize_mentions(text: str | None, format: FormatType = \"human\") -> str | None:\n",
    "    if text is None: return None\n",
    "    def _replace(match: re.Match[str]) -> str:\n",
    "        prefix = match.group(\"prefix\") or \"@\"\n",
    "        label = match.group(\"label\").strip()\n",
    "        if not label: return prefix\n",
    "        pseudonym = anonymize_nickname(label, format)\n",
    "        return f\"{prefix}{pseudonym}\"\n",
    "    return _MENTION_PATTERN.sub(_replace, text).replace(\"\\u2068\", \"\").replace(\"\\u2069\", \"\")\n",
    "\n",
    "# --- 2. Anonymize DataFrame ---\n",
    "\n",
    "df_anonymized = df_with_media.with_columns(\n",
    "    pl.col(\"author\").map_elements(lambda author: anonymize_nickname(author), return_dtype=pl.String).alias(\"anon_author\"),\n",
    "    pl.col(\"message\").map_elements(anonymize_mentions, return_dtype=pl.String).alias(\"message\")\n",
    ")\n",
    "\n",
    "print(\"### Anonymized DataFrame:\")\n",
    "# Display the final DataFrame, showing the original author and the new anonymized author\n",
    "df_anonymized.select([\"timestamp\", \"author\", \"anon_author\", \"message\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- 1. Enrichment Logic (adapted from src/egregora/enrichment.py) ---\n",
    "\n",
    "URL_RE = re.compile(r\"(https?://[^\s>)]+)\", re.IGNORECASE)\n",
    "\n",
    "def _extract_urls(message: str | None) -> list[str]:\n",
    "    if not message: return []\n",
    "    return URL_RE.findall(message)\n",
    "\n",
    "def _create_mock_summary(urls: list[str]) -> str | None:\n",
    "    if not urls: return None\n",
    "    summaries = []\n",
    "    for url in urls:\n",
    "        domain = urlparse(url).hostname\n",
    "        summaries.append(f\"This is a mock summary for a link from {domain}.\")\n",
    "    return \"\\n\".join(summaries)\n",
    "\n",
    "# --- 2. Enrich DataFrame ---\n",
    "\n",
    "df_final = df_anonymized.with_columns(\n",
    "    pl.col(\"message\").map_elements(_extract_urls, return_dtype=pl.List(pl.String)).alias(\"urls\")\n",
    ").with_columns(\n",
    "    pl.col(\"urls\").map_elements(_create_mock_summary, return_dtype=pl.String).alias(\"enriched_summary\")\n",
    ").drop(\"urls\")\n",
    "\n",
    "print(\"### Final Enriched DataFrame:\")\n",
    "# Display the final DataFrame, now with the enriched_summary column\n",
    "df_final.select([\"timestamp\", \"anon_author\", \"message\", \"enriched_summary\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
