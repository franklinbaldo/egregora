# üìä AN√ÅLISE DE ARQUITETURA - EGREGORA

**Data da An√°lise:** 2026-01-22
**Vers√£o Analisada:** Current HEAD (commit e138d3b)
**Analista:** Claude (Sonnet 4.5)

---

## 1. RESUMO EXECUTIVO

O Egregora √© um sistema sofisticado de transforma√ß√£o de conversas em narrativas conectadas, implementado com uma arquitetura em camadas bem definida:

**Stack Principal:**
- **Dados:** Ibis + DuckDB (OLAP local) + LanceDB (vector store)
- **IA:** Pydantic-AI + Google Gemini (2.5 Flash, com fallback)
- **Config:** Pydantic Settings + TOML
- **Output:** MkDocs Material (site est√°tico)

**Filosofia:** "Invisible Intelligence, Visible Magic" - RAG, ranking e perfis de autores funcionam automaticamente sem configura√ß√£o.

---

## 2. ‚úÖ PONTOS FORTES DA ARQUITETURA

### A. Separa√ß√£o de Responsabilidades Clara

```
CLI ‚Üí Orchestration ‚Üí Agents ‚Üí Database
      ‚Üì               ‚Üì
  Adapters      Transformations
```

- Cada camada tem responsabilidade bem definida
- Baixo acoplamento entre m√≥dulos
- F√°cil navega√ß√£o no c√≥digo

### B. Extensibilidade por Design

- **Adapters Pattern:** Novos formatos de entrada (WhatsApp, Slack, etc.) via protocolo
- **Output Sinks:** Formatos de sa√≠da plug√°veis (MkDocs, Notion, etc.)
- **Skills System:** Extens√µes customizadas via `.egregora/skills/`
- **Model Rotation:** Suporte a m√∫ltiplos LLMs via Pydantic-AI

### C. Resili√™ncia Operacional

- **Journaling:** Idempot√™ncia - reprocessamento seguro de windows
- **Auto-split:** Janelas grandes divididas automaticamente em caso de `PromptTooLargeError`
- **Non-fatal failures:** Enrichment e media n√£o bloqueiam pipeline
- **Task coalescing:** Otimiza updates redundantes de perfil

### D. Performance Consciente

- **Streaming:** Processa ZIPs grandes sem carregar tudo em mem√≥ria
- **LRU Cache:** Embeddings com cache de 16 entradas
- **Lazy RAG:** Vector DB s√≥ inicializa quando necess√°rio
- **Batch processing:** Banner generation em lote

### E. Type Safety & Testing

- MyPy strict mode na maioria dos m√≥dulos
- 216 arquivos de teste (unit, integration, e2e, benchmarks)
- Property-based testing com Hypothesis
- Snapshot testing com Syrupy

---

## 3. ‚ö†Ô∏è √ÅREAS DE PREOCUPA√á√ÉO

### A. Complexidade do Orchestration Layer

**Problema:**
- `write.py` tem **1400+ linhas** - viola princ√≠pio de Single Responsibility
- `runner.py` com 578 linhas - dificulta manuten√ß√£o
- L√≥gica de window splitting misturada com processamento

**Impacto:**
- Curva de aprendizado alta para novos contribuidores
- Dificulta testes unit√°rios isolados
- Mudan√ßas arriscadas (efeitos colaterais n√£o √≥bvios)

**Evid√™ncia:**
```python
# write.py tem m√∫ltiplas responsabilidades:
- ETL setup
- Window iteration
- Agent execution
- Media processing
- Background task scheduling
- Error handling
```

**Localiza√ß√£o:** `src/egregora/orchestration/pipelines/write.py:1-1400`

### B. Gest√£o de Estado Fragmentada

**Problema:**
- Estado distribu√≠do entre: Journal, TaskStore, EloStore, ContentRepository
- Nenhuma vis√£o unificada do "estado do pipeline"
- Dif√≠cil rastrear progresso de execu√ß√£o

**Impacto:**
- Debugging complexo em caso de falhas parciais
- Imposs√≠vel "replay" de pipeline com estado consistente
- Checkpoints fragmentados

**Arquivos Afetados:**
- `src/egregora/orchestration/journal.py`
- `src/egregora/database/task_store.py`
- `src/egregora/database/elo_store.py`
- `src/egregora/database/repository.py`

### C. Configura√ß√£o com Defaults Impl√≠citos

**Problema:**
- Muitos defaults espalhados pelo c√≥digo:
  - `DEFAULT_MODEL` em `config/settings.py`
  - `DEFAULT_EMBEDDING_MODEL` hardcoded
  - Magic numbers (0.8 / 5 para window splitting)
  - Rate limit = 2 req/s hardcoded

**Impacto:**
- Dif√≠cil entender comportamento real sem ler c√≥digo
- Mudan√ßas de defaults quebram sites existentes
- Testes dependem de valores m√°gicos

**Exemplo:**
```python
# src/egregora/orchestration/pipelines/write.py
if window_size > (max_tokens * 0.8 / 5):  # ??? Por que 0.8/5?
    split_proactively()
```

### D. Error Handling Inconsistente

**Problema:**
- Alguns erros s√£o fatais, outros n√£o (sem crit√©rio claro)
- Journal failures silenciadas mas importantes para idempot√™ncia
- Enrichment errors n√£o propagados ao usu√°rio

**Impacto:**
- Comportamento imprevis√≠vel em falhas
- Logs importantes perdidos
- Usu√°rio n√£o sabe quando features falharam

**Exemplo:**
```python
# Diferentes estrat√©gias de erro sem padr√£o claro:
try:
    journal.persist()
except Exception:
    logger.warning("Journal failed")  # Silent fail

try:
    enrich_media()
except Exception:
    logger.error("Enrichment failed")  # Logged but continues

try:
    writer_agent.run()
except Exception:
    raise  # Fatal
```

### E. Testing Gaps

**Problema:**
- Coverage atual: **39%** (baixo para projeto cr√≠tico)
- Faltam testes de integra√ß√£o para RAG + Writer
- End-to-end tests com mocks - n√£o validam LLM real
- Benchmarks n√£o executam em CI

**Impacto:**
- Regress√µes podem passar despercebidas
- Mudan√ßas arriscadas sem rede de seguran√ßa
- Performance pode degradar sem detec√ß√£o

**√Åreas Cr√≠ticas Sem Cobertura:**
- `orchestration/runner.py` - Window processing loop
- `agents/writer.py` - RAG integration
- `rag/lancedb_backend.py` - Vector search

### F. Acoplamento ao Google Gemini

**Problema:**
- Todo pipeline depende de `GOOGLE_API_KEY`
- Pydantic-AI suporta outros providers, mas config n√£o
- Fallback s√≥ entre modelos Gemini (n√£o cross-provider)

**Impacto:**
- Vendor lock-in
- Falhas da Google API param todo pipeline
- Usu√°rios sem Gemini n√£o podem usar

**C√≥digo Afetado:**
```python
# src/egregora/config/settings.py
DEFAULT_MODEL = "google-gla:gemini-2.5-flash"  # Hardcoded Google

# src/egregora/llm/providers/model_cycler.py
# S√≥ rotaciona entre modelos Gemini
```

### G. Documenta√ß√£o de C√≥digo Limitada

**Problema:**
- CLAUDE.md excelente, mas c√≥digo tem poucos docstrings
- Fun√ß√µes complexas sem explica√ß√£o (ex: window splitting heuristic)
- Falta de ADRs (Architecture Decision Records) para decis√µes cr√≠ticas

**Impacto:**
- Onboarding dif√≠cil
- Decis√µes arquiteturais podem ser revertidas por desconhecimento
- AI agents (Jules) podem fazer mudan√ßas incompat√≠veis

---

## 4. üéØ RECOMENDA√á√ïES PRIORIZADAS

### CR√çTICAS (Fazer Agora)

#### 1. Refatorar `orchestration/pipelines/write.py`

**Objetivo:** Dividir 1400 linhas em m√≥dulos coesos

**Plano:**
```python
# Proposta de estrutura:
orchestration/
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ write.py (reduzir para ~200 linhas - entry point)
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup.py           # _prepare_pipeline_data()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversation.py    # get_pending_conversations()
‚îÇ   ‚îú‚îÄ‚îÄ execution/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor.py       # process_item()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ window_handler.py  # window splitting logic
‚îÇ   ‚îî‚îÄ‚îÄ coordination/
‚îÇ       ‚îú‚îÄ‚îÄ background_tasks.py
‚îÇ       ‚îî‚îÄ‚îÄ checkpointing.py
```

**Benef√≠cios:**
- Testes unit√°rios isolados
- Responsabilidades claras
- F√°cil entender fluxo

**Arquivos a Criar:**
- `src/egregora/orchestration/pipelines/etl/setup.py`
- `src/egregora/orchestration/pipelines/etl/conversation.py`
- `src/egregora/orchestration/pipelines/execution/processor.py`
- `src/egregora/orchestration/pipelines/execution/window_handler.py`
- `src/egregora/orchestration/pipelines/coordination/background_tasks.py`
- `src/egregora/orchestration/pipelines/coordination/checkpointing.py`

**Esfor√ßo Estimado:** 3-5 dias

---

#### 2. Centralizar Configura√ß√£o de Defaults

**Criar:** `src/egregora/config/defaults.py`

```python
# defaults.py
from dataclasses import dataclass

@dataclass(frozen=True)
class PipelineDefaults:
    """Pipeline processing defaults."""

    MAX_PROMPT_TOKENS: int = 400_000
    """Maximum tokens per prompt before auto-splitting."""

    PROACTIVE_SPLIT_THRESHOLD: float = 0.8
    """Threshold for proactive splitting (80% of max)."""

    PROACTIVE_SPLIT_DIVISOR: int = 5
    """Divisor for proactive split calculation."""

    STEP_SIZE: int = 100
    """Default window step size."""

    STEP_UNIT: str = "messages"
    """Default window step unit (messages, hours, bytes)."""

    OVERLAP_RATIO: float = 0.2
    """Overlap ratio between consecutive windows."""

@dataclass(frozen=True)
class ModelDefaults:
    """AI model defaults."""

    WRITER: str = "google-gla:gemini-2.5-flash"
    """Default model for Writer agent."""

    READER: str = "google-gla:gemini-2.5-flash"
    """Default model for Reader agent."""

    ENRICHER: str = "google-gla:gemini-2.5-flash"
    """Default model for Enrichment agent."""

    EMBEDDING: str = "models/gemini-embedding-001"
    """Default embedding model for RAG."""

@dataclass(frozen=True)
class RateLimitDefaults:
    """Rate limiting defaults."""

    REQUESTS_PER_SECOND: int = 2
    """Maximum requests per second to LLM APIs."""

    BURST_SIZE: int = 5
    """Maximum burst size for rate limiting."""
```

**Benef√≠cios:**
- Descoberta f√°cil de configura√ß√µes
- Documenta√ß√£o inline
- Testes n√£o dependem de magic numbers
- F√°cil override por ambiente

**Arquivos a Modificar:**
- `src/egregora/config/settings.py` - Importar de defaults
- `src/egregora/orchestration/pipelines/write.py` - Usar defaults
- `src/egregora/llm/rate_limit.py` - Usar defaults

**Esfor√ßo Estimado:** 1-2 dias

---

#### 3. Implementar Error Boundary Pattern

**Criar:** `src/egregora/orchestration/error_boundary.py`

```python
from enum import Enum
from typing import Protocol, Callable
from egregora.exceptions import EgregoraError

class FailureStrategy(Enum):
    """Strategy for handling different types of failures."""

    FATAL = "fatal"
    """Stop pipeline immediately and raise exception."""

    WARN = "warn"
    """Continue pipeline, log warning, notify user."""

    SILENT = "silent"
    """Continue pipeline, log at debug level."""

    RETRY = "retry"
    """Retry operation with exponential backoff."""

class ErrorBoundary(Protocol):
    """
    Define error handling policies for different operations.

    This centralizes error handling logic and makes behavior predictable.
    Each operation type has a clear failure strategy.
    """

    def handle_journal_error(self, e: Exception) -> None:
        """
        Handle journal persistence errors.

        Strategy: FATAL
        Reason: Breaks idempotency guarantees.
        """

    def handle_enrichment_error(self, e: Exception) -> None:
        """
        Handle media enrichment errors.

        Strategy: WARN
        Reason: Non-critical feature, user should know.
        """

    def handle_rag_error(self, e: Exception) -> None:
        """
        Handle RAG/vector search errors.

        Strategy: WARN + FALLBACK
        Reason: Degrades to no-context mode gracefully.
        """

    def handle_writer_error(self, e: Exception) -> None:
        """
        Handle Writer agent errors.

        Strategy: RETRY then FATAL
        Reason: Core feature, but may be transient API error.
        """

class DefaultErrorBoundary:
    """Default implementation of error boundary."""

    def __init__(self, logger, user_notifier):
        self.logger = logger
        self.notifier = user_notifier

    def handle_journal_error(self, e: Exception) -> None:
        self.logger.critical(f"Journal error: {e}")
        raise ConfigurationError("Cannot proceed without journal") from e

    def handle_enrichment_error(self, e: Exception) -> None:
        self.logger.warning(f"Enrichment failed: {e}")
        self.notifier.warn("Media enrichment unavailable for this batch")
        # Continue processing

    def handle_rag_error(self, e: Exception) -> None:
        self.logger.warning(f"RAG error: {e}")
        self.notifier.warn("Contextual memory unavailable, falling back to no-context mode")
        # Return empty context

    def handle_writer_error(self, e: Exception) -> None:
        # Retry logic handled by retry decorator
        self.logger.error(f"Writer error: {e}")
        raise  # Fatal
```

**Benef√≠cios:**
- Comportamento previs√≠vel
- F√°cil raciocinar sobre falhas
- Usu√°rio sabe o que falhou
- Centralized error policy

**Arquivos a Modificar:**
- `src/egregora/orchestration/runner.py` - Usar error boundary
- `src/egregora/orchestration/pipelines/write.py` - Usar error boundary
- `src/egregora/agents/writer.py` - Usar error boundary

**Esfor√ßo Estimado:** 2-3 dias

---

### IMPORTANTES (Pr√≥ximas Sprints)

#### 4. Aumentar Cobertura de Testes para 60%+

**Focos:**
- `orchestration/runner.py` - testes de window processing
- `agents/writer.py` - testes de RAG integration
- `rag/lancedb_backend.py` - testes de vector search
- `database/repository.py` - testes de persistence

**Estrat√©gia:**
```bash
# Identificar m√≥dulos cr√≠ticos sem cobertura:
uv run pytest --cov=egregora --cov-report=term-missing | grep "0%"

# Priorizar:
1. L√≥gica de neg√≥cio (agents, orchestration)
2. Persist√™ncia (database, repository)
3. Transforma√ß√µes (windowing, media)
```

**Testes a Criar:**

1. **`tests/unit/orchestration/test_runner_coverage.py`**
   - `test_window_processing_success()`
   - `test_window_auto_split_on_prompt_too_large()`
   - `test_journal_deduplication()`
   - `test_checkpoint_persistence()`

2. **`tests/integration/test_rag_writer_integration.py`**
   - `test_writer_uses_rag_context()`
   - `test_writer_fallback_no_rag()`
   - `test_rag_search_quality()`

3. **`tests/unit/database/test_repository_persistence.py`**
   - `test_content_repository_routes_document_types()`
   - `test_persistence_idempotency()`

**Esfor√ßo Estimado:** 5-7 dias

---

#### 5. Implementar Pipeline State Machine

**Objetivo:** Unificar gest√£o de estado do pipeline

**Criar:** `src/egregora/orchestration/state.py`

```python
from enum import Enum
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional

class PipelinePhase(Enum):
    """Pipeline execution phases."""

    INITIALIZING = "initializing"
    PARSING = "parsing"
    WINDOWING = "windowing"
    PROCESSING = "processing"
    PERSISTING = "persisting"
    FINALIZING = "finalizing"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class PipelineState:
    """
    Unified pipeline state tracking.

    Provides single source of truth for pipeline progress,
    enabling recovery, monitoring, and debugging.
    """

    phase: PipelinePhase
    windows_total: int
    windows_processed: int
    posts_created: int
    profiles_created: int
    media_processed: int
    errors: list[Exception] = field(default_factory=list)
    started_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

    def checkpoint(self) -> dict:
        """Serialize state for recovery."""
        return {
            "phase": self.phase.value,
            "windows_total": self.windows_total,
            "windows_processed": self.windows_processed,
            "posts_created": self.posts_created,
            "profiles_created": self.profiles_created,
            "media_processed": self.media_processed,
            "errors": [str(e) for e in self.errors],
            "started_at": self.started_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
        }

    @classmethod
    def resume(cls, checkpoint: dict) -> "PipelineState":
        """Restore from checkpoint."""
        return cls(
            phase=PipelinePhase(checkpoint["phase"]),
            windows_total=checkpoint["windows_total"],
            windows_processed=checkpoint["windows_processed"],
            posts_created=checkpoint["posts_created"],
            profiles_created=checkpoint["profiles_created"],
            media_processed=checkpoint["media_processed"],
            errors=[Exception(e) for e in checkpoint["errors"]],
            started_at=datetime.fromisoformat(checkpoint["started_at"]),
            updated_at=datetime.fromisoformat(checkpoint["updated_at"]),
        )

    def progress_percentage(self) -> float:
        """Calculate progress as percentage."""
        if self.windows_total == 0:
            return 0.0
        return (self.windows_processed / self.windows_total) * 100

    def estimated_time_remaining(self) -> Optional[float]:
        """Estimate remaining time in seconds."""
        if self.windows_processed == 0:
            return None

        elapsed = (self.updated_at - self.started_at).total_seconds()
        avg_time_per_window = elapsed / self.windows_processed
        remaining_windows = self.windows_total - self.windows_processed

        return avg_time_per_window * remaining_windows
```

**Benef√≠cios:**
- Visibilidade de progresso
- Recovery de falhas
- Debugging facilitado
- Estimativa de tempo
- Metrics para monitoring

**Integra√ß√£o:**
```python
# src/egregora/orchestration/runner.py
def run_pipeline(config):
    state = PipelineState(
        phase=PipelinePhase.INITIALIZING,
        windows_total=0,
        windows_processed=0,
        posts_created=0,
    )

    try:
        state.phase = PipelinePhase.PARSING
        messages = parse_input(config.input_path)

        state.phase = PipelinePhase.WINDOWING
        windows = create_windows(messages)
        state.windows_total = len(windows)

        state.phase = PipelinePhase.PROCESSING
        for window in windows:
            posts = process_window(window)
            state.posts_created += len(posts)
            state.windows_processed += 1
            state.updated_at = datetime.now()

            # Persist checkpoint
            save_checkpoint(state.checkpoint())

        state.phase = PipelinePhase.COMPLETED
    except Exception as e:
        state.phase = PipelinePhase.FAILED
        state.errors.append(e)
        raise
```

**Esfor√ßo Estimado:** 3-4 dias

---

#### 6. Adicionar Multi-Provider Support

**Objetivo:** Reduzir vendor lock-in

**Configura√ß√£o:**
```toml
# .egregora.toml
[models.providers]
primary = "google-gla"
fallback = ["openai", "anthropic"]

[models.google-gla]
api_key_env = "GOOGLE_API_KEY"
default_model = "gemini-2.5-flash"

[models.openai]
api_key_env = "OPENAI_API_KEY"
default_model = "gpt-4o-mini"

[models.anthropic]
api_key_env = "ANTHROPIC_API_KEY"
default_model = "claude-sonnet-4-5"
```

**Implementa√ß√£o:**

**Criar:** `src/egregora/llm/provider_router.py`

```python
from typing import Protocol, Optional
from pydantic_ai import Agent
from egregora.config.settings import EgregoraConfig

class ProviderRouter:
    """
    Routes to next available provider on failures.

    Enables multi-provider fallback for resilience.
    """

    def __init__(self, config: EgregoraConfig):
        self.config = config
        self.current_provider_index = 0

    def get_next_model(self, current_error: Exception) -> Optional[str]:
        """
        Route to next provider in fallback chain.

        Args:
            current_error: Error from current provider

        Returns:
            Next model string or None if exhausted
        """
        from egregora.llm.exceptions import GoogleAPIError

        if isinstance(current_error, GoogleAPIError):
            # Move to OpenAI
            if "openai" in self.config.models.providers.fallback:
                return self.config.models.openai.default_model

        # Move to next in chain
        self.current_provider_index += 1

        if self.current_provider_index >= len(self.config.models.providers.fallback):
            return None  # Exhausted

        next_provider = self.config.models.providers.fallback[self.current_provider_index]
        return getattr(self.config.models, next_provider).default_model
```

**Modificar:** `src/egregora/agents/writer.py`

```python
# Add provider rotation logic
def create_writer_agent(config: EgregoraConfig) -> Agent:
    router = ProviderRouter(config)

    try:
        return Agent(model=config.models.writer, ...)
    except Exception as e:
        next_model = router.get_next_model(e)
        if next_model:
            return Agent(model=next_model, ...)
        raise
```

**Benef√≠cios:**
- Reduz vendor lock-in
- Maior resili√™ncia
- Flexibilidade de custos
- Fallback cross-provider

**Esfor√ßo Estimado:** 4-5 dias

---

### DESEJ√ÅVEIS (Backlog)

#### 7. Performance Monitoring & Observability

**Adicionar:**
- M√©tricas: tempo por window, tokens consumidos, custo estimado
- Tracing: OpenTelemetry para rastrear fluxo
- Profiling: cProfile para identificar gargalos

**Implementa√ß√£o:**

**Criar:** `src/egregora/observability/metrics.py`

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

@dataclass
class WindowMetrics:
    """Metrics for window processing."""

    window_id: str
    start_time: datetime
    end_time: Optional[datetime]
    messages_count: int
    tokens_estimated: int
    tokens_actual: int
    api_calls: int
    api_cost_usd: float
    posts_created: int

    def duration_seconds(self) -> Optional[float]:
        if not self.end_time:
            return None
        return (self.end_time - self.start_time).total_seconds()

class MetricsCollector:
    """Collect and export pipeline metrics."""

    def __init__(self):
        self.window_metrics: list[WindowMetrics] = []

    def record_window(self, metrics: WindowMetrics):
        self.window_metrics.append(metrics)

    def export_summary(self) -> dict:
        total_cost = sum(m.api_cost_usd for m in self.window_metrics)
        total_tokens = sum(m.tokens_actual for m in self.window_metrics)
        avg_duration = sum(m.duration_seconds() or 0 for m in self.window_metrics) / len(self.window_metrics)

        return {
            "total_windows": len(self.window_metrics),
            "total_cost_usd": total_cost,
            "total_tokens": total_tokens,
            "avg_duration_seconds": avg_duration,
        }
```

**OpenTelemetry Integration:**

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@tracer.start_as_current_span("process_window")
def process_window(window: Window) -> list[str]:
    span = trace.get_current_span()
    span.set_attribute("window.size", len(window.messages))
    span.set_attribute("window.id", window.id)

    # Process...

    span.set_attribute("posts.created", len(posts))
    return posts
```

**Esfor√ßo Estimado:** 5-7 dias

---

#### 8. Implementar Dry-Run Mode

**Objetivo:** Validar pipeline sem executar LLM

**CLI:**
```bash
egregora write input.zip --dry-run
```

**Output:**
```
üîç DRY RUN MODE - No LLM calls will be made

‚úì Input file parsed successfully
  - Format: WhatsApp ZIP export
  - Messages: 12,453
  - Date range: 2023-01-15 to 2024-12-20
  - Authors: 5 (Alice, Bob, Charlie, Diana, Eve)

‚úì Windowing configuration validated
  - Step size: 100 messages
  - Overlap: 20%
  - Estimated windows: 15

‚ö† Window #7 will be auto-split
  - Reason: Estimated 450,000 tokens (exceeds 400,000 limit)
  - Sub-windows: 2

‚úì Commands detected: 3
  - /egregora profile Alice
  - /avatar https://example.com/alice.jpg
  - /egregora tag important

‚úì Media references: 42
  - Images: 35
  - Videos: 5
  - Audio: 2

üí∞ Cost Estimation:
  - RAG indexing: $2.50 (embeddings)
  - Writer agent: $15.00 (text generation)
  - Banner generation: $3.00 (image generation)
  - Total estimated: $20.50

‚è±Ô∏è Time Estimation:
  - Based on rate limit: 2 req/s
  - Estimated duration: 8.5 minutes

‚úÖ Dry run completed. Pipeline is ready to execute.
```

**Implementa√ß√£o:**

**Modificar:** `src/egregora/cli/write.py`

```python
@app.command()
def write(
    input_path: Path,
    dry_run: bool = typer.Option(False, "--dry-run", help="Validate without executing"),
):
    if dry_run:
        run_dry_run(input_path)
    else:
        run_pipeline(input_path)
```

**Criar:** `src/egregora/orchestration/dry_run.py`

```python
def run_dry_run(input_path: Path) -> DryRunReport:
    """Execute dry run validation."""

    # Parse input
    messages = parse_input(input_path)

    # Create windows
    windows = create_windows(messages)

    # Estimate tokens
    token_estimates = [estimate_tokens(w) for w in windows]

    # Detect auto-splits
    splits = [w for w in windows if needs_split(w)]

    # Extract commands
    commands = extract_commands(messages)

    # Count media
    media = extract_media_refs(messages)

    # Estimate cost
    cost = estimate_cost(windows, media)

    return DryRunReport(...)
```

**Esfor√ßo Estimado:** 3-4 dias

---

#### 9. Adicionar Architecture Decision Records (ADRs)

**Objetivo:** Documentar decis√µes arquiteturais cr√≠ticas

**Criar:** `architecture/decisions/`

```
architecture/
‚îú‚îÄ‚îÄ decisions/
‚îÇ   ‚îú‚îÄ‚îÄ 0001-use-ibis-instead-of-pandas.md
‚îÇ   ‚îú‚îÄ‚îÄ 0002-duckdb-for-local-analytics.md
‚îÇ   ‚îú‚îÄ‚îÄ 0003-pydantic-ai-for-structured-outputs.md
‚îÇ   ‚îú‚îÄ‚îÄ 0004-lancedb-for-vector-search.md
‚îÇ   ‚îú‚îÄ‚îÄ 0005-toml-over-yaml-config.md
‚îÇ   ‚îî‚îÄ‚îÄ template.md
‚îî‚îÄ‚îÄ README.md
```

**Template:** `architecture/decisions/template.md`

```markdown
# ADR-XXXX: [Title]

## Status

[Proposed | Accepted | Deprecated | Superseded]

## Context

[What is the issue that we're seeing that is motivating this decision or change?]

## Decision

[What is the change that we're proposing and/or doing?]

## Consequences

### Positive

- [Benefit 1]
- [Benefit 2]

### Negative

- [Drawback 1]
- [Drawback 2]

### Neutral

- [Trade-off 1]

## Alternatives Considered

### Alternative 1: [Name]

[Description and why rejected]

### Alternative 2: [Name]

[Description and why rejected]

## References

- [Link to discussion]
- [Link to implementation PR]
```

**Exemplo:** `architecture/decisions/0001-use-ibis-instead-of-pandas.md`

```markdown
# ADR-0001: Use Ibis Instead of Pandas

## Status

Accepted (2025-01-15)

## Context

Egregora needs to process chat message tables with 100k+ rows. Initial implementation used Pandas, but we faced:

1. **Backend lock-in**: Hard to migrate from DuckDB to PostgreSQL
2. **Memory issues**: Large DataFrames consume significant RAM
3. **Performance**: Pandas not optimized for analytics workloads

## Decision

Use Ibis as abstraction layer over DuckDB (and potentially other backends).

## Consequences

### Positive

- ‚úÖ **Backend-agnostic**: Can switch from DuckDB to PostgreSQL/BigQuery without code changes
- ‚úÖ **Lazy evaluation**: Queries optimized before execution
- ‚úÖ **Better performance**: DuckDB optimizations for OLAP
- ‚úÖ **Type safety**: Schema-aware operations

### Negative

- ‚ùå **Learning curve**: Ibis less common than Pandas
- ‚ùå **Smaller ecosystem**: Fewer integrations than Pandas
- ‚ùå **API differences**: Some operations require different syntax

### Neutral

- Requires explicit `.to_pandas()` when Pandas needed
- Need to maintain Ibis knowledge in team

## Alternatives Considered

### Alternative 1: Pure Pandas

**Pros:**
- Well-known API
- Large ecosystem
- Easy to hire for

**Cons:**
- Backend lock-in
- Memory inefficient
- Slower for large datasets

**Rejected because:** Backend lock-in and performance issues.

### Alternative 2: Pure SQL

**Pros:**
- Maximum control
- Optimal performance
- Portable

**Cons:**
- String concatenation for queries
- No type safety
- Hard to compose

**Rejected because:** Lack of type safety and composability.

## References

- Ibis documentation: https://ibis-project.org/
- DuckDB integration: https://ibis-project.org/backends/duckdb/
- Implementation PR: #123
```

**Esfor√ßo Estimado:** 2-3 days (initial ADRs)

---

#### 10. Documentar Padr√µes de C√≥digo

**Criar:** `architecture/patterns.md`

```markdown
# Padr√µes de C√≥digo Egregora

Este documento descreve os padr√µes de c√≥digo estabelecidos no projeto Egregora.
Seguir esses padr√µes garante consist√™ncia e facilita manuten√ß√£o.

---

## 1. Transforma√ß√µes Funcionais

**Sempre use fun√ß√µes puras para transforma√ß√µes de dados:**

```python
# ‚úÖ BOM - Fun√ß√£o pura
def filter_recent_messages(table: ibis.Table, days: int) -> ibis.Table:
    """Filter messages from last N days."""
    cutoff = datetime.now() - timedelta(days=days)
    return table.filter(ibis._['ts'] >= cutoff)

# ‚ùå RUIM - Efeitos colaterais
def filter_recent_messages(table: ibis.Table, days: int) -> ibis.Table:
    """Filter messages from last N days."""
    cutoff = datetime.now() - timedelta(days=days)
    filtered = table.filter(ibis._['ts'] >= cutoff)

    # Side effect: writes to database
    save_to_cache(filtered)  # ‚ùå

    return filtered
```

**Regras:**
- Sem side effects (I/O, estado global, logging excessivo)
- Sem muta√ß√£o de argumentos
- Resultado determin√≠stico para mesmos inputs
- Composabilidade: `f(g(x))` deve funcionar

---

## 2. Error Handling

**Use exce√ß√µes tipadas da hierarquia `EgregoraError`:**

```python
# ‚úÖ BOM - Exce√ß√£o tipada
from egregora.exceptions import ConfigurationError

def load_config(path: Path) -> EgregoraConfig:
    if not path.exists():
        raise ConfigurationError(f"Config file not found: {path}")

    try:
        return parse_toml(path)
    except TOMLDecodeError as e:
        raise ConfigurationError(f"Invalid TOML: {e}") from e

# ‚ùå RUIM - Exce√ß√£o gen√©rica
def load_config(path: Path) -> EgregoraConfig:
    if not path.exists():
        raise Exception("File not found")  # ‚ùå N√£o tipada
```

**Hierarquia:**
```
Exception
‚îî‚îÄ‚îÄ EgregoraError
    ‚îú‚îÄ‚îÄ ConfigurationError
    ‚îú‚îÄ‚îÄ AgentError
    ‚îú‚îÄ‚îÄ DatabaseError
    ‚îî‚îÄ‚îÄ ...
```

**Regras:**
- Sempre derive de `EgregoraError`
- Use `from e` para chain de exce√ß√µes
- Mensagens de erro devem ser actionable

---

## 3. Repository Pattern

**Abstraia acesso a dados com repositories:**

```python
# ‚úÖ BOM - Repository com protocolo
from typing import Protocol

class MessageRepository(Protocol):
    def get_messages(self, filters: dict) -> ibis.Table: ...
    def count(self) -> int: ...

class DuckDBMessageRepository:
    def __init__(self, conn: DuckDBConnection):
        self.conn = conn

    def get_messages(self, filters: dict) -> ibis.Table:
        table = self.conn.table("messages")
        # Apply filters...
        return table

# Usage (dependency injection)
def process_pipeline(repo: MessageRepository):
    messages = repo.get_messages({"from_date": "2024-01-01"})
    # Process...

# ‚ùå RUIM - Acesso direto ao banco
def process_pipeline(conn):
    messages = conn.execute("SELECT * FROM messages WHERE ...")  # ‚ùå
```

**Regras:**
- Use protocols para interfaces
- Repositories retornam Ibis Tables, n√£o DataFrames
- Dependency injection para testabilidade

---

## 4. Adapter Pattern

**Use adapters para diferentes formatos de entrada/sa√≠da:**

```python
# ‚úÖ BOM - Adapter com protocolo
from typing import Protocol
from pathlib import Path
import ibis

class InputAdapter(Protocol):
    def parse(self, input_path: Path) -> ibis.Table: ...
    def get_metadata(self, input_path: Path) -> dict: ...

class WhatsAppAdapter:
    def parse(self, input_path: Path) -> ibis.Table:
        # Parse WhatsApp ZIP
        return messages_table

    def get_metadata(self, input_path: Path) -> dict:
        return {"format": "whatsapp", "version": "2.0"}

# Registry pattern
ADAPTERS = {
    "whatsapp": WhatsAppAdapter,
    "telegram": TelegramAdapter,
}

# ‚ùå RUIM - Hard-coded format
def parse_input(path: Path) -> ibis.Table:
    if path.suffix == ".zip":
        return parse_whatsapp(path)  # ‚ùå Hard-coded
    elif path.suffix == ".json":
        return parse_telegram(path)  # ‚ùå Hard-coded
```

**Regras:**
- Protocols definem interface
- Registry para descoberta de adapters
- Retornar formato can√¥nico (Ibis Table com schema padronizado)

---

## 5. Configuration Management

**Use Pydantic Settings para configura√ß√£o:**

```python
# ‚úÖ BOM - Pydantic Settings
from pydantic_settings import BaseSettings
from pydantic import Field

class PipelineConfig(BaseSettings):
    step_size: int = Field(default=100, ge=1, le=10000)
    step_unit: str = Field(default="messages", pattern="^(messages|hours|bytes)$")

    model_config = {
        "env_prefix": "EGREGORA_",
        "env_file": ".env",
    }

# Validation autom√°tica
config = PipelineConfig(step_size=50)  # ‚úÖ

# ‚ùå RUIM - Dict com valida√ß√£o manual
config = {
    "step_size": 50,
    "step_unit": "messages",
}

if config["step_size"] < 1:  # ‚ùå Valida√ß√£o manual
    raise ValueError("Invalid step_size")
```

**Regras:**
- Use Pydantic para valida√ß√£o
- Environment variables com prefix
- Defaults expl√≠citos
- Validators para regras complexas

---

## 6. Testing Patterns

### Unit Tests

```python
# ‚úÖ BOM - Unit test isolado
def test_filter_recent_messages():
    # Arrange
    table = create_test_table([
        {"ts": "2024-01-01", "content": "old"},
        {"ts": "2024-12-01", "content": "recent"},
    ])

    # Act
    result = filter_recent_messages(table, days=30)

    # Assert
    assert result.count().execute() == 1
    assert result.execute()["content"][0] == "recent"

# ‚ùå RUIM - Depende de estado externo
def test_filter_recent_messages():
    # ‚ùå L√™ de arquivo externo
    table = load_from_file("test_data.csv")

    result = filter_recent_messages(table, days=30)

    # ‚ùå Assert vago
    assert result.count().execute() > 0
```

### Integration Tests

```python
# ‚úÖ BOM - Integration test com fixture
@pytest.fixture
def temp_db():
    db_path = ":memory:"
    conn = ibis.duckdb.connect(db_path)
    yield conn
    conn.disconnect()

def test_repository_integration(temp_db):
    # Setup
    repo = DuckDBMessageRepository(temp_db)

    # Act
    messages = repo.get_messages({})

    # Assert
    assert isinstance(messages, ibis.Table)
```

---

## 7. Type Annotations

**Sempre use type hints:**

```python
# ‚úÖ BOM - Type hints completos
def process_window(
    window: Window,
    config: EgregoraConfig,
    repo: ContentRepository,
) -> list[str]:
    """Process window and return post IDs."""
    posts: list[str] = []
    # ...
    return posts

# ‚ùå RUIM - Sem types
def process_window(window, config, repo):  # ‚ùå
    posts = []
    return posts
```

**Regras:**
- Type hints em todos os par√¢metros
- Type hints em retornos
- Use `typing.Protocol` para interfaces
- MyPy strict mode

---

## 8. Logging

**Use structured logging:**

```python
# ‚úÖ BOM - Structured logging
import structlog

logger = structlog.get_logger()

def process_window(window: Window):
    logger.info(
        "processing_window",
        window_id=window.id,
        message_count=len(window.messages),
        start_time=window.start_time,
    )

# ‚ùå RUIM - String formatting
import logging

logger = logging.getLogger(__name__)

def process_window(window: Window):
    logger.info(f"Processing window {window.id} with {len(window.messages)} messages")  # ‚ùå
```

**Regras:**
- Structured logging (key-value pairs)
- Levels apropriados (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- N√£o logar informa√ß√µes sens√≠veis

---

## 9. Async/Await

**Use async para I/O-bound operations:**

```python
# ‚úÖ BOM - Async para I/O
async def enrich_media_batch(media_refs: list[str]) -> list[Document]:
    tasks = [enrich_media(ref) for ref in media_refs]
    return await asyncio.gather(*tasks)

# ‚ùå RUIM - Sync para I/O-bound
def enrich_media_batch(media_refs: list[str]) -> list[Document]:
    results = []
    for ref in media_refs:
        results.append(enrich_media(ref))  # ‚ùå Blocking
    return results
```

**Regras:**
- Async para API calls, file I/O
- Sync para CPU-bound (transforma√ß√µes)
- Use `asyncio.gather` para paraleliza√ß√£o

---

## 10. Docstrings

**Use Google-style docstrings:**

```python
# ‚úÖ BOM - Google-style docstring
def create_windows(
    table: ibis.Table,
    step_size: int,
    step_unit: str,
) -> list[Window]:
    """
    Create windows from message table.

    Divides messages into overlapping windows for processing.

    Args:
        table: Message table (must have 'ts' column)
        step_size: Size of each window
        step_unit: Unit for step_size ('messages', 'hours', 'bytes')

    Returns:
        List of windows with overlapping boundaries

    Raises:
        ValueError: If step_unit is invalid

    Example:
        >>> table = create_test_table()
        >>> windows = create_windows(table, step_size=100, step_unit="messages")
        >>> len(windows)
        15
    """

# ‚ùå RUIM - Sem docstring ou docstring vaga
def create_windows(table, step_size, step_unit):
    """Create windows."""  # ‚ùå Muito vago
```

**Regras:**
- Google-style format
- Descrever Args, Returns, Raises
- Incluir Examples para fun√ß√µes complexas
- Obrigat√≥rio para APIs p√∫blicas

---

Seguir esses padr√µes garante c√≥digo consistente, test√°vel e manuten√≠vel.
```

**Esfor√ßo Estimado:** 2-3 days

---

## 5. üìã PLANO DE A√á√ÉO SUGERIDO

### Sprint 1 (Semana 1-2): Funda√ß√µes

**Objetivo:** Reduzir complexidade e centralizar configura√ß√£o

- [ ] **Rec #1:** Refatorar `write.py` em m√≥dulos menores
  - Criar estrutura `etl/`, `execution/`, `coordination/`
  - Migrar fun√ß√µes para m√≥dulos apropriados
  - Atualizar imports
  - Executar testes

- [ ] **Rec #2:** Centralizar defaults em `config/defaults.py`
  - Criar `defaults.py` com dataclasses
  - Atualizar `settings.py` para importar defaults
  - Atualizar `write.py` e outros para usar defaults
  - Remover magic numbers

- [ ] **Rec #3:** Implementar Error Boundary pattern
  - Criar `orchestration/error_boundary.py`
  - Definir failure strategies
  - Integrar em `runner.py` e `write.py`
  - Adicionar testes

**Deliverables:**
- `write.py` reduzido para ~200 linhas
- `config/defaults.py` com todos defaults centralizados
- `orchestration/error_boundary.py` funcionando

---

### Sprint 2 (Semana 3-4): Qualidade

**Objetivo:** Aumentar cobertura de testes e visibilidade

- [ ] **Rec #4:** Aumentar coverage para 60%+ (focar em critical paths)
  - Identificar m√≥dulos sem cobertura
  - Criar testes para `runner.py`
  - Criar testes para integra√ß√£o RAG + Writer
  - Criar testes para `repository.py`

- [ ] **Rec #5:** Implementar Pipeline State Machine
  - Criar `orchestration/state.py`
  - Definir `PipelinePhase` enum
  - Integrar em `runner.py`
  - Adicionar checkpoint persistence

- [ ] Adicionar ADR-001 (Por que Ibis?)
  - Documentar decis√£o
  - Explicar trade-offs
  - Referenciar c√≥digo

**Deliverables:**
- Coverage > 60%
- `orchestration/state.py` funcionando
- ADR-001 documentado

---

### Sprint 3 (Semana 5-6): Extensibilidade

**Objetivo:** Reduzir vendor lock-in e melhorar UX

- [ ] **Rec #6:** Multi-provider support (OpenAI fallback)
  - Atualizar `settings.py` para suportar m√∫ltiplos providers
  - Criar `llm/provider_router.py`
  - Integrar em `writer.py`
  - Adicionar testes

- [ ] **Rec #8:** Dry-run mode para estimativa de custo
  - Adicionar flag `--dry-run` ao CLI
  - Criar `orchestration/dry_run.py`
  - Implementar cost estimation
  - Adicionar formata√ß√£o de output

- [ ] Documentar padr√µes de c√≥digo
  - Criar `architecture/patterns.md`
  - Documentar 10 padr√µes principais
  - Adicionar examples

**Deliverables:**
- Suporte a OpenAI/Anthropic como fallback
- Comando `egregora write --dry-run` funcionando
- `architecture/patterns.md` completo

---

### Sprint 4 (Semana 7-8): Observabilidade

**Objetivo:** Monitoring e debugging

- [ ] **Rec #7:** Adicionar m√©tricas b√°sicas (tempo, tokens)
  - Criar `observability/metrics.py`
  - Integrar em `runner.py`
  - Exportar summary no final

- [ ] Implementar tracing b√°sico
  - Adicionar OpenTelemetry
  - Instrumentar fun√ß√µes cr√≠ticas
  - Configurar exporters

- [ ] Criar health checks
  - Comando `egregora health`
  - Verificar API keys
  - Verificar dependencies

**Deliverables:**
- M√©tricas de execu√ß√£o exportadas
- Tracing b√°sico funcionando
- Comando `egregora health` implementado

---

## 6. üéñÔ∏è CLASSIFICA√á√ÉO FINAL

### Arquitetura Geral: **8/10**

**Pontos Fortes:**
- ‚úÖ Separa√ß√£o de camadas bem definida
- ‚úÖ Extensibilidade por design (adapters, skills)
- ‚úÖ Resili√™ncia operacional (journaling, auto-split)
- ‚úÖ Type safety com Pydantic + MyPy
- ‚úÖ Performance consciente (streaming, cache)

**Pontos de Melhoria:**
- ‚ö†Ô∏è Complexidade em `orchestration/` (write.py 1400 linhas)
- ‚ö†Ô∏è Gest√£o de estado fragmentada
- ‚ö†Ô∏è Coverage de testes baixo (39%)
- ‚ö†Ô∏è Vendor lock-in (Google Gemini)
- ‚ö†Ô∏è Defaults impl√≠citos (magic numbers)

---

### Manutenibilidade: **7/10**

**Positivo:**
- ‚úÖ C√≥digo bem organizado em m√≥dulos l√≥gicos
- ‚úÖ Naming conventions consistentes
- ‚úÖ Type annotations na maioria do c√≥digo

**Negativo:**
- ‚ö†Ô∏è Falta documenta√ß√£o inline (docstrings)
- ‚ö†Ô∏è Alguns m√≥dulos muito grandes (`write.py`)
- ‚ö†Ô∏è Decis√µes arquiteturais n√£o documentadas (falta ADRs)

---

### Testabilidade: **6/10**

**Positivo:**
- ‚úÖ Boa estrutura de testes (unit/integration/e2e)
- ‚úÖ Uso de property-based testing (Hypothesis)
- ‚úÖ Snapshot testing para templates

**Negativo:**
- ‚ö†Ô∏è Coverage baixo (39%)
- ‚ö†Ô∏è Faltam testes de integra√ß√£o RAG
- ‚ö†Ô∏è E2E tests com mocks (n√£o validam LLM real)

---

### Extensibilidade: **9/10**

**Positivo:**
- ‚úÖ Excelente sistema de adapters
- ‚úÖ Skills customiz√°veis
- ‚úÖ Protocol-based design
- ‚úÖ Output sinks plug√°veis

**Negativo:**
- ‚ö†Ô∏è Vendor lock-in (Gemini apenas)

---

### Performance: **8/10**

**Positivo:**
- ‚úÖ Streaming para grandes arquivos
- ‚úÖ LRU cache para embeddings
- ‚úÖ Lazy initialization (RAG)
- ‚úÖ Batch processing (banners)

**Negativo:**
- ‚ö†Ô∏è Falta monitoring/profiling
- ‚ö†Ô∏è Sem benchmarks em CI

---

### Seguran√ßa: **7/10**

**Positivo:**
- ‚úÖ Filesystem sandboxing
- ‚úÖ Input validation com Pydantic
- ‚úÖ SQL injection prote√ß√£o (Ibis)

**Negativo:**
- ‚ö†Ô∏è API keys em vari√°veis de ambiente (n√£o secrets manager)
- ‚ö†Ô∏è Falta rate limiting robusto

---

## 7. CONCLUS√ÉO

O Egregora demonstra **arquitetura s√≥lida e bem pensada**, com clara separa√ß√£o de responsabilidades e foco em extensibilidade. As decis√µes t√©cnicas (Ibis, Pydantic-AI, DuckDB) s√£o justificadas e bem executadas.

**Principais oportunidades:**

1. **Refatora√ß√£o de `write.py`** para reduzir complexidade ‚≠ê CR√çTICO
2. **Centraliza√ß√£o de configura√ß√£o** para facilitar descoberta ‚≠ê CR√çTICO
3. **Aumento de cobertura de testes** para garantir qualidade ‚≠ê IMPORTANTE
4. **Multi-provider support** para reduzir vendor lock-in ‚≠ê IMPORTANTE

**Com as recomenda√ß√µes implementadas**, a arquitetura evoluiria de **8/10 para 9.5/10**, mantendo a simplicidade enquanto ganha robustez e manutenibilidade.

---

## AP√äNDICE A: Estrutura de Arquivos Detalhada

```
src/egregora/
‚îú‚îÄ‚îÄ orchestration/          # 578 LOC (runner.py)
‚îÇ   ‚îú‚îÄ‚îÄ runner.py           # ‚ö†Ô∏è 578 linhas
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ write.py        # ‚ö†Ô∏è 1400+ linhas - PRECISA REFATORAR
‚îÇ   ‚îú‚îÄ‚îÄ context.py
‚îÇ   ‚îú‚îÄ‚îÄ factory.py
‚îÇ   ‚îú‚îÄ‚îÄ cache.py
‚îÇ   ‚îú‚îÄ‚îÄ persistence.py
‚îÇ   ‚îú‚îÄ‚îÄ journal.py
‚îÇ   ‚îú‚îÄ‚îÄ materializer.py
‚îÇ   ‚îî‚îÄ‚îÄ worker_base.py
‚îú‚îÄ‚îÄ agents/                 # Pydantic-AI agents
‚îÇ   ‚îú‚îÄ‚îÄ writer.py
‚îÇ   ‚îú‚îÄ‚îÄ writer_*.py         # Writer utilities
‚îÇ   ‚îú‚îÄ‚îÄ reader/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ elo.py
‚îÇ   ‚îú‚îÄ‚îÄ profile/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ worker.py
‚îÇ   ‚îú‚îÄ‚îÄ banner/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ worker.py
‚îÇ   ‚îî‚îÄ‚îÄ capabilities.py
‚îú‚îÄ‚îÄ database/               # DuckDB + repositories
‚îÇ   ‚îú‚îÄ‚îÄ duckdb_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ repository.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îú‚îÄ‚îÄ views.py
‚îÇ   ‚îú‚îÄ‚îÄ message_repository.py
‚îÇ   ‚îú‚îÄ‚îÄ elo_store.py
‚îÇ   ‚îú‚îÄ‚îÄ task_store.py
‚îÇ   ‚îî‚îÄ‚îÄ streaming/
‚îÇ       ‚îî‚îÄ‚îÄ stream.py
‚îú‚îÄ‚îÄ input_adapters/         # Source parsers
‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îú‚îÄ‚îÄ whatsapp/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapter.py
‚îÇ   ‚îú‚îÄ‚îÄ iperon_tjro.py
‚îÇ   ‚îî‚îÄ‚îÄ self_reflection.py
‚îú‚îÄ‚îÄ output_sinks/        # Format writers
‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îú‚îÄ‚îÄ mkdocs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ paths.py
‚îÇ   ‚îî‚îÄ‚îÄ conventions.py
‚îú‚îÄ‚îÄ transformations/        # Pure functions
‚îÇ   ‚îú‚îÄ‚îÄ windowing.py
‚îÇ   ‚îî‚îÄ‚îÄ media.py
‚îú‚îÄ‚îÄ rag/                    # Vector store
‚îÇ   ‚îú‚îÄ‚îÄ backend.py
‚îÇ   ‚îú‚îÄ‚îÄ lancedb_backend.py
‚îÇ   ‚îú‚îÄ‚îÄ embedding_router.py
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
‚îÇ   ‚îú‚îÄ‚îÄ chunking.py
‚îÇ   ‚îî‚îÄ‚îÄ ingestion.py
‚îú‚îÄ‚îÄ llm/                    # LLM integration
‚îÇ   ‚îú‚îÄ‚îÄ api_keys.py
‚îÇ   ‚îú‚îÄ‚îÄ model_fallback.py
‚îÇ   ‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_cycler.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_key_rotator.py
‚îÇ   ‚îú‚îÄ‚îÄ rate_limit.py
‚îÇ   ‚îú‚îÄ‚îÄ retry.py
‚îÇ   ‚îú‚îÄ‚îÄ token_utils.py
‚îÇ   ‚îî‚îÄ‚îÄ usage.py
‚îú‚îÄ‚îÄ config/                 # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îî‚îÄ‚îÄ defaults.py         # üÜï A CRIAR
‚îú‚îÄ‚îÄ cli/                    # Command-line
‚îÇ   ‚îú‚îÄ‚îÄ init.py
‚îÇ   ‚îú‚îÄ‚îÄ write.py
‚îÇ   ‚îú‚îÄ‚îÄ read.py
‚îÇ   ‚îú‚îÄ‚îÄ show.py
‚îÇ   ‚îî‚îÄ‚îÄ health.py
‚îú‚îÄ‚îÄ data_primitives/        # Core abstractions
‚îú‚îÄ‚îÄ ops/                    # Operations
‚îú‚îÄ‚îÄ knowledge/              # Domain knowledge
‚îú‚îÄ‚îÄ resources/              # Prompts, SQL
‚îú‚îÄ‚îÄ rendering/              # Templates
‚îú‚îÄ‚îÄ security/               # Security utils
‚îî‚îÄ‚îÄ exceptions.py           # Exception hierarchy

tests/                      # 216 test files
‚îú‚îÄ‚îÄ unit/                   # ~100 files
‚îú‚îÄ‚îÄ integration/            # ~30 files
‚îú‚îÄ‚îÄ e2e/                    # ~15 files
‚îú‚îÄ‚îÄ features/               # ~20 files
‚îú‚îÄ‚îÄ security/               # ~10 files
‚îú‚îÄ‚îÄ benchmarks/             # ~10 files
‚îú‚îÄ‚îÄ evals/                  # ~10 files
‚îú‚îÄ‚îÄ skills/                 # ~5 files
‚îî‚îÄ‚îÄ fixtures/               # Test data

architecture/               # üÜï A CRIAR
‚îú‚îÄ‚îÄ decisions/              # ADRs
‚îÇ   ‚îú‚îÄ‚îÄ 0001-use-ibis-instead-of-pandas.md
‚îÇ   ‚îú‚îÄ‚îÄ 0002-duckdb-for-local-analytics.md
‚îÇ   ‚îî‚îÄ‚îÄ template.md
‚îî‚îÄ‚îÄ patterns.md             # Code patterns
```

---

## AP√äNDICE B: M√©tricas de C√≥digo

| M√©trica | Valor | Status |
|---------|-------|--------|
| **Linhas de c√≥digo (src/)** | ~15,000 | ‚úÖ M√©dio |
| **Arquivos Python (src/)** | ~120 | ‚úÖ Organizado |
| **Arquivos de teste** | 216 | ‚úÖ Bom |
| **Coverage** | 39% | ‚ö†Ô∏è Baixo |
| **MyPy strict modules** | ~90% | ‚úÖ Excelente |
| **Maior arquivo** | write.py (1400 LOC) | ‚ö†Ô∏è Refatorar |
| **M√©dia LOC/arquivo** | ~125 | ‚úÖ Razo√°vel |
| **Complexidade ciclom√°tica m√©dia** | ~8 | ‚úÖ Aceit√°vel |

---

## AP√äNDICE C: Depend√™ncias Cr√≠ticas

| Depend√™ncia | Vers√£o | Prop√≥sito | Risco |
|-------------|--------|-----------|-------|
| `ibis-framework` | >=11.0 | Data abstraction | Baixo |
| `duckdb` | (via Ibis) | Local OLAP | Baixo |
| `lancedb` | >=0.25 | Vector store | M√©dio |
| `pydantic-ai` | >=1.25 | AI agents | M√©dio |
| `google-generativeai` | >=0.8.6 | Gemini API | **Alto** ‚ö†Ô∏è |
| `mkdocs-material` | >=9.7 | Site generation | Baixo |
| `typer` | >=0.20 | CLI | Baixo |
| `pytest` | (test) | Testing | Baixo |

**Risco Alto**: `google-generativeai` - Vendor lock-in, sem fallback cross-provider

---

**Documento gerado em:** 2026-01-22
**Pr√≥xima revis√£o:** Ap√≥s Sprint 2 (estimado 2026-02-15)
